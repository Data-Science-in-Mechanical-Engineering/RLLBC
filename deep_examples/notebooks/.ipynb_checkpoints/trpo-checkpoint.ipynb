{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a5f846-de2e-4db3-acf7-9073fd61d8c1",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"static/DSME_logo.png\" style=\"float: right;height: 6.5em;\">\n",
    "</div>\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "TRPO With Generalized Advantage Estimate\n",
    "\n",
    "Notebook Authors: Jyotirmaya Patra\n",
    "\n",
    "Adapted from: [Stable Baselines3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n",
    "\n",
    "Orignal Paper: [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "\n",
    "Additional References:\n",
    "1. OpenAI SpinningUp:\n",
    "    * [Trust Region Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/trpo.html)\n",
    "2. CS 285 at UC Berkeley:\n",
    "    * [Lecture 9: Advanced Policy Gradients](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-9.pdf) (Lecture recordings also available on Youtube!)\n",
    "3. Generalised Advantage Estimate:\n",
    "    * [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a83f6f-418b-4959-b725-9f4f32ecb319",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818d134-4fcc-4756-aa73-0d236efd42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'trpo.ipynb'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97075de-2532-473e-af22-e6cb8c3035a5",
   "metadata": {},
   "source": [
    "## TRPO Specific Helper Functions\n",
    "\n",
    "Apart from the general helper functions in `/uitls/helper_fns.py` here you will find helper functions specific to TRPO. Here's a quick preview:\n",
    "1. `kl_divergence`: compute kl divergence between two pytorch distributions\n",
    "2. `compute_policy_grad`: compute gradients for kl div and surrogate objective wrt the policy parameters \n",
    "3. `hessian_vector_product`: computes the matrix-vector product with the Fisher information matrix.\n",
    "4. `conjugate_gradient_solver`: finds an approximate solution to a set of linear equations Ax = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03865a9c-3ff9-4993-ba57-93096ae86478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(dist_true, dist_pred):\n",
    "    \"\"\"\n",
    "    Wrapper for the PyTorch implementation of the full form KL Divergence\n",
    "\n",
    "    :param dist_true: the p distribution\n",
    "    :param dist_pred: the q distribution\n",
    "    :return: KL(dist_true||dist_pred)\n",
    "    \"\"\"\n",
    "    return torch.distributions.kl_divergence(dist_true, dist_pred)\n",
    "\n",
    "\n",
    "def compute_policy_grad(actor, kl_div, policy_objective):\n",
    "    \"\"\"\n",
    "    Compute gradients for kl div and surrogate objective wrt the policy parameters\n",
    "    :param actor: actor model in agent\n",
    "    :param kl_div: The KL divergence objective\n",
    "    :param policy_objective: The surrogate advantage\n",
    "    :return: List of actor params, gradients and gradients shape.\n",
    "    \"\"\"\n",
    "    # Contains the gradients of surrogate advantage wrt to each policy parameters\n",
    "    policy_objective_gradients = []\n",
    "    # Contains gradients of the KL divergence wrt each policy parameter\n",
    "    grad_kl = []\n",
    "    # Save the shape of the gradients of the KL divergence w.r.t each policy parameter\n",
    "    # Will be used to unflatten the gradients into its original shape later in the training loop\n",
    "    grad_shape = []\n",
    "    # List of policy parameters\n",
    "    actor_params = []\n",
    "\n",
    "    for param in actor.parameters():\n",
    "        kl_param_grad, *_ = torch.autograd.grad(\n",
    "            kl_div,\n",
    "            param,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True,\n",
    "            only_inputs=True\n",
    "        )\n",
    "\n",
    "        policy_objective_grad, *_ = torch.autograd.grad(\n",
    "            policy_objective,\n",
    "            param,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )\n",
    "\n",
    "        grad_shape.append(kl_param_grad.shape)\n",
    "        grad_kl.append(kl_param_grad.reshape(-1))\n",
    "        policy_objective_gradients.append(policy_objective_grad.reshape(-1))\n",
    "        actor_params.append(param)\n",
    "\n",
    "    # Concatenate gradients before the conjugate gradient step\n",
    "    policy_objective_gradients = torch.cat(policy_objective_gradients)\n",
    "    grad_kl = torch.cat(grad_kl)\n",
    "    return actor_params, policy_objective_gradients, grad_kl, grad_shape\n",
    "\n",
    "\n",
    "def hessian_vector_product(params, grad_kl, cg_damping, vector, retain_graph=True):\n",
    "    \"\"\"\n",
    "    Computes the matrix-vector product with the Fisher information matrix.\n",
    "\n",
    "    :param params: list of parameters used to compute the Hessian\n",
    "    :param grad_kl: flattened gradient of the KL divergence between the old and new policy\n",
    "    :param vector: vector to compute the dot product the hessian-vector dot product with\n",
    "    :param retain_graph: if True, the graph will be kept after computing the Hessian\n",
    "    :return: Hessian-vector dot product (with damping)\n",
    "    \"\"\"\n",
    "    jacobian_vector_product = (grad_kl * vector).sum()\n",
    "\n",
    "    hessian_vector_product = torch.autograd.grad(\n",
    "        jacobian_vector_product,\n",
    "        params,\n",
    "        create_graph=False,\n",
    "        retain_graph=retain_graph,\n",
    "        allow_unused=True\n",
    "    )\n",
    "\n",
    "    hessian_vector_product_flattened = torch.cat([torch.ravel(grad) for grad in hessian_vector_product if grad is not None])\n",
    "    return hessian_vector_product_flattened + cg_damping * vector\n",
    "\n",
    "\n",
    "def conjugate_gradient_solver(matrix_vector_dot_fn, b, max_iter=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    Finds an approximate solution to a set of linear equations Ax = b\n",
    "\n",
    "    Sources:\n",
    "     - https://github.com/ajlangley/trpo-pytorch/blob/master/conjugate_gradient.py\n",
    "     - https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py#L122\n",
    "\n",
    "    Reference:\n",
    "     - https://epubs.siam.org/doi/abs/10.1137/1.9781611971446.ch6\n",
    "\n",
    "    :param matrix_vector_dot_fn:\n",
    "        a function that right multiplies a matrix A by a vector v\n",
    "    :param b:\n",
    "        the right hand term in the set of linear equations Ax = b\n",
    "    :param max_iter:\n",
    "        the maximum number of iterations (default is 10)\n",
    "    :param residual_tol:\n",
    "        residual tolerance for early stopping of the solving (default is 1e-10)\n",
    "    :return x:\n",
    "        the approximate solution to the system of equations defined by `matrix_vector_dot_fn`\n",
    "        and b\n",
    "    \"\"\"\n",
    "\n",
    "    # The vector is not initialized at 0 because of the instability issues when the gradient becomes small.\n",
    "    # Hence, a small random gaussian noise is used for the initialization.\n",
    "    x = 1e-4 * torch.randn_like(b)\n",
    "    residual = b - matrix_vector_dot_fn(x)\n",
    "    # Equivalent to th.linalg.norm(residual) ** 2 (L2 norm squared)\n",
    "    residual_squared_norm = torch.matmul(residual, residual)\n",
    "\n",
    "    if residual_squared_norm < residual_tol:\n",
    "        # If the gradient becomes extremely small\n",
    "        # The denominator in alpha will become zero\n",
    "        # Leading to a division by zero\n",
    "        return x\n",
    "\n",
    "    p = residual.clone()\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # A @ p (matrix vector multiplication)\n",
    "        A_dot_p = matrix_vector_dot_fn(p)\n",
    "\n",
    "        alpha = residual_squared_norm / p.dot(A_dot_p)\n",
    "        x += alpha * p\n",
    "\n",
    "        if i == max_iter - 1:\n",
    "            return x\n",
    "\n",
    "        residual -= alpha * A_dot_p\n",
    "        new_residual_squared_norm = torch.matmul(residual, residual)\n",
    "\n",
    "        if new_residual_squared_norm < residual_tol:\n",
    "            return x\n",
    "\n",
    "        beta = new_residual_squared_norm / residual_squared_norm\n",
    "        residual_squared_norm = new_residual_squared_norm\n",
    "        p = residual + beta * p\n",
    "    # Note: this return statement is only used when max_iter=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56a7a7-bace-4ddc-8c90-93aa86d8ae69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7705efc-11be-4fa7-8e1c-054bc3efbd7a",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment being run. Although initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a56a4-7e93-4ec9-8bd8-de7efc02e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'TRPO'  # algorithm name, in this case it should be 'PPO'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "exp.random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = 2\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if exp.random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = exp.random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7685598-5e38-4124-bf7e-2c76faef0607",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rollout Buffer\n",
    "\n",
    "The second dictionary, `hypp`, is initialized in the following cell. It has keys and values containing the hyperparameters necessary to the algorithm. Similar to the `exp` dict, new keys and values are added to the `hypp` in the later sections. \n",
    "\n",
    "Define both `exp.num_envs` and `hypp.num_steps`.\n",
    "\n",
    "Initialize the multiple environments and run them in parallel using the `SyncVectorEnv` class from the gym library [More info: [Link](https://www.gymlibrary.dev/content/vectorising/)]. \n",
    "\n",
    "Next, create a rollout buffer based on the number of parallel envs `exp.num_envs` and the number of steps per env `hypp.num_steps`. It is later used to save episode trajectories during agent training. The buffer gets replaced with new trajectories at the beginning of every iteration of the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632173c-fb81-48fe-8378-d3ac93736c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "exp.num_envs = 4  # number of parallel game environments\n",
    "hypp.num_steps = 128  # number of steps to run in each environment per policy rollout\n",
    "\n",
    "# Intialize vectorized gym evn\n",
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "\n",
    "# RollOut Buffer Init\n",
    "observations = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "rewards = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "dones = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "values = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080f052-8cfd-47bb-83e8-772b8c22f780",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of a deep MLP value function called the `critic`, and a deep MLP policy called the `actor`, both learned during training. \n",
    "\n",
    "The class has three methods:\n",
    "1. `get_value` method evaluates the `critic` with a given observation (state) to obtain the learned estimate of the observation's value.\n",
    "2. `get_distribution` method first evaluates the `actor` to obtain the un-normalized probabilities of the actions, which is then used to create a categorical distribution over the actions.\n",
    "3. `get_action` method returns an action. The method first obtains the categorical distribution over actions the `actor` has learned. Then, if `greedy = True`, it returns the action with the highest probability else, an action sampled from the distribution.\n",
    "4. `get_action_and_value` method when given an observation, returns the action sampled from the probability distribution (if `action=None`), the respective action's log probability, and the estimate of the observation's value according to the `critic` network.\n",
    "\n",
    "The `actor` and `critic` networks in the `Agent` class make use of the `layer_init` function to implement ideas derived from [Engstrom, Ilyas, et al., (2020)](https://openreview.net/forum?id=r1etN1rtPB), and [Andrychowicz et al. (2021)](https://openreview.net/forum?id=nIAxjsniDzg). The former introduces the idea of orthogonal initialization of weights for tanh activation. In the latter paper, decision C57 shows that initializing the policy such that the action distribution is centered as zero gives better performance. This is done by initializing the `actor`'s output layer weights with 0.01 std. (see the final layer of `self.actor`)\n",
    "\n",
    "Note: observation and state mean the same in the context of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fff29a-690d-4dc8-bf2c-191243f98a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_distribution(self, x):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        return probs\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        distr = self.get_distribution(x)\n",
    "        action = distr.sample() if not greedy else distr.mode\n",
    "        return action\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        distr = self.get_distribution(x)\n",
    "        if action is None:\n",
    "            action = distr.sample()\n",
    "        return action, distr.log_prob(action), self.critic(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08382f29-5bfb-4814-94ef-53d39db5cfea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - They are stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by deafult to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.5.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d426a0-fca1-4608-9ac7-31ceabd0b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = False  # disable to speed up training\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 20\n",
    "exp.device = device.type\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent learning specific flags\n",
    "hypp.total_timesteps = 500000  # the training duration in number of time steps\n",
    "hypp.num_minibatches = 4  # number of minibatches for gradient updates\n",
    "hypp.update_epochs = 10  # only applies to critic updates in TRPO\n",
    "\n",
    "hypp.batch_size = int(exp.num_envs * hypp.num_steps)  # len of the rollout buffer\n",
    "hypp.minibatch_size = int(hypp.batch_size // hypp.num_minibatches)  # rollout buffer size / minibatch count\n",
    "\n",
    "hypp.learning_rate = 1e-3  # size of gradient update step\n",
    "hypp.anneal_lr = False  # when True reduces the learning rate as the training progresses\n",
    "hypp.gamma = 0.99  # discount factor over future rewards\n",
    "hypp.gae_lambda = 0.95  # factor for trade-off bias vs variance for Generalized Advantage Estimator\n",
    "hypp.norm_adv = True\n",
    "hypp.cg_max_steps = 10                   # maximum number of steps in the Conjugate Gradient algorithm for computing the Hessian vector product\n",
    "hypp.cg_damping = 0.1                    # damping in the Hessian vector product computation\n",
    "hypp.line_search_shrinking_factor = 0.8  # step-size reduction factor for the line-search (i.e., `theta_new = theta + shrinking_factor^i * step`)\n",
    "hypp.line_search_max_iter = 10           # maximum number of iteration for the backtracking line-search\n",
    "hypp.target_kl = 0.01  # the target KL divergence threshold for the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3493918-2c78-4ea4-bd28-391141dc0b83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the Agent\n",
    "Before we begin training the agent we first initalize the logging (based on the repsective flags in the `exp` dict), object of the `Agent` class and the optimizer, followed by an inital set of observations. \n",
    "\n",
    "After that comes the main training loop which is comprised of:  \n",
    "1. learning rate annealing, \n",
    "2. collecting trajectories, ie, filling the rollout buffer with the actor's interaction with the environment,\n",
    "3. computing the generalized advantage estimate $A^{\\pi_{\\theta_k}}$ and returns $G$ of the trajectories saved in the rollout buffer,\n",
    "4. Policy parameters update:\n",
    "    1. Computing the gradient of the surrogate objective wrt the policy parameters, ie, $g_k = \\nabla_{\\theta} L_{\\pi_{\\theta_{k}}}\\left(\\pi_\\theta\\right)|_{\\theta_{k}}$, where: \n",
    "    $$ L_{\\pi_{\\theta_{k}}}\\left(\\pi_\\theta\\right) = \\mathbb{E}_{s , a \\sim \\pi_{\\theta_{k}}} \\left[ \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{k}}(a|s)}A^{\\pi_{\\theta_k}}(s, a) \\right]$$\n",
    "    2. Computing the Hessian of KL Divergence (also called the Fisher Information Matrix) wrt to policy parameters, ie, $H_k ={\\nabla^2_\\theta} \\bar{D}_{KL}\\left(\\pi_{\\theta_{k}}, \\pi_{\\theta}\\right)$, where\n",
    "    $$\\bar{D}_{KL}\\left(\\pi_{\\theta_{k}}, \\pi_{\\theta}\\right) = \\mathbb{E}_{s \\sim \\pi_{\\theta_{k}}}\\left[D_{\\mathrm{KL}}\\left(\\pi_{\\theta_k}(\\cdot \\mid s) \\| \\pi_\\theta(\\cdot \\mid s)\\right)\\right]$$\n",
    "    3. Using the conjugate gradient algorithm to compute: $$ x_k \\approx H_k^{-1} g_k$$\n",
    "    4. Updating the policy by bracktracing line search with: $$\\theta_{k+1}=\\theta_k+\\alpha^j \\sqrt{\\frac{2 \\delta}{x_k^T H_k x_k}} x_k $$\n",
    "5. Critic network update:\n",
    "    1. Computing the value loss for critic network update, ie, $$L^{V}(s, \\theta) = \\left(V_{\\theta}(s)-G(s)\\right)^{2}$$\n",
    "    2. Perform gradient descent over the critic network `hypp.update_epochs` times wrt the value loss function $L^{V}$.\n",
    "    \n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "\n",
    "Note: we have two vectorised gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faadf6f-01ed-4318-8e21-fd41df1faa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinit run_name\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "# create two vectorized envs: one to fill the rollout buffer with trajectories, and\n",
    "# another to evaluate the agent performance at different stages of training  \n",
    "envs.close()\n",
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.eval_count)])\n",
    "\n",
    "# init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated\n",
    "greedy_evaluation = False  # whether to perform the evaluation in a greedy way or not\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "# Create Agent class Instance and network optimizer\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.critic.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "# Init observation to start learning\n",
    "start_time = time.time()\n",
    "obs = torch.Tensor(envs.reset()).to(device)\n",
    "done = torch.zeros(exp.num_envs).to(device)\n",
    "num_updates = int(hypp.total_timesteps // hypp.batch_size)\n",
    "\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "pbar = notebook.tqdm(range(1, num_updates + 1))\n",
    "\n",
    "# training loop\n",
    "for update in pbar:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if hypp.anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * hypp.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    # collect trajectories\n",
    "    for step in range(0, hypp.num_steps):\n",
    "        observations[step] = obs\n",
    "        dones[step] = done\n",
    "\n",
    "        # sample action and collect value from learned agent policy and value networks\n",
    "        with torch.no_grad():\n",
    "            action, logprob, value = agent.get_action_and_value(obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # execute the game and log data.\n",
    "        next_obs, reward, done, infos = envs.step(action.cpu().numpy())\n",
    "\n",
    "        for idx, info in enumerate(infos):\n",
    "            agent.eval()\n",
    "            # bootstrap value of the observation when done is true and the episode is truncated\n",
    "            if (\n",
    "                done[idx]\n",
    "                and info.get(\"terminal_observation\") is not None\n",
    "                and info.get(\"TimeLimit.truncated\", False)\n",
    "               ):\n",
    "                terminal_obs = torch.tensor(info[\"terminal_observation\"]).to(device)\n",
    "                with torch.no_grad():\n",
    "                    terminal_value = agent.get_value(terminal_obs)\n",
    "                reward[idx] += hypp.gamma * terminal_value\n",
    "\n",
    "            # log episode return and length to tensorboard\n",
    "            if \"episode\" in info.keys():\n",
    "                episode_step += 1\n",
    "                pbar.set_description(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"rollout/episodic_return\", info[\"episode\"][\"r\"], global_step+idx)\n",
    "                writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], global_step+idx)\n",
    "                writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "                writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        global_step += 1 * exp.num_envs\n",
    "        obs = next_obs\n",
    "\n",
    "        # generate average performance statistics of current learned agent\n",
    "        if exp.eval_agent and episode_step % exp.eval_frequency == 0 and last_evaluated_episode != episode_step:\n",
    "            last_evaluated_episode = episode_step\n",
    "            tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "            tracked_returns_over_training.append(tracked_return)\n",
    "            tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "            tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "            # if there has been improvment of the model -\n",
    "            if np.mean(tracked_return) > eval_max_return:\n",
    "                eval_max_return = np.mean(tracked_return)\n",
    "                # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "                hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    next_done = done\n",
    "    agent.train()\n",
    "\n",
    "    # compute generalised advantage estimate and returns\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(hypp.num_steps)):\n",
    "            if t == hypp.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + hypp.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + hypp.gamma * hypp.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_distribution = copy.copy(agent.get_distribution(b_observations))\n",
    "\n",
    "    distribution = agent.get_distribution(b_observations)\n",
    "    log_prob = distribution.log_prob(b_actions)\n",
    "\n",
    "    if hypp.norm_adv:\n",
    "        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\n",
    "\n",
    "    # ratio between old and new policy, should be one at the start of each iteration\n",
    "    ratio = torch.exp(log_prob - b_logprobs)\n",
    "\n",
    "    # compute the surrogate advantage\n",
    "    policy_objective = (b_advantages * ratio).mean()\n",
    "\n",
    "    # KL Divergence\n",
    "    kl_div = kl_divergence(distribution, old_distribution).mean()\n",
    "\n",
    "    # Compute the gradient of the surroagate advantage and the kl divergence wrt to the policy network parameters\n",
    "    optimizer.zero_grad()\n",
    "    actor_params, policy_objective_gradients, grad_kl, grad_shape = compute_policy_grad(agent.actor, kl_div,\n",
    "                                                                                        policy_objective)\n",
    "\n",
    "    # Compute search_direction, ie ${H^-1}.{g}$, using the conjugate gradient method\n",
    "    hessian_vector_product_fn = partial(hessian_vector_product, actor_params, grad_kl, hypp.cg_damping)\n",
    "\n",
    "    search_direction = conjugate_gradient_solver(\n",
    "        hessian_vector_product_fn,\n",
    "        policy_objective_gradients,\n",
    "        max_iter=hypp.cg_max_steps\n",
    "    )\n",
    "\n",
    "    line_search_max_step_size = 2 * hypp.target_kl\n",
    "    line_search_max_step_size /= torch.matmul(\n",
    "        search_direction, hessian_vector_product_fn(search_direction, retain_graph=False)\n",
    "    )\n",
    "    line_search_max_step_size = torch.sqrt(line_search_max_step_size)\n",
    "\n",
    "    line_search_backtrack_coeff = 1.0\n",
    "    original_actor_params = [param.detach().clone() for param in actor_params]\n",
    "\n",
    "    policy_objective_values = []  # average value of the surrogate advantage\n",
    "    kl_divergences = []  # average KL-divergence between policies across states visited by the old policy\n",
    "\n",
    "    is_line_search_success = False\n",
    "    with torch.no_grad():\n",
    "        # update policy parameters by backtracking line-search\n",
    "        for _ in range(hypp.line_search_max_iter):\n",
    "\n",
    "            start_idx = 0\n",
    "            # Applying the scaled step direction\n",
    "            for param, original_param, shape in zip(actor_params, original_actor_params, grad_shape):\n",
    "                n_params = param.numel()\n",
    "                param.data = (\n",
    "                        original_param.data\n",
    "                        + line_search_backtrack_coeff\n",
    "                        * line_search_max_step_size\n",
    "                        * search_direction[start_idx: (start_idx + n_params)].view(shape)\n",
    "                )\n",
    "                start_idx += n_params\n",
    "\n",
    "            # Recomputing the policy log-probabilities\n",
    "            distribution = agent.get_distribution(b_observations)\n",
    "            log_prob = distribution.log_prob(b_actions)\n",
    "\n",
    "            # New policy objective\n",
    "            ratio = torch.exp(log_prob - b_logprobs)\n",
    "            new_policy_objective = (b_advantages * ratio).mean()\n",
    "\n",
    "            # New KL-divergence\n",
    "            kl_div = kl_divergence(distribution, old_distribution).mean()\n",
    "\n",
    "            # Constraint criteria:\n",
    "            # we need to improve the surrogate policy objective\n",
    "            # while being close enough (in terms of kl div) to the old policy\n",
    "            if (kl_div < hypp.target_kl) and (new_policy_objective > policy_objective):\n",
    "                is_line_search_success = True\n",
    "                break\n",
    "\n",
    "            # Reducing step size if line-search wasn't successful\n",
    "            line_search_backtrack_coeff *= hypp.line_search_shrinking_factor\n",
    "\n",
    "        if not is_line_search_success:\n",
    "            # If the line-search wasn't successful we revert to the original parameters\n",
    "            for param, original_param in zip(actor_params, original_actor_params):\n",
    "                param.data = original_param.data.clone()\n",
    "\n",
    "            policy_objective_values = policy_objective.item()\n",
    "            kl_divergences = 0\n",
    "        else:\n",
    "            policy_objective_values = new_policy_objective.item()\n",
    "            kl_divergences = kl_div.item()\n",
    "\n",
    "    b_inds = np.arange(hypp.batch_size)\n",
    "\n",
    "    value_losses = []\n",
    "\n",
    "    # update the critic network\n",
    "    for i in range(hypp.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, hypp.batch_size, hypp.minibatch_size):\n",
    "            end = start + hypp.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            newvals = agent.get_value(b_observations[mb_inds])\n",
    "            newvals = newvals.view(-1)\n",
    "\n",
    "            v_loss = 0.5 * ((newvals - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            v_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            value_losses.append(v_loss.item())\n",
    "            gradient_step += 1\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # log losses to tensorboard summary writer\n",
    "    writer.add_scalar(\"hyperparameters/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"train/value_loss\", np.mean(value_losses), global_step)\n",
    "    writer.add_scalar(\"train/kl_divergence\", kl_divergences, global_step)\n",
    "    writer.add_scalar(\"train/explained_variance\", explained_var, global_step)\n",
    "    writer.add_scalar(\"train/is_line_search_success\", int(is_line_search_success), global_step)\n",
    "    writer.add_scalar(\"train/surrogate_objective\", policy_objective_values, global_step)\n",
    "    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    last_evaluated_episode = episode_step\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvment of the model -\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=True)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)\n",
    "\n",
    "envs.close()\n",
    "envs_eval.close()\n",
    "writer.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2408a-aeea-4c1f-800c-5067bdd305e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare Trained Agents and Display Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f1482-cf6b-4a95-b64a-c0d8e8626a30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display Trained Agent Behaviour\n",
    "\n",
    "Set `agent_name` and `agent_exp_type` to load the saved agent model in the respective log folder and generate a video of the agent's interaction with the gym environment. After the cell is executed, you should see a video embedding as output, and the video is also available in the following directory: `/logs/<exp.exp_type>/<exp.run_name>/videos` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419ed1c-cfb3-4e86-be95-e418a3ffe51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = exp.run_name\n",
    "agent_exp_type = exp.exp_type  # both are needed to identify the agent location\n",
    "\n",
    "\n",
    "exp_folder = \"\" if agent_exp_type is None else agent_exp_type\n",
    "filepath, _ = hf.create_folder_relative(f\"{exp_folder}/{agent_name}/videos\")\n",
    "\n",
    "hf.record_video(exp.env_id, agent_name, f\"{filepath}/best.mp4\", exp_type=agent_exp_type, greedy=True)\n",
    "Video(data=f\"{filepath}/best.mp4\", html_attributes='loop autoplay', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ae10e-2713-4f25-a07f-b925d01c4461",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Performance of Agents During Training\n",
    "\n",
    "During the training loop, if `exp.eval_agent = True`, the performance progress of the agent during its training is saved as a csv file. To compare the saved progress of different agents, create a `dict` containing the parent folder's name of each of the csv files and use the helper function `plotter_agents_training_stats`.\n",
    "\n",
    "To load the data, you can either set `eval_params.run_name00 = exp.run_name` (if only a `tracked_performance_training.csv` file for the corresponding `exp.run_name` exists) or manually enter the folder name containing the csv file. \n",
    "\n",
    "If the agent performance you want to load is inside an exp_type folder, set `eval_params.exp_type00` to experiment type, and if not, set it to `None`. \n",
    "\n",
    "You can add more than one experiment by initializing dict keys and values of the format `eval_params.run_namexx` and `eval_params.exp_typexx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffead0b-a7a3-4062-8c93-5b209c63e43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - evaluation settings for trained agent\n",
    "\n",
    "eval_params.run_name00 = exp.run_name\n",
    "eval_params.exp_type00 = exp.exp_type\n",
    "\n",
    "# eval_params.run_name01 = \"CartPole-v1__PPO__1__230323_224300\"\n",
    "# eval_params.exp_type01 = None \n",
    "\n",
    "# eval_params.run_name02 = \"CartPole-v1__PPO__1__230302_221245\"\n",
    "# eval_params.exp_type02 = None\n",
    "\n",
    "agent_labels = []\n",
    "\n",
    "episode_axis_limit = None\n",
    "\n",
    "hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26b1fa-7c14-4bc0-8183-e59450f5d021",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TensorBoard Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abce1b0-018f-4761-954d-8015bbda779b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
