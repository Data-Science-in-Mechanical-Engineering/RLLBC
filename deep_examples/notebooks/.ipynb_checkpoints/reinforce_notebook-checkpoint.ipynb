{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a210fd1",
   "metadata": {},
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Reinforce Implementation\n",
    "\n",
    "Notebook Authors: Ramil Sabirov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493b02b",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "import math\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8223db4",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ddf45",
   "metadata": {},
   "source": [
    "### Experiment Init\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment. Although the dict is initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'REINFORCE'  # algorithm name, in this case it should be 'REINFORCE'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "set_random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = 2\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if set_random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba383a",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of a deep MLP policy that is trained during training. The network takes as input the representation of the state, passes it through several hidden layers, and finally evaluates to a probability distribution over all actions with the `softmax` function.\n",
    "\n",
    "The `Agent` class has two methods:\n",
    "1. `get_action_and_logprob` evaluates the network and samples an action from the resulting probability distribution. It also returns the logarithm of the action probability $\\log \\pi(a_t| s_t)$ which is used for obtaining gradient estimates for training.\n",
    "\n",
    "2. `get_action` evaluates the network to the probability distribution and either samples an action from that distribution (greedy = false) or returns the action with the maximal probability (greedy = true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def get_action_and_logprob(self, x):\n",
    "\n",
    "        action_probs = self.network(x)\n",
    "        probs = Categorical(probs=action_probs)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        action_probs = self.network(x)\n",
    "\n",
    "        if greedy:\n",
    "            action = action_probs.argmax(dim=1)\n",
    "        else:\n",
    "            probs = Categorical(probs=action_probs)\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b57a9",
   "metadata": {},
   "source": [
    "### Agent Hyperparams & Training Params Init\n",
    "The second dictionary, `hypp`, is initialized in the following cell. It has keys and values containing the hyperparameters necessary to the algorithm.\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - Stored in the `exp` dict. \n",
    "    - By default, all logged parameters are saved as tensorboard logs with the name `exp.run_name`\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If you'd like to later see and compare the performance of multiple agents during training (in Section 1.5.1), set `exp.eval_agent = True`\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the  \"Training The Agent\" cell enter your wandb's api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = True\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 50\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 100000  # the training duration in number of time steps\n",
    "hypp.learning_rate = 3e-4  # the learning rate for the optimizer\n",
    "hypp.gamma = 0.99  # decay factor of future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b81a75",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent, we first initialize the logging (based on the respective flags in the `exp` dict), the object of the `Agent` class, and the optimizer, followed by an initial set of observations. \n",
    "\n",
    "\n",
    "After that comes the main training loop which is comprised of:  \n",
    "1. Collect the trajectory i.e. executing the environment until it is finished, while keeping track of received rewards\n",
    "2. Compute the returns $G_t$ for every step $t$ of the trajectory\n",
    "3. Compute the loss $L =  -\\sum_{t=1}^{n}\\log \\pi_\\theta(a_t|s_t) * G_t$.\n",
    "4. Perform gradient descent (which is equivalent to gradient ascent regarding the gradient $\\frac{\\delta}{\\delta\\theta}{-L}$)\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following:\n",
    "1. `exp` and `hypp` dicts into a `.config` file (can be found in `trained_agent` folder)\n",
    "2. `agent` (instance of `Agent` class) for later evaluation (can be found in `trained_agent` folder)\n",
    "3. agent performance progress throughout training if `exp.eval_agent=True` (can be found in `tracked_data` folder)\n",
    "\n",
    "Note: Both episode and training stats are tracked by function `add_summary`. The `log_training_param_flag` ensures that the the training stats after each `hypp.update_epochs` is logged on to tensorboard (tb) when the first episode stat is logged while filling the rollout buffer\n",
    "\n",
    "Note: we have two vectorised gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da39b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "env = hf.make_single_env(exp.env_id, exp.seed, False, exp.run_name)\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i, i, False, None) for i in range(1)])\n",
    "\n",
    "# init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "greedy_evaluation = True\n",
    "eval_max_return = -math.inf\n",
    "\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pbar = notebook.tqdm(total=hypp.total_timesteps)\n",
    "\n",
    "# the maximum number of steps an evironment is rolled out\n",
    "max_steps = 1000\n",
    "\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "while global_step < hypp.total_timesteps:\n",
    "\n",
    "    next_obs = env.reset()\n",
    "    rewards = torch.zeros(max_steps).to(device)\n",
    "    actions = torch.zeros((max_steps,) + env.action_space.shape).to(device)\n",
    "    obs = torch.zeros((max_steps, ) + env.observation_space.shape).to(device)\n",
    "    logprobs = torch.zeros(max_steps).to(device)\n",
    "\n",
    "    episode_length = 0\n",
    "\n",
    "    # collect trajectory\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        episode_length = episode_length + 1\n",
    "        global_step = global_step + 1\n",
    "\n",
    "        next_obs = torch.tensor(next_obs).to(device)\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        # choose action according to agent network\n",
    "        action, log_prob = agent.get_action_and_logprob(next_obs)\n",
    "\n",
    "        # apply action to envs\n",
    "        next_obs, reward, done, info = env.step(action.cpu().item())\n",
    "\n",
    "        rewards[step] = torch.tensor(reward).to(device)\n",
    "        actions[step] = action\n",
    "        logprobs[step] = log_prob\n",
    "\n",
    "        if done:\n",
    "            # episode has been finished\n",
    "            episode_step = episode_step + 1\n",
    "            break\n",
    "\n",
    "    # evaluate model\n",
    "    if (episode_step % exp.eval_frequency == 0) and exp.eval_agent:\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed)\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        # if there has been improvment of the model - save model, create video, log video to wandb\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            hf.save_model(agent, exp.run_name, print_path=False)\n",
    "            if exp.capture_video:\n",
    "                filepath, _ = hf.create_folder_relative(f\"videos/{exp.run_name}\")\n",
    "                video_file = f\"{filepath}/{episode_step}.mp4\"\n",
    "                hf.record_video(exp.env_id, agent, device, video_file, greedy=greedy_evaluation)\n",
    "                if wandb.run is not None:\n",
    "                    wandb.log({\"video\": wandb.Video(video_file, fps=4, format=\"gif\")})\n",
    "\n",
    "    # calculate returns\n",
    "    returns = torch.zeros(episode_length)\n",
    "    for t in reversed(range(episode_length)):\n",
    "\n",
    "        if t == episode_length-1:\n",
    "            returns[t] = rewards[t]\n",
    "        else:\n",
    "            returns[t] = returns[t+1] * hypp.gamma + rewards[t]\n",
    "\n",
    "    # calculate loss from tensors\n",
    "    loss = torch.zeros((1,))\n",
    "    for t in range(episode_length):\n",
    "        loss -= returns[t] * logprobs[t]\n",
    "\n",
    "    # logging information regarding agent performance\n",
    "    writer.add_scalar(\"rollout/episodic_return\", sum(rewards), global_step)\n",
    "    writer.add_scalar(\"rollout/episodic_length\", episode_length, global_step)\n",
    "\n",
    "    # logging information about the loss\n",
    "    writer.add_scalar(\"train/policy_loss\", loss, global_step)\n",
    "    writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    gradient_step = gradient_step + 1\n",
    "    pbar.update(min(episode_length, hypp.total_timesteps - pbar.n))\n",
    "    pbar.set_description(f\"episode={episode_step}, episodic_return={sum(rewards)}\")\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvment of the model - save model, create video, log video to wandb\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        hf.save_model(agent, exp.run_name, print_path=False)\n",
    "        if exp.capture_video:\n",
    "            filepath = f\"{os.getcwd()}/logs/videos/{exp.run_name}/{episode_step}.mp4\"\n",
    "            hf.record_video(exp.env_id, agent, device, filepath, greedy=greedy_evaluation)\n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\"video\": wandb.Video(filepath, fps=4, format=\"gif\")})\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name)    \n",
    "\n",
    "env.close()\n",
    "writer.close()\n",
    "pbar.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode= 'disabled')\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13093318",
   "metadata": {},
   "source": [
    "## Compare Trained Agents and Display Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18094b5",
   "metadata": {},
   "source": [
    "### Compare Agents\n",
    "\n",
    "Here you can build a plot to compare average rewards over multiple episodes. In the dict `eval_params`, you can specify the episode count in the key `num_episodes`.\n",
    "\n",
    "After successfully running the previous cells, you will now have a trained agent model with the name `<exp.run_name>.st` saved to the trained_agent folder. To load the agent, you can either set `eval_params.agent_name00 = exp.run_name` or manually enter its name (without the extension .st).\n",
    "\n",
    "An example of a trained agent model would be `MountainCar-v0__PPO__1__230223_101914.st`.\n",
    "\n",
    "Add additional agent models from the trained_agent folder to build an average performance comparison. To do this, create new keys of the format `agent_namexx` in the `eval_params` dict. \n",
    "\n",
    "An example would be: `eval_params.agent_namexx = MountainCar-v0__PPO__1__230223_001255`\n",
    "\n",
    "The function `load_multiple_models` will load the agents to the dict `agents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - dict containing parameters necessary for the setup to evalaute trained agents\n",
    "\n",
    "eval_params.env_id = 'CartPole-v1'\n",
    "eval_params.num_envs = 4\n",
    "eval_params.num_episodes = 50\n",
    "eval_params.capture_video = False\n",
    "\n",
    "def record_trigger(x): return x==0 # \n",
    "\n",
    "eval_params.folder_path = f\"{os.getcwd()}/logs/trained_agent\"\n",
    "eval_params.agent_name00 = exp.run_name\n",
    "\n",
    "\n",
    "agents = edict()  # agents - dict containing trained agent models\n",
    "agents = hf.load_multiple_agent_models(eval_params)\n",
    "\n",
    "agent_stats = edict()  # agent_stats - dict containing statistics captured during evaluation of each agent listed in eval_params\n",
    "\n",
    "epsiodic_return_over_runs = []\n",
    "avg_eps_return_over_runs = []\n",
    "std_eps_return_over_runs = []\n",
    "\n",
    "# self-define plot labels using a list of strings (length should be equal to number of agents being evaluated)\n",
    "# when set to None: plotter function generates default labels\n",
    "agent_labels = None\n",
    "\n",
    "for idx, agent_model in enumerate(agents.values()):\n",
    "    agent_idx = f\"{idx:02d}\"\n",
    "    agent_name_key = f\"agent_name{agent_idx}\"\n",
    "    save_vid_dir = f\"captured_vid_dir{agent_idx}\"\n",
    "    epsiodic_return_over_runs = f\"epsiodic_return_over_runs{agent_idx}\"\n",
    "\n",
    "    agent_name = eval_params[agent_name_key]\n",
    "    eval_params[save_vid_dir] = f\"{agent_name}__TrainedAgent\"\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([hf.make_env(eval_params.env_id, exp.seed+i, i, False, eval_params[save_vid_dir], record_trigger) for i in range(eval_params.num_envs)])\n",
    "\n",
    "    agent_stats[epsiodic_return_over_runs] = hf.evaluate_agent(envs, agents[f\"agent{agent_idx}\"], eval_params.num_episodes, exp.seed, greedy_actor=False)\n",
    "\n",
    "    avg_return = np.mean(agent_stats[epsiodic_return_over_runs])\n",
    "    std_return = np.std(agent_stats[epsiodic_return_over_runs])\n",
    "\n",
    "    avg_eps_return_over_runs.append(avg_return), std_eps_return_over_runs.append(std_return)\n",
    "\n",
    "hf.plotter_trained_agent_comparison(eval_params, avg_eps_return_over_runs, std_eps_return_over_runs, eval_params.num_episodes, agent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca9e83",
   "metadata": {},
   "source": [
    "### Compare Performance of Agents During Training\n",
    "\n",
    "When `exp.eval_agent = True`, the performance progress of the agent throughout its training is saved as a csv file in the `tracked_data` folder. In order to compare performance progress of different agents create an `dict` containing the names of the csv files (without `.csv` extension) and use function `plotter_agents_training_stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008addbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - dict containing parameters necessary for the setup to evalaute trained agents\n",
    "\n",
    "eval_params.agent_name00 = exp.run_name\n",
    "# eval_params.agent_nameXX = \"CartPole-v1__PPO__1__230128_211640\"\n",
    "\n",
    "agent_labels = []\n",
    "\n",
    "episode_axis_limit = None\n",
    "\n",
    "hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bf7d0",
   "metadata": {},
   "source": [
    "### Display Trained Agent Behaviour \n",
    "\n",
    "Set `agent_name` to run name of previous run or to `exp.run_name` for the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = exp.run_name\n",
    "\n",
    "filepath, _ = hf.create_folder_relative(f\"videos/{agent_name}\")\n",
    "hf.record_video(exp.env_id, agent_name, device, f\"{filepath}/best.mp4\", greedy=True)\n",
    "Video(filename=f\"{filepath}/best.mp4\", html_attributes='loop autoplay')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af260c86",
   "metadata": {},
   "source": [
    "## TensorBoard Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
