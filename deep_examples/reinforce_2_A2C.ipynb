{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a210fd1",
   "metadata": {},
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Reinforce Implementation\n",
    "\n",
    "Notebook Authors: Ramil Sabirov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493b02b",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d58f9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:09:59.486037Z",
     "start_time": "2025-06-23T11:09:52.160715Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'reinforce.ipynb'\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/PycharmProjects/RLLBC-library/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a8223db4",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ddf45",
   "metadata": {},
   "source": [
    "### Experiment Init\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment. Although the dict is initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "id": "45b9b8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:09:59.769927Z",
     "start_time": "2025-06-23T11:09:59.494622Z"
    }
   },
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'REINFORCE-2-A2C'  # algorithm name, in this case it should be 'REINFORCE'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "set_random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = int(os.getenv(\"SEED\", 1))\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if set_random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = set_random_seed"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ebba383a",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of a deep MLP policy that is trained during training. The network takes as input the representation of the state, passes it through several hidden layers, and finally evaluates to a probability distribution over all actions with the `softmax` function.\n",
    "\n",
    "The `Agent` class has two methods:\n",
    "1. `get_action_logprob_and_value` evaluates the network and samples an action from the resulting probability distribution. It also returns the logarithm of the action probability $\\log \\pi(a_t| s_t)$ which is used for obtaining gradient estimates for training. In addition, it returns the current value estimate $\\hat{V}(s_t)$ of the state.\n",
    "\n",
    "2. `get_action` evaluates the network to the probability distribution and either samples an action from that distribution (greedy = false) or returns the action with the maximal probability (greedy = true)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b80e1114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:10:00.986317Z",
     "start_time": "2025-06-23T11:10:00.049414Z"
    }
   },
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.p_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.v_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def get_action_logprob_and_value(self, x):\n",
    "\n",
    "        action_probs = self.p_network(x)\n",
    "        probs = Categorical(probs=action_probs)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), self.v_network(x)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        action_probs = self.p_network(x)\n",
    "\n",
    "        if greedy:\n",
    "            action = action_probs.argmax(dim=1)\n",
    "        else:\n",
    "            probs = Categorical(probs=action_probs)\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.v_network(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "885b57a9",
   "metadata": {},
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - Stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by default to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.4.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "id": "53f8844a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:10:01.328589Z",
     "start_time": "2025-06-23T11:10:00.999646Z"
    }
   },
   "source": [
    "hypp = edict()\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = bool(os.getenv(\"CAPTURE_VIDEO\", True))\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = bool(os.getenv(\"EVAL\", True))  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 20\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 200_000  # the training duration in number of time steps\n",
    "hypp.learning_rate = 1e-4  # the learning rate for the optimizer\n",
    "hypp.v_coeff = 0.5\n",
    "hypp.gamma = 0.99  # decay factor of future rewards"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "45b81a75",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent, we first initialize the logging (based on the respective flags in the `exp` dict), the object of the `Agent` class, and the optimizer, followed by an initial set of observations. \n",
    "\n",
    "\n",
    "After that comes the main training loop which is comprised of:  \n",
    "1. Collect the trajectory i.e. executing the environment until it is finished, while keeping track of received rewards\n",
    "2. Compute the returns $G_t$ for every step $t$ of the trajectory\n",
    "3. Compute the policy loss with baseline $L_p = -\\sum_{t=1}^{n}\\log \\pi_\\theta(a_t|s_t) * (G_t - \\hat{V}(s_t))$.\n",
    "4. Compute the value loss $L_v = 0.5\\sum_{t=1}^{n}(G_t-\\hat{V}(s_t))^2$\n",
    "5. Perform gradient descent (which is equivalent to gradient ascent regarding the gradient $\\frac{\\delta}{\\delta\\theta}{-L}$)\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "\n",
    "Note: we have two gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "id": "0da39b1c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-23T11:20:14.297914Z",
     "start_time": "2025-06-23T11:10:01.342284Z"
    }
   },
   "source": [
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "env = hf.make_single_env(exp.env_id, exp.seed)\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])\n",
    "\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "greedy_evaluation = False\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = notebook.tqdm(total=hypp.total_timesteps)\n",
    "\n",
    "max_steps = 1000\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "# Initial manual reset\n",
    "next_obs_np = env.reset()\n",
    "next_obs = torch.tensor(next_obs_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "while global_step < hypp.total_timesteps:\n",
    "    obs = next_obs\n",
    "    episodic_return = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and episode_length < max_steps and global_step < hypp.total_timesteps:\n",
    "        global_step += 1\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get action, logprob, and value\n",
    "        action, logprob, value = agent.get_action_logprob_and_value(obs)\n",
    "\n",
    "        # Interact with environment\n",
    "        next_obs_np, reward, done, _ = env.step(action.cpu().item())\n",
    "        done = done or episode_length >= max_steps\n",
    "\n",
    "        next_obs = torch.tensor(next_obs_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "\n",
    "        # Get value estimate for next state\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs) * (1.0 - float(done))\n",
    "\n",
    "        # TD Advantage\n",
    "        advantage = reward + hypp.gamma * next_value - value\n",
    "\n",
    "        # Losses\n",
    "        policy_loss = -advantage.detach() * logprob\n",
    "        value_loss = 0.5 * (advantage ** 2)\n",
    "        loss = policy_loss + hypp.v_coeff * value_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        gradient_step += 1\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar(\"train/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"train/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        episodic_return += reward.item()\n",
    "        obs = next_obs\n",
    "\n",
    "    episode_step += 1\n",
    "    writer.add_scalar(\"rollout/episodic_return\", episodic_return, global_step)\n",
    "    writer.add_scalar(\"rollout/episodic_length\", episode_length, global_step)\n",
    "    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "    pbar.update(min(episode_length, hypp.total_timesteps - pbar.n))\n",
    "    pbar.set_description(f\"episode={episode_step}, episodic_return={episodic_return}\")\n",
    "\n",
    "    # Manual reset after episode ends\n",
    "    next_obs_np = env.reset()\n",
    "    next_obs = torch.tensor(next_obs_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Optional: Evaluate agent\n",
    "    if (episode_step % exp.eval_frequency == 0) and exp.eval_agent:\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,\n",
    "                                                                exp.seed, greedy_actor=greedy_evaluation)\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "# Final evaluation\n",
    "if exp.eval_agent:\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,\n",
    "                                                            exp.seed, greedy_actor=greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=True)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training,\n",
    "                           tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)\n",
    "\n",
    "env.close()\n",
    "writer.close()\n",
    "pbar.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode='disabled')\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpaul-kruse\u001B[0m (\u001B[33mRLLBC-atari\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/data/PycharmProjects/RLLBC-library/deep_examples/logs/wandb/run-20250623_131002-kvqdyh2z</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/kvqdyh2z' target=\"_blank\">CartPole-v1__REINFORCE-2-A2C__1__250623_130959</a></strong> to <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/kvqdyh2z' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/kvqdyh2z</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96219070511b44bcaa5a5f83c58389e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b2e6644a-190b-4f35-88ba-468315320b16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TensorBoard Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b09fed1-8d04-4f24-b617-dc5e96049a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
