{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4f1407-ebe7-4afd-8e96-47877badf739",
   "metadata": {
    "tags": []
   },
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Authors: Jyotirmaya Patra, Ramil Sabirov\n",
    "\n",
    "Adapted from: [CleanRL](https://github.com/vwxyzjn/cleanrl)\n",
    "\n",
    "Orignal Paper: [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n",
    "](https://arxiv.org/abs/1801.01290#)\n",
    "\n",
    "Variation of the Orignal Paper on which the notebook is based: [Soft Actor-Critic Algorithms and Applications\n",
    "](https://arxiv.org/abs/1812.05905)\n",
    "\n",
    "Additional References:\n",
    "1. Generalised Advantage Estimate:\n",
    "    * [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n",
    "2. OpenAI SpinningUp:\n",
    "    * [Soft Actor-Critic](https://spinningup.openai.com/en/latest/algorithms/sac.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efbe47-50d4-41ca-a03d-1994d4751eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fb41e0-6fe7-4210-b784-63049e1cac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'sac.ipynb'\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de5b92-f042-4dbb-9b78-56f094553a21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86412c6-d16f-4f3c-b03a-540715d0146a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment being run. Although initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports continuous gym environments, e.g. Pendulum-v1, MountainCarContinuous-v0, BipedalWalker-v3, and LunarLanderContinuous-v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86354c3c-479d-467e-a9ee-f277e602292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'SAC'  # algorithm name, in this case it should be 'SAC'\n",
    "exp.env_id = 'Pendulum-v1'  # name of the gym environment to be used in this experiment. Eg: Pendulum-v1, BipedalWalker-v3\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "exp.random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = 1\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if exp.random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = exp.random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be4f89-0b03-4166-9a68-e9b3b8f0de29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent Model Class\n",
    "\n",
    "\n",
    "The `Agent` class consists of two different architectures of neural networks: a network architecture for the actor (`ActorNetwork`) and a network architecture for the Q-values (`QNetwork`). The Q-network takes state and action as input and evaluates to a scalar value that approximates the expected return until the end of the episode. The actor network takes the state as input and evaluates to a (multidimensional) gaussian distribution by specifying mean and (logarithm of) standard deviation. The logarithm of the standard deviation is bounded by a specified interval. Just as in [TD3](https://arxiv.org/abs/1802.09477), to counteract the overestimation of Q-values, SAC uses two Q-networks instead of one.\n",
    "\n",
    "The `Agent` class uses target networks to stabilize the learning process. In particular, it holds a target network for both Q-networks. The class provides methods to query the four different Q-networks, i.e. `get_q1_value`, `get_target_q1_value`, `get_q2_value`, and `get_target_q2_value`. The class also provides methods to query the actor network: `get_action` samples an action for a state according to the computed gaussian distribution and `get_action_and_logprob` additionally outputs the logarithm of the gaussian for the sampled action as well as the mean action. The sampled action is always first transformed to $[-1,1]$ by a hyperbolic tangent before it is scaled to the desired action space range.\n",
    "\n",
    "Note that the target networks will not be subject to the optimizer, meaning they will not be modified through gradient steps. Instead, the `Agent` class provides the function `track_networks` to make the parameters of the target networks slowly track the parameters of the learned networks by the linear combination $\\theta^t \\leftarrow \\tau\\theta + (1-\\tau)\\theta^t$ with $\\tau \\ll 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9cdf50-c151-4f11-aff4-071449a9d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape, dtype=int), 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.LOG_STD_MIN = -5\n",
    "\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.act = F.relu\n",
    "        self.fc_mean = nn.Linear(64, np.prod(env.single_action_space.shape, dtype=int))\n",
    "        self.fc_logstd = nn.Linear(64, np.prod(env.single_action_space.shape, dtype=int))\n",
    "\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = self.LOG_STD_MIN + 0.5 * (self.LOG_STD_MAX - self.LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ab7544-6785-4663-813f-583b3ba2d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.q1_net = Critic(env)\n",
    "        self.q2_net = Critic(env)\n",
    "        self.target_q1_net = Critic(env)\n",
    "        self.target_q2_net = Critic(env)\n",
    "        self.actor_net = Actor(env)\n",
    "\n",
    "        self.target_q1_net.load_state_dict(self.q1_net.state_dict())\n",
    "        self.target_q2_net.load_state_dict(self.q2_net.state_dict())\n",
    "\n",
    "    def get_q1_value(self, x, a):\n",
    "        return self.q1_net(x, a)\n",
    "\n",
    "    def get_q2_value(self, x, a):\n",
    "        return self.q2_net(x, a)\n",
    "\n",
    "    def get_target_q1_value(self, x, a):\n",
    "        return self.target_q1_net(x, a)\n",
    "\n",
    "    def get_target_q2_value(self, x, a):\n",
    "        return self.target_q2_net(x, a)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        mean, log_std = self.actor_net(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        if not greedy:\n",
    "            x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        else:\n",
    "            x_t = mean  # greedy action\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.actor_net.action_scale + self.actor_net.action_bias\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_action_and_logprob(self, x):\n",
    "        mean, log_std = self.actor_net(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.actor_net.action_scale + self.actor_net.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.actor_net.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.actor_net.action_scale + self.actor_net.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def update_target_networks(self, tau):\n",
    "        for param, target_param in zip(self.q1_net.parameters(), self.target_q1_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for param, target_param in zip(self.q2_net.parameters(), self.target_q2_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa01160-2779-4ced-b3b6-fef310652fe9",
   "metadata": {},
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The second dictionary we use, `hypp`, is initialized in the following cell. It has keys and values containing the hyperparameters necessary to the algorithm.\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - They are stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by deafult to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.5.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc74697-2586-40cc-b626-5e460ae92705",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "# env count\n",
    "exp.num_envs = 1  # currently replaybuffer from SB3 supports only single environment\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = True  # disable to speed up training\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 1 # CHANGE: From 10\n",
    "exp.eval_frequency = 100 # CHANGE: from 20\n",
    "exp.device = device.type\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 100000  # the training duration in number of time steps\n",
    "hypp.learning_starts = 5000  # timestep after which gradient descent updates begins\n",
    "hypp.buffer_size = 10000  # size of replay buffer\n",
    "hypp.batch_size = 256  # number of minibatches for gradient updates\n",
    "\n",
    "hypp.gamma = 0.99  # discount factor over future rewards\n",
    "hypp.tau = 0.005  # smoothing coefficient for target q neworks parameters\n",
    "hypp.exploration_noise = 0.1  # the scale of exploration noise\n",
    "hypp.policy_lr = 3e-4  # learning rate of the policy network optimizer\n",
    "hypp.q_lr = 1e-3  # learning rate of the Q network network optimizer\n",
    "hypp.policy_frequency = 2  # frequency of training policy (delayed)\n",
    "hypp.target_network_frequency = 1  # frequency of updates for the target networks\n",
    "hypp.alpha = 0.2  # Entropy regularization coefficient\n",
    "hypp.autotune = True  # automatic tuning of the entropy coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a3272-c548-415e-8423-ed3a6277c43e",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "In the following cell the **replay buffer** is initialised. It is used in 2 ways:\n",
    "- It **stores transitions** the agent took in the environment in the past (i.e. (state, action, next_state, reward, termination, truncation) tuples)\n",
    "- It is possible to **sample a batch** of transitions from the replay buffer. The batch is used to perform a gradient step on for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125c0614-13d0-44f6-a788-0b16efef30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    int(hypp.buffer_size),\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False, # CHANGE: Adopted from cleanRL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ab05-01a7-4562-92f6-867c6ed0605c",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "Before we begin training the agent, we first initialize the logging (based on the respective flags in the `exp` dict), the object of the `Agent` class and the optimizers: one for the Q-networks, one for the actor, and possibly one for tuning the entropy coefficient $\\alpha$. We also initialize an initial set of observations.\n",
    "\n",
    "After that follows the main training loop which consists of an initial **warm-up phase** followed by the **training phase**. In the warm-up phase we sample random trajectories to fill the replay buffer with observations. After that we enter the training phase which consists of:\n",
    "1. **Taking a step in the environment** according to the policy defined by the actor network.\n",
    "2. **Sampling a minibatch from the replay buffer** of previously experienced transitions. These are tuples of the form $(s,a,s',r,d)$=(state, action, next_state, reward, done) where done is a boolean indicating whether the environment has terminated after the transition.\n",
    "3. **Optimizing the Q-networks** by:\n",
    "    1. Sampling the target actions $\\tilde{a}_i$ in the states $s'_i$ of the minibatch from the policy network $\\pi$\n",
    "    2. Calculating the targets for each transition in the minibatch. Let $Q_j'(s,a)$ denote the Q-value of the $j$-th target network in state $s$ for action $a$, then the target $T_i$ for transition $i$ of the minibatch is:  \n",
    "    <br>$$\\begin{equation}T_i =\n",
    "\\begin{cases}\n",
    "r_i + \\min_{j=1,2}Q_j'(s'_i,\\tilde{a}_i) - \\alpha\\log (\\pi(s'_i, \\tilde{a}_i)) &, \\text{if } d_i = 0\\\\\n",
    "r_i &, \\text{else}\n",
    "\\end{cases}\n",
    "\\end{equation}$$  \n",
    "\n",
    "    3. Setting up the Q-loss functions $L_{Q_j}$ as a mean squared error (MSE) between Q-values and computed targets. Let $Q_j(s,a|\\theta^{Q_j})$ be the Q-value of the $j$-th Q-network for state $s$ and action $a$, then for $j=1,2$ the Q-losses are:  \n",
    "    <br>$$L_{Q_j} = \\frac{1}{m}\\sum_{i=1}^{m}(Q(s_i,a_i|\\theta^{Q_j}) - T_i)^2$$  \n",
    "    \n",
    "    4. Performing a gradient descent step for the parameters of the Q-networks $\\theta^{Q} = \\bigcup_{j=1,2}\\theta^{Q_j}$ along the gradient $\\nabla_{\\theta^Q} L_{Q_1} + L_{Q_2}$.\n",
    "4. **Optimizing the actor network** by:\n",
    "    1. Computing the policy loss $L_\\pi$ for the minibatch. Let $\\tilde{a}_i \\sim \\pi(\\cdot|s_i; \\theta^{\\pi})$ be the actions sampled (once again) by the actor network $\\pi$ in states $s_i$, then the policy loss is:  \n",
    "    <br>$$L_{\\pi} = \\frac{1}{m}\\sum_{i=1}^m\\left( \\alpha\\log\\pi(s_i, \\tilde{a}_i|\\theta^{\\pi})-\\min_{j=1,2}Q_j(s_i,\\tilde{a}_i)\\right)$$  \n",
    "    \n",
    "    2. Performing a gradient descent step for parameters of actor network $\\theta^\\pi$ along the gradient $\\nabla_{\\theta^\\pi} L_\\pi$.\n",
    "    \n",
    "5. **Tuning the temperature parameter $\\alpha$** by:\n",
    "    1. Setting up a loss function $L_{\\alpha}$ for the minibatch. Again, for the sampled actions $\\tilde{a}_i \\sim \\pi(\\cdot|s_i)$ from the actor network $\\pi$ in states $s_i$, we arrive at the loss: $$L_{\\alpha} = \\frac{1}{m}\\sum_{i=1}^m -\\alpha * (\\log\\pi(s_i, \\tilde{a}_i) + \\bar{H})$$ where $\\bar{H}$ is a specified target entropy.\n",
    "    2. Performing a gradient descent step for the parameter $\\alpha$ along the gradient $\\nabla_{\\alpha} L_{\\alpha}$.  \n",
    "      \n",
    "      Note: for numerical reasons, rather than tuning $\\alpha$ directly, this implementation tunes its logarithm. The actually implemented formulas can be obtained by replacing $\\alpha$ by $\\log\\alpha$.\n",
    "    \n",
    "6. **Updating the target networks** by letting them track the parameters of the learned networks.\n",
    "\n",
    "Note that steps 4 to 6 are generally not performed in every step of the main loop but in a delayed fashion.\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "Note: we have two vectorised gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f237b86-59fc-415e-879d-b3ad925497ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-kesper98\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kesperlukas/Projects/RLLBC/deep_examples/logs/wandb/run-20241022_145804-rbcnw7hw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1/runs/rbcnw7hw' target=\"_blank\">Pendulum-v1__SAC__1__241022_145803</a></strong> to <a href='https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1' target=\"_blank\">https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1/runs/rbcnw7hw' target=\"_blank\">https://wandb.ai/lukas-kesper98/RLLBC_Pendulum-v1/runs/rbcnw7hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f06febc5194b6cb984094cbbbe81f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_done\n",
      "steppin\n",
      "steppinn\n",
      "steppinnn\n",
      "steppinnnn\n",
      "ohoh1\n",
      "ohoh2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "ohoh3\n",
      "ohoh4\n",
      "ohoh5\n",
      "ohohoh\n",
      "ping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "size must be two numbers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m         eval_max_return \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(tracked_return)\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# call helper function save_and_log_agent to save model, create video, log video to wandb\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_and_log_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteppinnnnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# begin gradient descent updates\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/RLLBC/deep_examples/utils/helper_fns.py:466\u001b[0m, in \u001b[0;36msave_and_log_agent\u001b[0;34m(exp_dict, agent, episode_step, greedy, print_path, env_wrapper)\u001b[0m\n\u001b[1;32m    464\u001b[0m filepath, _ \u001b[38;5;241m=\u001b[39m create_folder_relative(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_dict\u001b[38;5;241m.\u001b[39mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/videos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m video_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 466\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb\u001b[38;5;241m.\u001b[39mVideo(video_file, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgif\u001b[39m\u001b[38;5;124m\"\u001b[39m)})\n",
      "File \u001b[0;32m~/Projects/RLLBC/deep_examples/utils/helper_fns.py:426\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env_id, agent, file, exp_type, greedy, env_wrapper)\u001b[0m\n\u001b[1;32m    423\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mping\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m    429\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Applications/Conda/miniconda3/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/Conda/miniconda3/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/Conda/miniconda3/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/Conda/miniconda3/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:362\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[0;32m--> 362\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m     _check_render_return(env\u001b[38;5;241m.\u001b[39mrender_mode, result)\n",
      "File \u001b[0;32m/Applications/Conda/miniconda3/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/envs/classic_control/pendulum.py:237\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m img \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mload(fname)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     scale_img \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothscale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_u\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_u\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     is_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    242\u001b[0m     scale_img \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(scale_img, is_flip, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: size must be two numbers"
     ]
    }
   ],
   "source": [
    "# reinit run_name\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "# create two vectorized envs: one to fill the rollout buffer with trajectories, and\n",
    "# another to evaluate the agent performance at different stages of training\n",
    "envs.close()\n",
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.eval_count)])\n",
    "\n",
    "# envs.single_observation_space.dtype = np.float32\n",
    "\n",
    "# init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated\n",
    "greedy_evaluation = True  # whether to perform the evaluation in a greedy way or not\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "q_optimizer = optim.Adam(list(agent.q1_net.parameters()) + list(agent.q2_net.parameters()), lr=hypp.q_lr)\n",
    "actor_optimizer = optim.Adam(agent.actor_net.parameters(), lr=hypp.policy_lr)\n",
    "\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "# Automatic entropy tuning\n",
    "if hypp.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=hypp.q_lr)\n",
    "else:\n",
    "    alpha = hypp.alpha\n",
    "\n",
    "# Init observation to start learning\n",
    "start_time = time.time()\n",
    "obs = torch.Tensor(envs.reset()[0]).to(device) # CHANGE: Reset returns a list of observations, so we need to index it\n",
    "\n",
    "pbar = notebook.tqdm(range(hypp.total_timesteps))\n",
    "\n",
    "print('setup_done')\n",
    "\n",
    "# training loop\n",
    "for global_step in pbar:\n",
    "\n",
    "    print('steppin')\n",
    "    \n",
    "    # collect trajectories; sample action randomly if learning/gradient updates has not begun\n",
    "    if global_step < hypp.learning_starts:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        # actions, _, _ = agent.get_action(torch.Tensor(obs).to(device))\n",
    "        actions = agent.get_action(torch.Tensor(obs).to(device))\n",
    "        actions = actions.detach().cpu().numpy()\n",
    "\n",
    "    # execute the game and log data\n",
    "    # next_obs, rewards, dones, infos = envs.step(actions)\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions) # CHANGE: step includes termination and truncation now\n",
    "\n",
    "    print('steppinn')\n",
    "    \n",
    "    \"\"\"\n",
    "    real_next_obs = next_obs.copy()\n",
    "    # record rewards for plotting purposes at the end of every episode\n",
    "    for idx, info in enumerate(infos):\n",
    "        if \"episode\" in info.keys():\n",
    "            pbar.set_description(f\"global_step={global_step}, episodic_return={info['episode']['r']:.3f}\")\n",
    "            writer.add_scalar(\"rollout/episodic_return\", info[\"episode\"][\"r\"], global_step+idx)\n",
    "            writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], global_step+idx)\n",
    "            writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "            writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "        # handle `terminal_observation`\n",
    "        if dones[idx]:\n",
    "            real_next_obs[idx] = info[\"terminal_observation\"]\n",
    "\n",
    "    # save data to reply buffer\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "    \"\"\"\n",
    "\n",
    "    print('steppinnn')\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos:\n",
    "        print('ouch')\n",
    "        for info in infos[\"final_info\"]:\n",
    "            pbar.set_description(f\"global_step={global_step}, episodic_return={info['episode']['r']:.3f}\")\n",
    "            writer.add_scalar(\"rollout/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "            writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "            episode_step += 1\n",
    "            break\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, trunc in enumerate(truncations):\n",
    "        if trunc:\n",
    "            real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    print('steppinnnn')\n",
    "\n",
    "    # generate average performance statistics of current learned agent\n",
    "    if exp.eval_agent and episode_step % exp.eval_frequency == 0 and last_evaluated_episode != episode_step:\n",
    "        print('ohoh1')\n",
    "        last_evaluated_episode = episode_step\n",
    "        print('ohoh2')\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "        print('ohoh3')\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        print('ohoh4')\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        print('ohoh5')\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        print('ohohoh')\n",
    "        \n",
    "        # if there has been improvment of the model -\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "            hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    print('steppinnnnn')\n",
    "    \n",
    "    # begin gradient descent updates\n",
    "    if global_step > hypp.learning_starts:\n",
    "\n",
    "        print('update')\n",
    "        \n",
    "        data = rb.sample(hypp.batch_size)\n",
    "\n",
    "        # compute targets for Q networks\n",
    "        with torch.no_grad():\n",
    "            next_state_actions, next_state_log_pi, _ = agent.get_action_and_logprob(data.next_observations)\n",
    "            q1_next_target = agent.get_target_q1_value(data.next_observations, next_state_actions)\n",
    "            q2_next_target = agent.get_target_q2_value(data.next_observations, next_state_actions)\n",
    "            min_q_next_target = torch.min(q1_next_target, q2_next_target) - alpha * next_state_log_pi\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * hypp.gamma * (min_q_next_target).view(-1)\n",
    "\n",
    "        # compute error loss for both Q networks\n",
    "        q1_a_values = agent.get_q1_value(data.observations, data.actions).view(-1)\n",
    "        q2_a_values = agent.get_q2_value(data.observations, data.actions).view(-1)\n",
    "        q1_loss = F.mse_loss(q1_a_values, next_q_value)\n",
    "        q2_loss = F.mse_loss(q2_a_values, next_q_value)\n",
    "        q_loss = q1_loss + q2_loss\n",
    "\n",
    "        # optimize Q network\n",
    "        q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # gradient_step += 1\n",
    "\n",
    "        # improve policy using entropy regulalized policy update\n",
    "        if global_step % hypp.policy_frequency == 0:  # TD 3 Delayed update support\n",
    "            for _ in range(hypp.policy_frequency):  # compensate for the delay by doing 'hypp.policy_frequency' times instead of 1\n",
    "                pi, log_pi, _ = agent.get_action_and_logprob(data.observations)\n",
    "                q1_pi = agent.get_q1_value(data.observations, pi)\n",
    "                q2_pi = agent.get_q2_value(data.observations, pi)\n",
    "                min_q_pi = torch.min(q1_pi, q2_pi).view(-1)\n",
    "                policy_loss = ((alpha * log_pi) - min_q_pi).mean()\n",
    "\n",
    "                # optimize policy\n",
    "                actor_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                gradient_step += 1\n",
    "\n",
    "                # update alpha parameter\n",
    "                if hypp.autotune:\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi, _ = agent.get_action_and_logprob(data.observations)\n",
    "                    alpha_loss = (-log_alpha * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    # tune entropy temperature\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "        # update the target networks\n",
    "        if global_step % hypp.target_network_frequency == 0:\n",
    "            agent.update_target_networks(hypp.tau)\n",
    "\n",
    "        # log training losses to tensorboard\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"train/q1_values\", q1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"train/q2_values\", q2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"train/q1_loss\", q1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/q2_loss\", q2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/q_loss\", q_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"train/policy_loss\", policy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"hyperparameters/alpha\", alpha, global_step)\n",
    "            writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "            writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "            writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "            if hypp.autotune:\n",
    "                writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    last_evaluated_episode = episode_step\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvment of the model -\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path = True)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)\n",
    "\n",
    "envs.close()\n",
    "envs_eval.close()\n",
    "writer.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa634a-5a38-4836-afeb-a4c7914c2f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare Trained Agents and Display Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b890d2c-f07f-412f-b7bf-b935f30cdf17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display Trained Agent Behaviour\n",
    "\n",
    "Set `agent_name` and `agent_exp_type` to load the saved agent model in the respective log folder and generate a video of the agent's interaction with the gym environment. After the cell is executed, you should see a video embedding as output, and the video is also available in the directory: `/logs/<exp.exp_type>/<exp.run_name>/videos` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982f24f-d21d-4883-bab7-7796a659a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = exp.run_name\n",
    "agent_exp_type = exp.exp_type  # both are needed to identify the agent location\n",
    "\n",
    "\n",
    "exp_folder = \"\" if agent_exp_type is None else agent_exp_type\n",
    "filepath, _ = hf.create_folder_relative(f\"{exp_folder}/{agent_name}/videos\")\n",
    "\n",
    "hf.record_video(exp.env_id, agent_name, f\"{filepath}/best.mp4\", exp_type=agent_exp_type, greedy=True)\n",
    "Video(data=f\"{filepath}/best.mp4\", html_attributes='loop autoplay', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa638a-fa93-43e6-8203-4b68ae27b7af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Performance of Agents During Training\n",
    "\n",
    "During the training loop, if `exp.eval_agent = True`, the performance progress of the agent during its training is saved in a csv file. To compare the saved progress of different agents, create a `dict` containing the parent folder's name of each of the csv files and use the helper function `plotter_agents_training_stats`.\n",
    "\n",
    "To load the data, you can either set `eval_params.run_name00 = exp.run_name` (if only a `tracked_performance_training.csv` file for the corresponding `exp.run_name` exists) or manually enter the folder name containing the csv file. \n",
    "\n",
    "If the agent performance you want to load is inside an exp_type folder, set `eval_params.exp_type00` to experiment type, and if not, set it to `None`. \n",
    "\n",
    "You can add more than one experiment by initializing dict keys and values of the format `eval_params.run_namexx` and `eval_params.exp_typexx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474adef-84da-42d1-86dd-c38a87a99256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - evaluation settings for trained agent\n",
    "\n",
    "eval_params.run_name00 = exp.run_name\n",
    "eval_params.exp_type00 = exp.exp_type\n",
    "\n",
    "# eval_params.run_name01 = \"CartPole-v1__PPO__1__230302_224624\"\n",
    "# eval_params.exp_type01 = None\n",
    "\n",
    "# eval_params.run_name02 = \"CartPole-v1__PPO__1__230302_221245\"\n",
    "# eval_params.exp_type02 = None\n",
    "\n",
    "agent_labels = []\n",
    "\n",
    "episode_axis_limit = None\n",
    "\n",
    "hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df4217-f704-4cbc-8924-ffe30ce15820",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TensorBoard Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17554498-120b-434b-83de-7ae947d7062a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
