{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed12c1b3-ae50-43e6-adc3-7103e61108e2",
   "metadata": {},
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Ramil Sabirov\n",
    "\n",
    "Orignal Paper: [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783v2)\n",
    "\n",
    "Additional References:\n",
    "1. OpenAI A2C vs. A3C:\n",
    "    * [Blog post A2C/A3C](https://openai.com/blog/baselines-acktr-a2c/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd55142-31d3-4998-a6e0-23b7dd567920",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e29db0-e819-45e3-ae08-132cc8303350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'a2c-simple-adv.ipynb'\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2a19d-c255-46f7-b2e1-3ce76b269705",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cc925-afc7-41a7-ac1e-fa5d63dd0033",
   "metadata": {},
   "source": [
    "### Experiment Init\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook, we use a few `dicts`, one of which is `exp`. It is initialized in the following cell and has keys and values containing information about the experiment being run. Though the dict is initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ed13d-db86-4529-90ce-6c514377c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'A2C'  # algorithm name, in this case it should be 'A2C'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "set_random_seed = True  # set random seed for reproducibility\n",
    "exp.seed = 1\n",
    "\n",
    "# project name under which logs are saved on to Weights & Biases (if weights and biases logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix for files logged in this notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "if set_random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b1c9c-c434-4781-9f6b-a5848f6e1297",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The Agent class consists of the `critic` and `actor` models that are learned during training. Both models take the state as the input. The `actor` evaluates the state to a probability distribution of the action space. The `critic` evaluates to a scalar value that is seen as an approximation of the value function. In the following there are two different Agent classes. The first class, `Agent`, uses one MLP for the `critic` and another one for the `actor`. In the second, `Agent_shared`, the `actor` and `critic` share every layer except for the last.\n",
    "\n",
    "An Agent class has three methods: \n",
    "1. `get_action_and_value` evaluates the networks for `actor` and `critic` and returns an action sampled from the resulting probability distribution of the `actor` and the value approximation as returned by the `critic`. It also returns the logarithm of the action probability $\\log \\pi(a_t|s_t)$ which is used for obtaining gradient estimates for training, as well as the entropy of the probability distribution.\n",
    "2. `get_value` evalutes the network for the `critic` only and returns the resulting value function approximation.\n",
    "3. `get_action` valuates the network for the `actor` only to receive the probability distribution and either samples an action from that distribution (greedy = false) or returns the action with the maximal probability (greedy = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fcbff-6313-4b5d-a675-ed913dc882a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def get_action_and_value(self, x):\n",
    "        value, logits = self.value_network(x), self.policy_network(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), probs.entropy(), value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.value_network(x)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "\n",
    "        logits = self.policy_network(x)\n",
    "\n",
    "        if greedy:\n",
    "            action = logits.argmax(1)\n",
    "        else:\n",
    "            probs = Categorical(logits=logits)\n",
    "            action = probs.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffd634-4eff-4fce-bee0-a958573b0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_shared(nn.Module):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        # shared layers between actor and critic\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # linear layers for actor and critic\n",
    "        self.value_layer = nn.Linear(64, 1)\n",
    "        self.action_layer = nn.Linear(64, env.single_action_space.n)\n",
    "\n",
    "    def get_action_and_value(self, x):\n",
    "        out = self.network(x)\n",
    "        value, logits = self.value_layer(out), self.action_layer(out)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), probs.entropy(), value\n",
    "\n",
    "    def get_value(self, x):\n",
    "        out = self.network(x)\n",
    "        return self.value_layer(out)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        out = self.network(x)\n",
    "        logits = self.action_layer(out)\n",
    "\n",
    "        if greedy:\n",
    "            action = logits.argmax(1)\n",
    "        else:\n",
    "            probs = Categorical(logits=logits)\n",
    "            action = probs.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30db707-771c-4415-9558-80cc89c52c4d",
   "metadata": {},
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - Stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by default to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.4.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd215260-fe70-4f18-96da-39e3df829fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = True\n",
    "exp.num_envs = 4  # number of parallel game environments\n",
    "\n",
    "# flags to generate agent average performance during training.\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 20\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent learning specific flags\n",
    "hypp.total_timesteps = 300000\n",
    "\n",
    "hypp.num_steps = 5  # number of environment steps before optimization\n",
    "hypp.learning_rate = 3e-4  # learning rate used by the optimizer\n",
    "hypp.gamma = 0.99  # discount factor of future rewards\n",
    "hypp.norm_adv = True  # whether the advantages are normalized\n",
    "hypp.ent_coef = 0.01  # weighting coefficient for the entropy loss\n",
    "hypp.vf_coef = 0.5  # weighting coefficient for the value function loss\n",
    "hypp.max_grad_norm = 1  # the maximum norm of the gradients computed\n",
    "hypp.batch_size = int(exp.num_envs * hypp.num_steps)  # batch size for one step of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cc36c-5df4-4725-8f33-05242dc135ec",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent we first initalize the logging (based on the repsective flags in the `exp` dict), object of the `Agent` class and the optimizer, followed by an inital set of observations. \n",
    "\n",
    "\n",
    "Collect the trajectory i.e. executing the environment until it is finished, while keeping track of received rewards\n",
    "\n",
    "After that comes the main training loop which comprises of:  \n",
    "1. Run each parallel environment $k$ for `hypp.num_steps` steps and for each environment keep track of received rewards $R_t^k$ the logprobs $\\log\\pi_{\\theta}(A_t^k|S_t^k)$, the entropies of the probability distributions $H_t^k$ as well as the value function approximation $v_t^k$\n",
    "2. Compute advantage estimates $a_t^k$ for every time step using the generalized advantage estimation as well as the estimated return $g_t^k$\n",
    "3. Constructing the loss function $$L = L_{policy} + c_v * L_{value} + c_e * L_{entropy}$$ consisting of a term for the policy $$L_{policy} = -\\frac{1}{n*m}\\sum_{k=1}^{m}\\sum_{t=1}^{n}\\log\\pi_{\\theta}(A_t^k|S_t^k) * a_t^k,$$ a term for the value function $$L_{value} = \\frac{1}{n*m}\\sum_{k=1}^{m}\\sum_{t=1}^{n}\\frac{1}{2}(v_t^k - g_t^k)^2,$$ and a term for the entropy $$L_{entropy} = -\\frac{1}{n*m}\\sum_{k=1}^{m}\\sum_{t=1}^{n} H_t^k.$$ The coefficients $c_v$ and $c_e$ are managing the relative importance of the different terms.\n",
    "4. Perform gradient descent (possibly with clipped gradient). That simultaneously tries to increase the policy return, decrease the error of value function approximation and increase the entropy (to support exploration)\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "Note: we have two vectorised gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e239f06-e8ad-417f-b46f-67646a465a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinit run_name\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "# Create two vectorized envs: one to fill the rollout buffer with trajectories and\n",
    "# another to evaluate the agent performance at different stages of training\n",
    "envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])\n",
    "\n",
    "# Init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated\n",
    "greedy_evaluation = False  # whether to perform the evaluation in a greedy way or not\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "# Create Agent class instance and network optimizer\n",
    "agent = Agent(envs).to(device)\n",
    "# agent = Agent_shared(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "# Init observation to start learning\n",
    "gradient_step = 0\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "next_done = torch.zeros(exp.num_envs).to(device)\n",
    "num_updates = hypp.total_timesteps // hypp.batch_size\n",
    "\n",
    "pbar = notebook.tqdm(range(1, num_updates + 1))\n",
    "\n",
    "# training loop\n",
    "for update in pbar:\n",
    "\n",
    "    # initialize the rollout buffer\n",
    "    observations = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "    rewards = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "    dones = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "    values = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "    entropies = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)\n",
    "\n",
    "    # run environments for specified number of steps\n",
    "    for step in range(0, hypp.num_steps):\n",
    "        global_step += 1 * exp.num_envs\n",
    "        observations[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # get action from agent + bookkeeping\n",
    "        action, logprob, entropy, value = agent.get_action_and_value(next_obs)\n",
    "        values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        entropies[step] = entropy\n",
    "\n",
    "        # take action in environment\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        for item in info:\n",
    "            if \"episode\" in item.keys():\n",
    "                pbar.set_description(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                writer.add_scalar(\"rollout/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"rollout/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "                writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "                episode_step += 1\n",
    "                break\n",
    "\n",
    "        # evaluation of the agent\n",
    "        if exp.eval_agent and (episode_step % exp.eval_frequency == 0) and last_evaluated_episode != episode_step:\n",
    "            last_evaluated_episode = episode_step\n",
    "            tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,\n",
    "                                                                    exp.seed, greedy_actor=greedy_evaluation)\n",
    "            tracked_returns_over_training.append(tracked_return)\n",
    "            tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "            tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "            # if there has been improvement of the model - save model, create video, log video to wandb\n",
    "            if np.mean(tracked_return) > eval_max_return:\n",
    "                eval_max_return = np.mean(tracked_return)\n",
    "                # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "                hf.save_and_log_agent(exp, agent, episode_step,\n",
    "                                      greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    # calculate advantages and returns\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        returns = torch.zeros_like(rewards).to(device)\n",
    "        for t in reversed(range(hypp.num_steps)):\n",
    "            if t == hypp.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                returns[t] = rewards[t] + hypp.gamma * next_value * nextnonterminal\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                returns[t] = rewards[t] + hypp.gamma * returns[t+1] * nextnonterminal\n",
    "        advantages = returns - values\n",
    "\n",
    "    # update policy and value function\n",
    "    # normalizing advantages\n",
    "    if hypp.norm_adv:\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # flatten the batch\n",
    "    b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    b_entropies = entropies.reshape(-1)\n",
    "\n",
    "    # policy Loss\n",
    "    policy_loss = -(b_logprobs * b_advantages).mean()\n",
    "\n",
    "    # value Loss - MSE\n",
    "    value_loss = 0.5 * ((b_values - b_returns)**2).mean()\n",
    "\n",
    "    # entropy loss\n",
    "    entropy_loss = -b_entropies.mean()\n",
    "\n",
    "    total_loss = policy_loss + hypp.vf_coef * value_loss + hypp.ent_coef * entropy_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    # clipping the gradients\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), hypp.max_grad_norm)\n",
    "\n",
    "    optimizer.step()\n",
    "    gradient_step += 1\n",
    "\n",
    "    # calculating the explained variance\n",
    "    y_pred, y_true = b_values.cpu().detach().numpy(), b_returns.cpu().detach().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # logging metrics\n",
    "    writer.add_scalar(\"train/value_loss\", value_loss.item(), global_step)\n",
    "    writer.add_scalar(\"train/policy_loss\", policy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"train/entropy\", -entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"train/explained_variance\", explained_var, global_step)\n",
    "    writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvement of the model - save model, create video, log video to wandb\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training,\n",
    "                           tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)\n",
    "\n",
    "envs.close()\n",
    "writer.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode=\"disabled\")\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899fdc9-2678-44fb-97c3-b52ecdf62738",
   "metadata": {},
   "source": [
    "## Compare Trained Agents and Display Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6319c6a-c2c2-4ba7-a08b-25583632a969",
   "metadata": {},
   "source": [
    "### Display Trained Agent Behaviour\n",
    "\n",
    "Set `agent_name` and `agent_exp_type` to load the saved agent model in the respective log folder and generate a video of the agent's interaction with the gym environment. After the cell is executed, you should see a video embedding as output, and the video is also available in the directory: `/logs/<exp.exp_type>/<exp.run_name>/videos` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76526c0-c91e-4d73-9cea-1dab5e6dfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = exp.run_name\n",
    "agent_exp_type = exp.exp_type  # both are needed to identify the agent location\n",
    "\n",
    "\n",
    "exp_folder = \"\" if agent_exp_type is None else agent_exp_type\n",
    "filepath, _ = hf.create_folder_relative(f\"{exp_folder}/{agent_name}/videos\")\n",
    "\n",
    "hf.record_video(exp.env_id, agent_name, f\"{filepath}/best.mp4\", exp_type=agent_exp_type, greedy=True)\n",
    "Video(data=f\"{filepath}/best.mp4\", html_attributes='loop autoplay', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17063b4d-e720-48e0-8c9c-794e4fe28d9c",
   "metadata": {},
   "source": [
    "### Compare Performance of Agents During Training\n",
    "\n",
    "During the training loop, if `exp.eval_agent = True`, the performance progress of the agent during its training is saved in a csv file. To compare the saved progress of different agents, create a `dict` containing the parent folder's name of each of the csv files and use the helper function `plotter_agents_training_stats`.\n",
    "\n",
    "To load the data, you can either set `eval_params.run_name00 = exp.run_name` (if only a `tracked_performance_training.csv` file for the corresponding `exp.run_name` exists) or manually enter the folder name containing the csv file. \n",
    "\n",
    "If the agent performance you want to load is inside an exp_type folder, set `eval_params.exp_type00` to experiment type, and if not, set it to `None`. \n",
    "\n",
    "You can add more than one experiment by initializing dict keys and values of the format `eval_params.run_namexx` and `eval_params.exp_typexx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da2884-a923-467c-a243-2e7322fbdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - evaluation settings for trained agent\n",
    "\n",
    "eval_params.run_name00 = exp.run_name\n",
    "eval_params.exp_type00 = exp.exp_type\n",
    "\n",
    "# eval_params.run_name01 = \"CartPole-v1__PPO__1__230302_224624\"\n",
    "# eval_params.exp_type01 = None\n",
    "\n",
    "# eval_params.run_name02 = \"CartPole-v1__PPO__1__230302_221245\"\n",
    "# eval_params.exp_type02 = None\n",
    "\n",
    "agent_labels = []\n",
    "\n",
    "episode_axis_limit = None\n",
    "\n",
    "hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc8b05-049e-42ad-9329-25527cba602d",
   "metadata": {},
   "source": [
    "## TensorBoard Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdd762-ec90-407e-9bcf-d7767c449e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
