{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a210fd1",
   "metadata": {},
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Reinforce Implementation\n",
    "\n",
    "Notebook Authors: Ramil Sabirov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493b02b",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d58f9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:02:11.021018Z",
     "start_time": "2025-06-20T14:02:10.742989Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'reinforce.ipynb'\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a8223db4",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ddf45",
   "metadata": {},
   "source": [
    "### Experiment Init\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment. Although the dict is initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "id": "45b9b8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:02:11.497498Z",
     "start_time": "2025-06-20T14:02:11.095157Z"
    }
   },
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'REINFORCE'  # algorithm name, in this case it should be 'REINFORCE'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "set_random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = int(os.getenv(\"SEED\", 1))\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if set_random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = set_random_seed"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ebba383a",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of a deep MLP policy that is trained during training. The network takes as input the representation of the state, passes it through several hidden layers, and finally evaluates to a probability distribution over all actions with the `softmax` function.\n",
    "\n",
    "The `Agent` class has two methods:\n",
    "1. `get_action_logprob_and_value` evaluates the network and samples an action from the resulting probability distribution. It also returns the logarithm of the action probability $\\log \\pi(a_t| s_t)$ which is used for obtaining gradient estimates for training. In addition, it returns the current value estimate $\\hat{V}(s_t)$ of the state.\n",
    "\n",
    "2. `get_action` evaluates the network to the probability distribution and either samples an action from that distribution (greedy = false) or returns the action with the maximal probability (greedy = true)."
   ]
  },
  {
   "cell_type": "code",
   "id": "b80e1114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:02:11.939489Z",
     "start_time": "2025-06-20T14:02:11.570887Z"
    }
   },
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.p_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.v_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def get_action_logprob_and_value(self, x):\n",
    "\n",
    "        action_probs = self.p_network(x)\n",
    "        probs = Categorical(probs=action_probs)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), self.v_network(x)\n",
    "\n",
    "    def get_action(self, x, greedy=False):\n",
    "        action_probs = self.p_network(x)\n",
    "\n",
    "        if greedy:\n",
    "            action = action_probs.argmax(dim=1)\n",
    "        else:\n",
    "            probs = Categorical(probs=action_probs)\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "885b57a9",
   "metadata": {},
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - Stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by default to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.4.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `envs_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "id": "53f8844a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:02:12.335006Z",
     "start_time": "2025-06-20T14:02:12.007205Z"
    }
   },
   "source": [
    "hypp = edict()\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = bool(os.getenv(\"CAPTURE_VIDEO\", True))\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = bool(os.getenv(\"EVAL\", True))  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 20\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 300_000  # the training duration in number of time steps\n",
    "hypp.learning_rate = 3e-4  # the learning rate for the optimizer\n",
    "hypp.gamma = 0.99  # decay factor of future rewards"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "45b81a75",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent, we first initialize the logging (based on the respective flags in the `exp` dict), the object of the `Agent` class, and the optimizer, followed by an initial set of observations. \n",
    "\n",
    "\n",
    "After that comes the main training loop which is comprised of:  \n",
    "1. Collect the trajectory i.e. executing the environment until it is finished, while keeping track of received rewards\n",
    "2. Compute the returns $G_t$ for every step $t$ of the trajectory\n",
    "3. Compute the policy loss with baseline $L_p = -\\sum_{t=1}^{n}\\log \\pi_\\theta(a_t|s_t) * (G_t - \\hat{V}(s_t))$.\n",
    "4. Compute the value loss $L_v = 0.5\\sum_{t=1}^{n}(G_t-\\hat{V}(s_t))^2$\n",
    "5. Perform gradient descent (which is equivalent to gradient ascent regarding the gradient $\\frac{\\delta}{\\delta\\theta}{-L}$)\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "\n",
    "Note: we have two gym environments, `envs` and `envs_eval` in the initalizations. `envs` is used to fill the rollout buffer with trajectories and `envs_eval` is used to evaluate the agent performance at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "id": "0da39b1c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-06-20T14:10:40.841268Z",
     "start_time": "2025-06-20T14:02:12.403855Z"
    }
   },
   "source": [
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "env = hf.make_single_env(exp.env_id, exp.seed)\n",
    "envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])\n",
    "\n",
    "# init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "greedy_evaluation = False\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pbar = notebook.tqdm(total=hypp.total_timesteps)\n",
    "\n",
    "# the maximum number of steps an evironment is rolled out\n",
    "max_steps = 1000\n",
    "\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "while global_step < hypp.total_timesteps:\n",
    "\n",
    "    next_obs = env.reset()\n",
    "    rewards = torch.zeros(max_steps).to(device)\n",
    "    actions = torch.zeros((max_steps,) + env.action_space.shape).to(device)\n",
    "    obs = torch.zeros((max_steps, ) + env.observation_space.shape).to(device)\n",
    "    logprobs = torch.zeros(max_steps).to(device)\n",
    "    values = torch.zeros(max_steps).to(device)\n",
    "\n",
    "    episode_length = 0\n",
    "\n",
    "    # collect trajectory\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        episode_length = episode_length + 1\n",
    "        global_step = global_step + 1\n",
    "\n",
    "        next_obs = torch.tensor(next_obs).to(device)\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        # choose action according to agent network\n",
    "        action, log_prob, value = agent.get_action_logprob_and_value(next_obs)\n",
    "\n",
    "        # apply action to envs\n",
    "        next_obs, reward, done, info = env.step(action.cpu().item())\n",
    "\n",
    "        rewards[step] = torch.tensor(reward).to(device)\n",
    "        actions[step] = action\n",
    "        logprobs[step] = log_prob\n",
    "        values[step] = value\n",
    "\n",
    "        if done:\n",
    "            # episode has been finished\n",
    "            episode_step = episode_step + 1\n",
    "            break\n",
    "\n",
    "    # evaluate model\n",
    "    if (episode_step % exp.eval_frequency == 0) and exp.eval_agent:\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,\n",
    "                                                                exp.seed, greedy_actor=greedy_evaluation)\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        # if there has been improvement of the model\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            # call helper function - save model, create video, log video to wandb\n",
    "            hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)\n",
    "\n",
    "    # calculate returns\n",
    "    returns = torch.zeros(episode_length, device=device)\n",
    "    for t in reversed(range(episode_length)):\n",
    "\n",
    "        if t == episode_length-1:\n",
    "            returns[t] = rewards[t]\n",
    "        else:\n",
    "            returns[t] = returns[t+1] * hypp.gamma + rewards[t]\n",
    "\n",
    "    # calculate loss from tensors\n",
    "    policy_loss = torch.zeros((1,), device=device)\n",
    "    for t in range(episode_length):\n",
    "        policy_loss -= returns[t] * logprobs[t]\n",
    "        #SOLUTION GOES HERE\n",
    "\n",
    "    #SOLUTION GOES HERE\n",
    "\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    # logging information regarding agent performance\n",
    "    writer.add_scalar(\"rollout/episodic_return\", sum(rewards), global_step)\n",
    "    writer.add_scalar(\"rollout/episodic_length\", episode_length, global_step)\n",
    "\n",
    "    # logging information about the loss\n",
    "    writer.add_scalar(\"train/policy_loss\", policy_loss, global_step)\n",
    "    writer.add_scalar(\"train/value_loss\", value_loss, global_step)\n",
    "    writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "    writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    gradient_step = gradient_step + 1\n",
    "    pbar.update(min(episode_length, hypp.total_timesteps - pbar.n))\n",
    "    pbar.set_description(f\"episode={episode_step}, episodic_return={sum(rewards)}\")\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,\n",
    "                                                            exp.seed, greedy_actor=greedy_evaluation)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvement of the model - save model, create video, log video to wandb\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        # call helper function - save model, create video, log video to wandb\n",
    "        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=True)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training,\n",
    "                           tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)    \n",
    "\n",
    "env.close()\n",
    "writer.close()\n",
    "pbar.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode= 'disabled')\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Charts/episode_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>Charts/gradient_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇█</td></tr><tr><td>others/SPS</td><td>▁▃▃▂▄▄▄▄▅▅▆▆▅▅▅▅▅▅▆▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███▇</td></tr><tr><td>rollout/episodic_length</td><td>▁▁▁▁▁▂▁▁▂▂▂▁▂▂▃▃▂▃▃▄▃▃▆█▄▃▃▂▃▃▃▃▃▃▃▃▃▃▃▅</td></tr><tr><td>rollout/episodic_return</td><td>▁▁▁▁▂▁▁▁▂▁▂▂▂▂▃▃▃▄▆▇▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▄▅███</td></tr><tr><td>train/policy_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▂▂▂▂▃▄▂█▅▁▁▁▁▁▁▁▁▁▁▁▁▂▆</td></tr><tr><td>train/value_loss</td><td>▁▁▁▂▁▁▂▃▁▅▂▂▂▁▁▄▄▅▅▃▄▄██▇▃▃▂▂▂▂▂▂▂▂▂▂▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Charts/episode_step</td><td>185</td></tr><tr><td>Charts/gradient_step</td><td>184</td></tr><tr><td>global_step</td><td>29983</td></tr><tr><td>others/SPS</td><td>470</td></tr><tr><td>rollout/episodic_length</td><td>500</td></tr><tr><td>rollout/episodic_return</td><td>500</td></tr><tr><td>train/policy_loss</td><td>8295.238</td></tr><tr><td>train/value_loss</td><td>2104.7375</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CartPole-v1__REINFORCE__1__250620_160024</strong> at: <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/109gxq9a' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/109gxq9a</a><br> View project at: <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1</a><br>Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 1 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./logs/wandb/run-20250620_160026-109gxq9a/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/data/PycharmProjects/RLLBC-library/deep_examples/logs/wandb/run-20250620_160214-09o96cx8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/09o96cx8' target=\"_blank\">REINFORCE baseline high lr</a></strong> to <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/09o96cx8' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/09o96cx8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/300000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94b99fe398d74db1976d4d5d5b3b0d08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `quiet` argument to `wandb.run.finish()` is deprecated, use `wandb.Settings(quiet=...)` to set this instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Charts/episode_step</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>Charts/gradient_step</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆█</td></tr><tr><td>others/SPS</td><td>▃▂▃▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>rollout/episodic_length</td><td>▂▅▁▁▁▁▁▁▁▂▃▃▃▄▄▅▆▇██▅▂▂▂▃▅▆▇████████████</td></tr><tr><td>rollout/episodic_return</td><td>▃▂▃▅▅▁▁▁▁▁▁▁▁▃▃▃██▂▂▄▄▄▆▆▇▇█████████████</td></tr><tr><td>train/policy_loss</td><td>▂▂▂▅▅█▂▂▂▂▂▂▂▂▂▂▂▃▂▆▂▂▂▂▂▂▂▃▂▃▂▂▁▂▁▂▂▂▂▂</td></tr><tr><td>train/value_loss</td><td>█▇▆▁▁▁▁▁▁▁▂▂▃▄▅▇▇▆▅▂▂▁▁▁▁▂▂▄▂▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Charts/episode_step</td><td>1304</td></tr><tr><td>Charts/gradient_step</td><td>1303</td></tr><tr><td>global_step</td><td>300151</td></tr><tr><td>others/SPS</td><td>598</td></tr><tr><td>rollout/episodic_length</td><td>500</td></tr><tr><td>rollout/episodic_return</td><td>500</td></tr><tr><td>train/policy_loss</td><td>-684.769</td></tr><tr><td>train/value_loss</td><td>600.00037</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">REINFORCE baseline high lr</strong> at: <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/09o96cx8' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1/runs/09o96cx8</a><br> View project at: <a href='https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1' target=\"_blank\">https://wandb.ai/RLLBC-atari/RLLBC_CartPole-v1</a><br>Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 1 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./logs/wandb/run-20250620_160214-09o96cx8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b09fed1-8d04-4f24-b617-dc5e96049a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
