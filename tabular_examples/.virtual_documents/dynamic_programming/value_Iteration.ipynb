





import gymnasium as gym
import numpy as np
import custom_envs
import random
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
%matplotlib inline
from IPython.display import Video
from IPython.display import display
from screeninfo import get_monitors
from typing import Optional
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)





class MarkovDecisionProcess():
    def __init__(self, env):
        self.num_states = env.observation_space.n
        self.P = env.unwrapped.P
        self.actions_per_state = [] # list containing the actions available per state
        for state in self.P:
            actions = list(self.P[state].keys())
            self.actions_per_state.append(actions)





class Agent():
    def __init__(self, mdp, gamma=0.9, update_threshold=1e-6):
        self.mdp = mdp
        self.update_threshold = update_threshold # stopping distance as criteria for stopping policy evaluation
        self.state_value_fn = np.zeros(self.mdp.num_states) # a table leading from state to value expectations
        # Create random policy
        self.policy = []
        for state in range(self.mdp.num_states):
            random_entry = random.randint(0, len(self.mdp.actions_per_state[state])-1)
            self.policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])
            self.policy[state][random_entry] = 1
        self.gamma = gamma # discount rate for return

    def get_greedy_action(self, state):
        # Choose action based on the probabilities defined within the policy
        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]))))
        return action

    def train(self, in_place=True):
        sweeps = 0
        stable = False
        while not stable:
            delta = 0
            if in_place:
                sweeps += 1
                # Update the value function by maximizing over actions
                for state in range(self.mdp.num_states):
                    old_state_value = self.state_value_fn[state]
                    action_values = []
                    for action in range(len(self.mdp.actions_per_state[state])):
                        action_values.append(self.get_action_value(state, action))
                    action_value_max = max(action_values)
                    # Update value function if the value is higher
                    self.state_value_fn[state] = max(action_value_max, old_state_value)
                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))
                if delta < self.update_threshold:
                    stable = True
            elif not in_place:
                new_state_value_fn = np.copy(self.state_value_fn)
                sweeps += 1
                # Update the value function by maximizing over actions
                for state in range(self.mdp.num_states):
                    # Find max action value for value function
                    action_values = []
                    for action in range(len(self.mdp.actions_per_state[state])):
                        action_values.append(self.get_action_value(state, action))
                    action_value_max = max(action_values)
                    # Update value function if the value is higher
                    new_state_value_fn[state] = max(action_value_max, self.state_value_fn[state])
                    delta = max(delta, np.abs(self.state_value_fn[state] - new_state_value_fn[state]))
                if delta < self.update_threshold:
                    stable = True
                self.state_value_fn = new_state_value_fn
        # Extract optimal policy
        self.extract_policy()
        self.visualize()
        print('Sweeps required for convergence ', str(sweeps))

    def get_action_value(self, state, action):
        # Value expectation without considering the policy
        action_value = 0
        for transition in self.mdp.P[state][action]:
            transition_prob = transition[0]  # prob of next state
            successor_state = transition[1]  # value/name of next state
            reward = transition[2]  # reward of next state
            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])
        return action_value

    def extract_policy(self):
        best_policy = []
        for state in range(self.mdp.num_states):
            best_policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])
            # Calculate best possible policy based on current value function
            action_values = []
            for action in range(len(self.mdp.actions_per_state[state])):
                action_values.append(self.get_action_value(state, action))
            best_actions = np.where(action_values == max(action_values))[0]
            for index in best_actions:
                best_policy[state][index] = 1
            best_policy[state] = [best_policy[state][action] / len(best_actions)
                                  for action in self.mdp.actions_per_state[state]]
            self.policy[state] = best_policy[state]





def visualize(self):
    x_axis = 1
    y_axis = 2
    X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))
    fig, ax = plt.subplots(1, 1)
    cmap = matplotlib.colormaps.get_cmap("Blues_r")
    cmap.set_under("black")
    img = ax.imshow(X1, interpolation="nearest", vmin=-2.5, vmax=max(self.state_value_fn), cmap=cmap)
    ax.axis('off')
    for i in range(x_axis):
        for j in range(y_axis):
            ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')
    plt.show()
    self.render_policy()

def render_policy(self):
    print('Policy of the agent:')
    out = ' | '
    render = out
    for i in range(self.mdp.num_states):
        token = ""
        if self.policy[i][0] > 0:   # search
            token += "Search"
        if self.policy[i][1] > 0:   # wait
            if token != "":
                token += " or Wait"
            else:
                token += "Wait"
        if len(self.policy[i]) > 2:
            if self.policy[i][2] > 0:   # recharge
                if token != "":
                    token += " and Recharge"
                else:
                    token += "Recharge"
        render += token + out
    print(render)

def evaluate(self, env, file, num_runs=5):
    frames = []  # collect rgb_image of agent env interaction
    for _ in range(num_runs):
        done = False
        obs, info = env.reset()
        out = env.render()
        frames.append(out)
        while not done:
            action = self.get_greedy_action(obs)
            obs, reward, done, truncated, info = env.step(action)
            out = env.render()
            for i in range(len(out)):
                frames.append([out[i]])
    # create animation out of saved frames
    if all(frame is not None for frame in frames):
        fig = plt.figure(figsize=(10, 6))
        plt.axis('off')
        img = plt.imshow(frames[0][0])
        def animate(index):
            img.set_data(frames[index][0])
            return [img]
        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)
        plt.close()
        anim.save(file, writer="ffmpeg", fps=3) 

setattr(Agent, 'visualize', visualize)
setattr(Agent, 'render_policy', render_policy)
setattr(Agent, 'evaluate', evaluate)





env = gym.make('RecyclingRobot-v1', render_mode='text')
env.reset()
mdp = MarkovDecisionProcess(env) # in our case contains dynamics function
agent = Agent(mdp, gamma=0.9, update_threshold=0.05)
agent.visualize()





agent.train()





env = gym.make('RecyclingRobot-v1', render_mode='rgb_array', render_type='node')
num_runs=5
video_file_1 = "value_iteration.mp4"
agent.evaluate(env, video_file_1, num_runs)
Video(video_file_1, html_attributes="loop autoplay")
