{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41b88be-31b3-4ea3-9563-47058d0d2880",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c429c4-dfcb-46af-9695-c5ccd62c1c7d",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa671843-9406-4e96-8cc7-d51c7084f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc384a04-e075-4e03-827f-ce0c0b5aaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, env):\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.P = env.unwrapped.P\n",
    "        self.actions_per_state = [] # list containing the actions available per state\n",
    "        for state in self.P:\n",
    "            actions = list(self.P[state].keys())\n",
    "            self.actions_per_state.append(actions)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, mdp, gamma=0.9, update_threshold=1e-6):\n",
    "        self.mdp = mdp # contains state transition function, num_states and actions\n",
    "        self.update_threshold = update_threshold # stopping distance as criteria for stopping policy evaluation\n",
    "        self.state_value_fn = np.zeros(self.mdp.num_states) # a table leading from state to value expectations\n",
    "        # Create random policy\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            random_entry = random.randint(0, len(self.mdp.actions_per_state[state])-1)\n",
    "            self.policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.gamma = gamma # discount rate for return\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        # Choose action based on the probabilities defined within the policy\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]), rtol=0.01)))\n",
    "        return action\n",
    "\n",
    "    def train(self, in_place=False):\n",
    "        policy_stable = False\n",
    "        iteration = 0\n",
    "        total_sweeps = 0\n",
    "        while not policy_stable:\n",
    "            iteration += 1\n",
    "            # Policy Evaluation\n",
    "            total_sweeps += self.policy_evaluation()\n",
    "            # Policy Improvement\n",
    "            policy_stable = self.policy_improvement()\n",
    "        print('Sweeps required for convergence ', str(total_sweeps))\n",
    "        print('Iterations required for convergence ', str(iteration))\n",
    "\n",
    "    def policy_evaluation(self, in_place=True): # in_place version\n",
    "        sweeps = 0\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            delta = 0\n",
    "            sweeps += 1\n",
    "            if in_place:\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    self.state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "            else:\n",
    "                new_state_value_fn = np.copy(self.state_value_fn)\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    new_state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "                self.state_value_fn = new_state_value_fn\n",
    "        return sweeps\n",
    "\n",
    "    def get_state_value(self, state, action):\n",
    "        # Value expectation considering the policy\n",
    "        policy_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            policy_value += self.policy[state][action] * transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return policy_value\n",
    "    \n",
    "    def get_action_value(self, state, action):\n",
    "        # Value expectation without considering the policy\n",
    "        action_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return action_value\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        current_policy = self.policy # Cache current policy\n",
    "        best_policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            best_policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                action_values.append(self.get_action_value(state, action))\n",
    "            best_actions = np.where(action_values == max(action_values))[0]\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in range(len(self.mdp.actions_per_state[state]))]\n",
    "            # If the current policy is not the best policy, update it\n",
    "            if not np.array_equal(current_policy[state], best_policy[state]):\n",
    "                policy_stable = False\n",
    "                self.policy[state] = best_policy[state]\n",
    "        self.visualize()\n",
    "        return policy_stable\n",
    "\n",
    "    # for frozen_lake:\n",
    "    def visualize(self):\n",
    "        print('Value function:\\n', np.asmatrix(self.state_value_fn))\n",
    "        x_axis = 4\n",
    "        y_axis = 4\n",
    "        X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        cmap = cm.get_cmap(\"Blues_r\")\n",
    "        cmap.set_under(\"black\")\n",
    "        img = ax.imshow(X1, interpolation=\"nearest\", vmin=0, vmax=1, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "        for i in range(x_axis):\n",
    "            for j in range(y_axis):\n",
    "                ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "        plt.show()\n",
    "        self.render_policy()\n",
    "\n",
    "    def render_policy(self):\n",
    "        print('Policy of the agent:')\n",
    "        print('-----------------------------')\n",
    "        out = '| '\n",
    "        for i in range(self.mdp.num_states):\n",
    "            token = \"\"\n",
    "            if self.policy[i][3] > 0:   # left\n",
    "                token += \"\\u2190\"\n",
    "            if self.policy[i][0] > 0:   # up\n",
    "                token += \"\\u2191\"\n",
    "            if self.policy[i][1] > 0:   # down\n",
    "                token += \"\\u2193\"\n",
    "            if self.policy[i][2] > 0:   # right\n",
    "                token += \"\\u2192\"\n",
    "            if token == \"\":     # empty\n",
    "                token += \"  \"\n",
    "            if len(token) == 1:\n",
    "                token += '  '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 2:\n",
    "                token += ' '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 3:\n",
    "                token += ' '\n",
    "            out += token + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                print('-----------------------------')\n",
    "                out = '| '\n",
    "    \n",
    "\n",
    "    \"\"\" # for basketball: \n",
    "    def visualize(self):\n",
    "        print('Value function:\\n', np.asmatrix(self.state_value_fn))\n",
    "        x_axis = 1\n",
    "        y_axis = self.mdp.num_states - 2\n",
    "        vmin = min(self.state_value_fn)\n",
    "        vmax = max(self.state_value_fn)\n",
    "        X1 = np.reshape(self.state_value_fn[:-2], (x_axis, y_axis))\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        cmap = plt.colormaps[\"Blues_r\"]\n",
    "        cmap.set_under(\"black\")\n",
    "        img = ax.imshow(X1, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(\"Values of the state value function on the field\")\n",
    "        for i in range(x_axis):\n",
    "            for j in range(y_axis):\n",
    "                ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "        plt.show()\n",
    "        self.render_policy()\n",
    "\n",
    "    def render_policy(self):\n",
    "        print('Policy of the agent:')\n",
    "        out = ' | '\n",
    "        render = out\n",
    "        for i in range(self.mdp.num_states-2):\n",
    "            token = \"\"\n",
    "            if self.policy[i][0] > 0:   # move\n",
    "                token += \"Move\"\n",
    "            if self.policy[i][1] > 0:   # up\n",
    "                token += \"Throw\"\n",
    "            if len(token) > 5:\n",
    "                token = 'Move or Throw'\n",
    "            render += token + out\n",
    "        print(render) \"\"\"\n",
    "\n",
    "    \"\"\" for recycling_bot: \n",
    "    def visualize(self):\n",
    "        print('Value function:\\n', np.asmatrix(self.state_value_fn))\n",
    "        x_axis = 1\n",
    "        y_axis = 2\n",
    "        X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        cmap = cm.get_cmap(\"Blues_r\")\n",
    "        cmap.set_under(\"black\")\n",
    "        img = ax.imshow(X1, interpolation=\"nearest\", vmin=0, vmax=max(self.state_value_fn), cmap=cmap)\n",
    "        ax.axis('off')\n",
    "        for i in range(x_axis):\n",
    "            for j in range(y_axis):\n",
    "                ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "        plt.show()\n",
    "        self.render_policy()\n",
    "\n",
    "    def render_policy(self):\n",
    "        print('Policy of the agent:')\n",
    "        out = ' | '\n",
    "        render = out\n",
    "        for i in range(self.mdp.num_states):\n",
    "            token = \"\"\n",
    "            if self.policy[i][0] > 0:   # search\n",
    "                token += \"Search\"\n",
    "            if self.policy[i][1] > 0:   # wait\n",
    "                if token != \"\":\n",
    "                    token += \" or Wait\"\n",
    "                else:\n",
    "                    token += \"Wait\"\n",
    "            if len(self.policy[i]) > 2:\n",
    "                if self.policy[i][2] > 0:   # recharge\n",
    "                    if token != \"\":\n",
    "                        token += \" and Recharge\"\n",
    "                    else:\n",
    "                        token += \"Recharge\"\n",
    "            render += token + out\n",
    "        print(render) \"\"\"\n",
    "    \n",
    "    def evaluate(self, env, num_runs=1):\n",
    "        tot_reward = [0]\n",
    "        for _ in range(num_runs):\n",
    "            done = False\n",
    "            obs, info = env.reset()\n",
    "            reward_per_run = 0\n",
    "            while not done:\n",
    "                action = self.get_greedy_action(obs)\n",
    "                obs, reward, done, _ , info = env.step(action)\n",
    "                reward_per_run += reward\n",
    "            tot_reward.append(reward_per_run + tot_reward[-1])\n",
    "        return tot_reward\n",
    "    \n",
    "        \n",
    "########################################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # for frozen_lake: \n",
    "    # Evaluation of the random policy.\n",
    "    env = gym.make('CustomFrozenLake-v1', render_mode='human', is_slippery=False)\n",
    "    env.reset()\n",
    "    mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "    agent = Agent(mdp, gamma=0.9, update_threshold=0.0001)\n",
    "\n",
    "    # Train the agent.\n",
    "    input(\"Press Enter to train the agent\")\n",
    "    agent.train()\n",
    "\n",
    "    # Evaluation of the new policy.\n",
    "    print('Sampling from final policy...')\n",
    "    num_runs=4\n",
    "    agent.evaluate(env, num_runs=num_runs)\n",
    "\n",
    "    \n",
    "    \"\"\" for basketball: \n",
    "    env = gym.make('CustomBasketBall-v1', render_mode=None, field_length=6, line_position = 2)\n",
    "    env.reset()\n",
    "    mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "    agent = Agent(mdp, gamma=1.0)\n",
    "\n",
    "    print(env.P)\n",
    "\n",
    "    # Train the agent\n",
    "    input(\"Press Enter to start\")\n",
    "    num_runs=10000\n",
    "    random_results = agent.evaluate(env, num_runs=num_runs)\n",
    "    input(\"Press Enter to train the agent\")\n",
    "    agent.train()\n",
    "\n",
    "    # Evaluate the agent\n",
    "    input(\"Press Enter to evaluate the agent\")\n",
    "    env = gym.make('CustomBasketBall-v1', render_mode='human', field_length=12, line_position = 6)\n",
    "    trained_results = agent.evaluate(env, num_runs=num_runs)\n",
    "\n",
    "    # Comparing both agents\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "    ax.plot(range(num_runs + 1), random_results, label=\"Random Agent\")\n",
    "    ax.plot(range(num_runs + 1), trained_results, label=\"Trained Agent\")\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Cumulative Reward')\n",
    "    ax.legend()\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\" for recycling_bot:\n",
    "    env = gym.make('RecyclingRobot-v1', render_mode='text')\n",
    "    env.reset()\n",
    "    mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "    agent = Agent(mdp, gamma=0.9, update_threshold=0.05)\n",
    "\n",
    "    # Train the agent.\n",
    "    input(\"Press Enter to train the agent\")\n",
    "    agent.train()\n",
    "\n",
    "    # Evaluation of the new policy.\n",
    "    print('Sampling from final policy...')\n",
    "    num_runs=4\n",
    "    agent.evaluate(env, num_runs=num_runs)\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
