{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68e6e68-4d81-4f03-8779-1f6f644fe614",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use the value iteration algorithm to find an optimal policy for the recycling bot from \"Reinforcement Learning: An Introduction.\" An overview of the MDP that belongs to the recycling bot is shown below. The robot has a battery, whose state can be high or low. When having a high battery level, the agent can search or wait for cans. When being in a low battery state, the agent can also recharge, to prevent running out of battery.\n",
    "\n",
    "<img src=\"./utils/recycling_bot.jpg\" alt=\"Value Iteration algorithm\" width=\"500\">\n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8b9bc-8c00-45a7-8838-5352ffcf1ce5",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4859e2ce-14cd-4b11-a3df-d7c8f856167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08836c9-7170-442b-a2a8-d5c7fdc45c58",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the MDP\n",
    "\n",
    "Dynamic Programming (DP) algorithms are a class of algorithms capable of finding the optimal policy for an environment, given a model of the environment. Below, we implement such a model in the class `MarkovDecisionProcess`. The class identifies the number of states in `self.num_states`, as well as the state dependent actions in `self.actions_per_state`. Furthermore, it contains the state transition probabilities, as well as the successor states and rewards in `self.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3506c82-8740-418f-8c84-26de0f5203e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, env):\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.P = env.unwrapped.P\n",
    "        self.actions_per_state = [] # list containing the actions available per state\n",
    "        for state in self.P:\n",
    "            actions = list(self.P[state].keys())\n",
    "            self.actions_per_state.append(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7e588-f1de-4785-a631-7d8ea521d4d7",
   "metadata": {},
   "source": [
    "### Initializing the Agent\n",
    "The DP Agent uses the MDP to obtain an optimal policy via the optimization algorithm. This is done using the method `train`. In this example, we consider value iteration. The idea of the algorithm is to obtain an estimate of the value function through iterative computations, and, to use the value function estimate to obtain an optimal policy. Below, we provide an overview of the algorithm.\n",
    "\n",
    "<img src=\"./utils/value_iteration.png\" alt=\"Value Iteration algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Since we are trying to find a policy for the recycling bot, we have implemented the method `train` with a state-dependent action space. This is rather unusual, as most environments are based on uniform action spaces. Furthermore, the algorithm also offers the option of using in-place updates for policy evaluation. For further details, we refer to the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9659a96-832b-4f43-92c4-5badb461e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, mdp, gamma=0.9, update_threshold=1e-6):\n",
    "        self.mdp = mdp\n",
    "        self.update_threshold = update_threshold # stopping distance as criteria for stopping policy evaluation\n",
    "        self.state_value_fn = np.zeros(self.mdp.num_states) # a table leading from state to value expectations\n",
    "        # Create random policy\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            random_entry = random.randint(0, len(self.mdp.actions_per_state[state])-1)\n",
    "            self.policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.gamma = gamma # discount rate for return\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        # Choose action based on the probabilities defined within the policy\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]))))\n",
    "        return action\n",
    "\n",
    "    def train(self, in_place=True):\n",
    "        sweeps = 0\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            delta = 0\n",
    "            if in_place:\n",
    "                sweeps += 1\n",
    "                # Update the value function by maximizing over actions\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    action_values = []\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        action_values.append(self.get_action_value(state, action))\n",
    "                    action_value_max = max(action_values)\n",
    "                    # Update value function if the value is higher\n",
    "                    self.state_value_fn[state] = max(action_value_max, old_state_value)\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "            elif not in_place:\n",
    "                new_state_value_fn = np.copy(self.state_value_fn)\n",
    "                sweeps += 1\n",
    "                # Update the value function by maximizing over actions\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    # Find max action value for value function\n",
    "                    action_values = []\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        action_values.append(self.get_action_value(state, action))\n",
    "                    action_value_max = max(action_values)\n",
    "                    # Update value function if the value is higher\n",
    "                    new_state_value_fn[state] = max(action_value_max, self.state_value_fn[state])\n",
    "                    delta = max(delta, np.abs(self.state_value_fn[state] - new_state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "                self.state_value_fn = new_state_value_fn\n",
    "        # Extract optimal policy\n",
    "        self.extract_policy()\n",
    "        self.visualize()\n",
    "        print('Sweeps required for convergence ', str(sweeps))\n",
    "\n",
    "    def get_action_value(self, state, action):\n",
    "        # Value expectation without considering the policy\n",
    "        action_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return action_value\n",
    "\n",
    "    def extract_policy(self):\n",
    "        best_policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            best_policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                action_values.append(self.get_action_value(state, action))\n",
    "            best_actions = np.where(action_values == max(action_values))[0]\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in self.mdp.actions_per_state[state]]\n",
    "            self.policy[state] = best_policy[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0a547-f715-4904-95d9-9bfd184bae0d",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluating the trained agent, we implement methods that can show us the value function (`visualize`) and the policy (`render_policy`). We also add the method `evaluate`, which allows us to show samples from an agent's policy in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f418948f-3dd9-4647-8afc-159ca53e03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self):\n",
    "    x_axis = 1\n",
    "    y_axis = 2\n",
    "    X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    cmap = matplotlib.colormaps.get_cmap(\"Blues_r\")\n",
    "    cmap.set_under(\"black\")\n",
    "    img = ax.imshow(X1, interpolation=\"nearest\", vmin=-2.5, vmax=max(self.state_value_fn), cmap=cmap)\n",
    "    ax.axis('off')\n",
    "    for i in range(x_axis):\n",
    "        for j in range(y_axis):\n",
    "            ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "    plt.show()\n",
    "    self.render_policy()\n",
    "\n",
    "def render_policy(self):\n",
    "    print('Policy of the agent:')\n",
    "    out = ' | '\n",
    "    render = out\n",
    "    for i in range(self.mdp.num_states):\n",
    "        token = \"\"\n",
    "        if self.policy[i][0] > 0:   # search\n",
    "            token += \"Search\"\n",
    "        if self.policy[i][1] > 0:   # wait\n",
    "            if token != \"\":\n",
    "                token += \" or Wait\"\n",
    "            else:\n",
    "                token += \"Wait\"\n",
    "        if len(self.policy[i]) > 2:\n",
    "            if self.policy[i][2] > 0:   # recharge\n",
    "                if token != \"\":\n",
    "                    token += \" and Recharge\"\n",
    "                else:\n",
    "                    token += \"Recharge\"\n",
    "        render += token + out\n",
    "    print(render)\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_greedy_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            for i in range(len(out)):\n",
    "                frames.append([out[i]])\n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0][0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index][0])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=3) \n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'render_policy', render_policy)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26395f8e-5def-412b-8317-51dbb273e7ef",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier and evaluate its performance.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium`, and extract its properties via our `MarkovDecisionProcess` class. Then, we can set up an Agent that we can train. We see that our initial Value function only contains zeros (state \"low\" is displayed in the left square; state \"high\" is displayed in the right square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838471ae-cabb-4812-94f4-9926c81c53bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAEMCAYAAABZZbUfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHtklEQVR4nO3cX2jd5R3H8W9soMWGMVOpGmRlTBadqRY6m3Vs82JQ5pRunVNb/+BUCsJQihMZVCfDbWwMBrtxkzFZaG0zZa3V0ipYiwqC9c/NktgieiFUoZBTKKFYOOHswlgMJmnc2WmGn9fr8jm/Jzzn4nvyPj+SX1er1WoVABDrnIU+AACwsMQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOG653vhR81OHgMA6IQl8/hN784AAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEQJCJiYm6/74t9dWv9NWXe5bU4OpV9eQ/h+e199ixY7X5zp/VxReeX71fOreu/s7aOvjigQ6fGJgv8007xECQjTf8pJ7YNlRbH3y49uzdX6u/eVXdfuumGt65Y859p06dqh+u+34dPHig/vinP9dTu/bU8gsuqPXX/qBeefmls3R6YC7mm3Z0tVqt1nwu/KjZ6aPQSc/t31cb1l9b/9i2o27auOn0+nXXrKuxsdF65733a9GiRTPufewvj9aWe39eB19+tb61dm1VVTWbzVqz+spaurSnXnn1tbPyHoCZmW/msqT7zNe4MxDimad3V09PT13/0xumrd92+x314Qcf1KHXZh/4Z/bsrq/395/+oKiq6u7urk0331pvvH6ojh492rFzA2dmvmmXGAgxOjpS/ZdeVt3d0xNx5corqqpqbHRk1r1joyM1MHXdp32y9vbY6P/wpMDnZb5plxgI0WiMV29v72fWz5taazTGZ907Pj5evefNsHdqbXx89r1A55lv2iUGgnR1dc314n+9d86fC5wV5pt2iIEQvb3LZiz8443Gx6/P8M3gE8uWLavxGb5ZHD8+tXeGbyTA2WO+aZcYCDEwsLKOHH67ms3p/xYyMvLvqqr6xuUDs+69fGBljU5d93n3Ap1nvmmXGAix/scbamJionbv+te09Se2DdVFfX21ZnBw9r0/2lBHDh+e9hfJzWazhndsr6vWDFZfX1/Hzg2cmfmmXZ4zEOS6a9bVW2++Ub/53R/qa5dcUk8O76zH//63enxoe226+Zaqqrp78121fdtQjR55t1asWFFVHz+U5NuDq+vEiRP1yG9/X8uXL6/H/vpo7dv7bO17/oX67veuXsi3BZT5Znbzec7APC7hi2L4qV318ENb65Ff/6oajUb1919aQ9t31o03bTx9zeTkZE1OTlZ9qhEXL15c+54/UFt/+UD9Yss9dfLkybriylW1Z+9+HxTwf8J80w53BgDgC8wTCAGAMxIDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQrqvVarUW+hAAwMJxZwAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAML9B4uHzIG1I5ioAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Wait | Search | \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='text')\n",
    "env.reset()\n",
    "mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "agent = Agent(mdp, gamma=0.9, update_threshold=0.05)\n",
    "agent.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668b15f-ea20-404a-ae3a-5092b33c042d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085b8204-db3f-4718-8498-8952781a6dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Search | \n",
      "Sweeps required for convergence  34\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260498a-8972-4e61-b0d0-025bb4f194e2",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The solution for the environment is a fairly simple behavior policy. If the agent is in the state \"high\", it searches for cans; if the state is \"low\", it recharges itself. This way, the agent receives the highest possible return. Below we show the resulting behavior highlighted in the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7961f1-46a6-43db-907c-ebaa2906ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='rgb_array', render_type='node')\n",
    "num_runs=5\n",
    "video_file_1 = \"value_iteration.mp4\"\n",
    "agent.evaluate(env, video_file_1, num_runs)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
