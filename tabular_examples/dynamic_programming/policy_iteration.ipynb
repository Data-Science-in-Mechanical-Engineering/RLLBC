{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41b88be-31b3-4ea3-9563-47058d0d2880",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use the policy iteration algorithm to find an optimal policy for the recycling bot from \"Reinforcement Learning: An Introduction.\" An overview of the MDP that belongs to the recycling bot is shown below. The robot has a battery, whose state can be high or low. When having a high battery level, the agent can search or wait for cans. When being in a low battery state, the agent can also recharge, to prevent running out of battery.\n",
    "\n",
    "<img src=\"./utils/recycling_bot.jpg\" alt=\"Policy Iteration algorithm\" width=\"500\">\n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c429c4-dfcb-46af-9695-c5ccd62c1c7d",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa671843-9406-4e96-8cc7-d51c7084f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6beaf-b124-4cc0-aa3d-90e3035e8c9a",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the MDP\n",
    "\n",
    "Dynamic Programming (DP) algorithms are a class of algorithms capable of finding the optimal policy for an environment, given a model of the environment. Below, we implement such a model in the class `MarkovDecisionProcess`. The class identifies the number of states in `self.num_states`, as well as the state dependent actions in `self.actions_per_state`. Furthermore, it contains the state transition probabilities, as well as the successor states and rewards in `self.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc384a04-e075-4e03-827f-ce0c0b5aaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, env):\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.P = env.unwrapped.P\n",
    "        self.actions_per_state = [] # list containing the actions available per state\n",
    "        for state in self.P:\n",
    "            actions = list(self.P[state].keys())\n",
    "            self.actions_per_state.append(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bbf43-745b-4deb-bc32-77deb21f0f12",
   "metadata": {},
   "source": [
    "### Initializing the Agent\n",
    "The DP Agent uses the MDP to obtain an optimal policy via the optimization algorithm. This is done using the method `train`. In this example, we consider policy iteration. The algorithm obtains an optimal policy by switching between the evaluation of a policy to obtain a value function, and using the value function to improve the policy. This is done until the policy converges. Below, we provide an overview of the algorithm.\n",
    "\n",
    "<img src=\"./utils/policy_iteration.png\" alt=\"Policy Iteration algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Since we are trying to find a policy for the recycling bot, we have implemented the method `train` with a state-dependent action space. This is rather unusual, as most environments are based on uniform action spaces.\n",
    "Furthermore, the algorithm also offers the option of using in-place updates for policy evaluation. For further details, we refer to the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5397e837-4d05-4861-a46c-499516e3059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, mdp, gamma=0.9, update_threshold=1e-6):\n",
    "        self.mdp = mdp\n",
    "        self.update_threshold = update_threshold # stopping criterium for the updates of policy and value function\n",
    "        self.state_value_fn = np.zeros(self.mdp.num_states) # table leading from state to value expectations\n",
    "        # Create random policy\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            random_entry = random.randint(0, len(self.mdp.actions_per_state[state])-1)\n",
    "            self.policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.gamma = gamma # discount factor\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        # Choose action based on the probabilities defined within the policy\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]), rtol=0.01)))\n",
    "        return action\n",
    "\n",
    "    def train(self, in_place=False):\n",
    "        policy_stable = False\n",
    "        iteration = 0\n",
    "        total_sweeps = 0\n",
    "        while not policy_stable:\n",
    "            iteration += 1\n",
    "            # Policy Evaluation\n",
    "            total_sweeps += self.policy_evaluation(in_place)\n",
    "            # Policy Improvement\n",
    "            policy_stable = self.policy_improvement()\n",
    "        print('Sweeps required for convergence ', str(total_sweeps))\n",
    "        print('Iterations required for convergence ', str(iteration))\n",
    "\n",
    "    def policy_evaluation(self, in_place=True): # in_place version\n",
    "        sweeps = 0\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            delta = 0\n",
    "            sweeps += 1\n",
    "            if in_place:\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    self.state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "            else:\n",
    "                new_state_value_fn = np.copy(self.state_value_fn)\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    new_state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "                self.state_value_fn = new_state_value_fn\n",
    "        return sweeps\n",
    "\n",
    "    def get_state_value(self, state, action):\n",
    "        # Value expectation considering the policy\n",
    "        policy_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            policy_value += self.policy[state][action] * transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return policy_value\n",
    "    \n",
    "    def get_action_value(self, state, action):\n",
    "        # Value expectation without considering the policy\n",
    "        action_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return action_value\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        current_policy = self.policy # Cache current policy\n",
    "        best_policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            best_policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                action_values.append(self.get_action_value(state, action))\n",
    "            best_actions = np.where(action_values == max(action_values))[0]\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in range(len(self.mdp.actions_per_state[state]))]\n",
    "            # If the current policy is not the best policy, update it\n",
    "            if not np.array_equal(current_policy[state], best_policy[state]):\n",
    "                policy_stable = False\n",
    "                self.policy[state] = best_policy[state]\n",
    "        self.visualize()\n",
    "        return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b45e5-0d10-4801-a777-200073f6ed37",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluating the trained agent, we implement methods that can show us the value function (`visualize`) and the policy (`render_policy`). We also add the method `evaluate`, which allows us to show samples from an agent's policy in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a3cbd9-f6ca-43de-bca3-276f4a6047ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self):\n",
    "    x_axis = 1\n",
    "    y_axis = 2\n",
    "    X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    cmap = matplotlib.colormaps.get_cmap(\"Blues_r\")\n",
    "    cmap.set_under(\"black\")\n",
    "    img = ax.imshow(X1, interpolation=\"nearest\", vmin=-2.5, vmax=max(self.state_value_fn), cmap=cmap)\n",
    "    ax.axis('off')\n",
    "    for i in range(x_axis):\n",
    "        for j in range(y_axis):\n",
    "            ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "    plt.show()\n",
    "    self.render_policy()\n",
    "\n",
    "def render_policy(self):\n",
    "    print('Policy of the agent:')\n",
    "    out = ' | '\n",
    "    render = out\n",
    "    for i in range(self.mdp.num_states):\n",
    "        token = \"\"\n",
    "        if self.policy[i][0] > 0:   # search\n",
    "            token += \"Search\"\n",
    "        if self.policy[i][1] > 0:   # wait\n",
    "            if token != \"\":\n",
    "                token += \" or Wait\"\n",
    "            else:\n",
    "                token += \"Wait\"\n",
    "        if len(self.policy[i]) > 2:\n",
    "            if self.policy[i][2] > 0:   # recharge\n",
    "                if token != \"\":\n",
    "                    token += \" and Recharge\"\n",
    "                else:\n",
    "                    token += \"Recharge\"\n",
    "        render += token + out\n",
    "    print(render)\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_greedy_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            for i in range(len(out)):\n",
    "                frames.append([out[i]])\n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0][0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index][0])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=3) \n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'render_policy', render_policy)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c600eb9-c402-4d72-8ee5-7bb3f1599a95",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium`, and extract its properties via our `MarkovDecisionProcess` class. Then, we can set up an Agent that we can train. We see that our initial Value function only contains zeros (state \"low\" is displayed in the left square; state \"high\" is displayed in the right square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3590daf5-696c-4148-997e-5e4465bc073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Search | Wait | \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='text')\n",
    "env.reset()\n",
    "mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "agent = Agent(mdp, gamma=0.9, update_threshold=0.05)\n",
    "agent.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94702010-4216-4668-ae08-a416f889969f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c6c8b4-c0e6-4f33-885f-838404249fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Search | \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Search | \n",
      "Sweeps required for convergence  2\n",
      "Iterations required for convergence  2\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfdbfe-7e65-4198-bcc5-c1013496fc65",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The solution for the environment is a fairly simple behavior policy. If the agent is in the state \"high\", it searches for cans; if the state is \"low\", it recharges itself. This way, the agent receives the highest possible return. Below we show the resulting behavior highlighted in the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6d0b7c-8da1-4f48-ae56-2deb38be5a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/test_rllbc/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:270: UserWarning: \u001b[33mWARN: RGB-array rendering should return a numpy array, got <class 'list'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"policy_iteration.mp4\" loop autoplay  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='rgb_array', render_type='node')\n",
    "num_runs=5\n",
    "video_file_1 = \"policy_iteration.mp4\"\n",
    "agent.evaluate(env, video_file_1, num_runs)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
