{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41b88be-31b3-4ea3-9563-47058d0d2880",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use the policy iteration algorithm to find an optimal policy for the recycling bot from \"Reinforcement Learning: An Introduction.\" An overview of the MDP that belongs to the recycling bot is shown below. The robot has a battery, whose state can be high or low. When having a high battery level, the agent can search or wait for cans. When being in a low battery state, the agent can also recharge, to prevent running out of battery.\n",
    "\n",
    "<img src=\"./utils/recycling_bot.jpg\" alt=\"Policy Iteration algorithm\" width=\"500\">\n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c429c4-dfcb-46af-9695-c5ccd62c1c7d",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa671843-9406-4e96-8cc7-d51c7084f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6beaf-b124-4cc0-aa3d-90e3035e8c9a",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the MDP\n",
    "\n",
    "Dynamic Programming (DP) algorithms are a class of algorithms capable of finding the optimal policy for an environment, given a model of the environment. Below, we implement such a model in the class `MarkovDecisionProcess`. The class identifies the number of states in `self.num_states`, as well as the state dependent actions in `self.actions_per_state`. Furthermore, it contains the state transition probabilities, as well as the successor states and rewards in `self.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc384a04-e075-4e03-827f-ce0c0b5aaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, env):\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.P = env.unwrapped.P\n",
    "        self.actions_per_state = [] # list containing the actions available per state\n",
    "        for state in self.P:\n",
    "            actions = list(self.P[state].keys())\n",
    "            self.actions_per_state.append(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bbf43-745b-4deb-bc32-77deb21f0f12",
   "metadata": {},
   "source": [
    "### Initializing the Agent\n",
    "The DP Agent uses the MDP to obtain an optimal policy via the optimization algorithm. This is done using the method `train`. In this example, we consider policy iteration. The algorithm obtains an optimal policy by switching between the evaluation of a policy to obtain a value function, and using the value function to improve the policy. This is done until the policy converges. Below, we provide an overview of the algorithm.\n",
    "\n",
    "<img src=\"./utils/policy_iteration.png\" alt=\"Policy Iteration algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Since we are trying to find a policy for the recycling bot, we have implemented the method `train` with a state-dependent action space. This is rather unusual, as most environments are based on uniform action spaces.\n",
    "Furthermore, the algorithm also offers the option of using in-place updates for policy evaluation. For further details, we refer to the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5397e837-4d05-4861-a46c-499516e3059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, mdp, gamma=0.9, update_threshold=1e-6):\n",
    "        self.mdp = mdp\n",
    "        self.update_threshold = update_threshold # stopping criterium for the updates of policy and value function\n",
    "        self.state_value_fn = np.zeros(self.mdp.num_states) # table leading from state to value expectations\n",
    "        # Create random policy\n",
    "        self.policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            random_entry = random.randint(0, len(self.mdp.actions_per_state[state])-1)\n",
    "            self.policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            self.policy[state][random_entry] = 1\n",
    "        self.gamma = gamma # discount factor\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        # Choose action based on the probabilities defined within the policy\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]), rtol=0.01)))\n",
    "        return action\n",
    "\n",
    "    def train(self, in_place=False):\n",
    "        policy_stable = False\n",
    "        iteration = 0\n",
    "        total_sweeps = 0\n",
    "        while not policy_stable:\n",
    "            iteration += 1\n",
    "            # Policy Evaluation\n",
    "            total_sweeps += self.policy_evaluation(in_place)\n",
    "            # Policy Improvement\n",
    "            policy_stable = self.policy_improvement()\n",
    "        print('Sweeps required for convergence ', str(total_sweeps))\n",
    "        print('Iterations required for convergence ', str(iteration))\n",
    "\n",
    "    def policy_evaluation(self, in_place=True): # in_place version\n",
    "        sweeps = 0\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            delta = 0\n",
    "            sweeps += 1\n",
    "            if in_place:\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    self.state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "            else:\n",
    "                new_state_value_fn = np.copy(self.state_value_fn)\n",
    "                for state in range(self.mdp.num_states):\n",
    "                    old_state_value = self.state_value_fn[state]\n",
    "                    new_state_value = 0\n",
    "                    # sum over potential actions\n",
    "                    for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                        new_state_value += self.get_state_value(state, action)\n",
    "                    new_state_value_fn[state] = new_state_value\n",
    "                    delta = max(delta, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "                if delta < self.update_threshold:\n",
    "                    stable = True\n",
    "                self.state_value_fn = new_state_value_fn\n",
    "        return sweeps\n",
    "\n",
    "    def get_state_value(self, state, action):\n",
    "        # Value expectation considering the policy\n",
    "        policy_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            policy_value += self.policy[state][action] * transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return policy_value\n",
    "    \n",
    "    def get_action_value(self, state, action):\n",
    "        # Value expectation without considering the policy\n",
    "        action_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            successor_state = transition[1]  # value/name of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[successor_state])\n",
    "        return action_value\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        current_policy = self.policy # Cache current policy\n",
    "        best_policy = []\n",
    "        for state in range(self.mdp.num_states):\n",
    "            best_policy.append([0 for _ in range(len(self.mdp.actions_per_state[state]))])\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                action_values.append(self.get_action_value(state, action))\n",
    "            best_actions = np.where(action_values == max(action_values))[0]\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in range(len(self.mdp.actions_per_state[state]))]\n",
    "            # If the current policy is not the best policy, update it\n",
    "            if not np.array_equal(current_policy[state], best_policy[state]):\n",
    "                policy_stable = False\n",
    "                self.policy[state] = best_policy[state]\n",
    "        self.visualize()\n",
    "        return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b45e5-0d10-4801-a777-200073f6ed37",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluating the trained agent, we implement methods that can show us the value function (`visualize`) and the policy (`render_policy`). We also add the method `evaluate`, which allows us to show samples from an agent's policy in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34a3cbd9-f6ca-43de-bca3-276f4a6047ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self):\n",
    "    x_axis = 1\n",
    "    y_axis = 2\n",
    "    X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    cmap = matplotlib.colormaps.get_cmap(\"Blues_r\")\n",
    "    cmap.set_under(\"black\")\n",
    "    img = ax.imshow(X1, interpolation=\"nearest\", vmin=-2.5, vmax=max(self.state_value_fn), cmap=cmap)\n",
    "    ax.axis('off')\n",
    "    for i in range(x_axis):\n",
    "        for j in range(y_axis):\n",
    "            ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "    plt.show()\n",
    "    self.render_policy()\n",
    "\n",
    "def render_policy(self):\n",
    "    print('Policy of the agent:')\n",
    "    out = ' | '\n",
    "    render = out\n",
    "    for i in range(self.mdp.num_states):\n",
    "        token = \"\"\n",
    "        if self.policy[i][0] > 0:   # search\n",
    "            token += \"Search\"\n",
    "        if self.policy[i][1] > 0:   # wait\n",
    "            if token != \"\":\n",
    "                token += \" or Wait\"\n",
    "            else:\n",
    "                token += \"Wait\"\n",
    "        if len(self.policy[i]) > 2:\n",
    "            if self.policy[i][2] > 0:   # recharge\n",
    "                if token != \"\":\n",
    "                    token += \" and Recharge\"\n",
    "                else:\n",
    "                    token += \"Recharge\"\n",
    "        render += token + out\n",
    "    print(render)\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_greedy_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            for i in range(len(out)):\n",
    "                frames.append([out[i]])\n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0][0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index][0])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=3) \n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'render_policy', render_policy)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c600eb9-c402-4d72-8ee5-7bb3f1599a95",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium`, and extract its properties via our `MarkovDecisionProcess` class. Then, we can set up an Agent that we can train. We see that our initial Value function only contains zeros (state \"low\" is displayed in the left square; state \"high\" is displayed in the right square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3590daf5-696c-4148-997e-5e4465bc073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAEMCAYAAABZZbUfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHtklEQVR4nO3cX2jd5R3H8W9soMWGMVOpGmRlTBadqRY6m3Vs82JQ5pRunVNb/+BUCsJQihMZVCfDbWwMBrtxkzFZaG0zZa3V0ipYiwqC9c/NktgieiFUoZBTKKFYOOHswlgMJmnc2WmGn9fr8jm/Jzzn4nvyPj+SX1er1WoVABDrnIU+AACwsMQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOG653vhR81OHgMA6IQl8/hN784AAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEQJCJiYm6/74t9dWv9NWXe5bU4OpV9eQ/h+e199ixY7X5zp/VxReeX71fOreu/s7aOvjigQ6fGJgv8007xECQjTf8pJ7YNlRbH3y49uzdX6u/eVXdfuumGt65Y859p06dqh+u+34dPHig/vinP9dTu/bU8gsuqPXX/qBeefmls3R6YC7mm3Z0tVqt1nwu/KjZ6aPQSc/t31cb1l9b/9i2o27auOn0+nXXrKuxsdF65733a9GiRTPufewvj9aWe39eB19+tb61dm1VVTWbzVqz+spaurSnXnn1tbPyHoCZmW/msqT7zNe4MxDimad3V09PT13/0xumrd92+x314Qcf1KHXZh/4Z/bsrq/395/+oKiq6u7urk0331pvvH6ojh492rFzA2dmvmmXGAgxOjpS/ZdeVt3d0xNx5corqqpqbHRk1r1joyM1MHXdp32y9vbY6P/wpMDnZb5plxgI0WiMV29v72fWz5taazTGZ907Pj5evefNsHdqbXx89r1A55lv2iUGgnR1dc314n+9d86fC5wV5pt2iIEQvb3LZiz8443Gx6/P8M3gE8uWLavxGb5ZHD8+tXeGbyTA2WO+aZcYCDEwsLKOHH67ms3p/xYyMvLvqqr6xuUDs+69fGBljU5d93n3Ap1nvmmXGAix/scbamJionbv+te09Se2DdVFfX21ZnBw9r0/2lBHDh+e9hfJzWazhndsr6vWDFZfX1/Hzg2cmfmmXZ4zEOS6a9bVW2++Ub/53R/qa5dcUk8O76zH//63enxoe226+Zaqqrp78121fdtQjR55t1asWFFVHz+U5NuDq+vEiRP1yG9/X8uXL6/H/vpo7dv7bO17/oX67veuXsi3BZT5Znbzec7APC7hi2L4qV318ENb65Ff/6oajUb1919aQ9t31o03bTx9zeTkZE1OTlZ9qhEXL15c+54/UFt/+UD9Yss9dfLkybriylW1Z+9+HxTwf8J80w53BgDgC8wTCAGAMxIDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQrqvVarUW+hAAwMJxZwAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAML9B4uHzIG1I5ioAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Wait | \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='text')\n",
    "env.reset()\n",
    "mdp = MarkovDecisionProcess(env) # in our case contains dynamics function\n",
    "agent = Agent(mdp, gamma=0.9, update_threshold=0.05)\n",
    "agent.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94702010-4216-4668-ae08-a416f889969f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c6c8b4-c0e6-4f33-885f-838404249fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Wait | Search | \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Search | \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy of the agent:\n",
      " | Recharge | Search | \n",
      "Sweeps required for convergence  3\n",
      "Iterations required for convergence  3\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfdbfe-7e65-4198-bcc5-c1013496fc65",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The solution for the environment is a fairly simple behavior policy. If the agent is in the state \"high\", it searches for cans; if the state is \"low\", it recharges itself. This way, the agent receives the highest possible return. Below we show the resulting behavior highlighted in the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d0b7c-8da1-4f48-ae56-2deb38be5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('RecyclingRobot-v1', render_mode='rgb_array', render_type='node')\n",
    "num_runs=5\n",
    "video_file_1 = \"policy_iteration.mp4\"\n",
    "agent.evaluate(env, video_file_1, num_runs)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
