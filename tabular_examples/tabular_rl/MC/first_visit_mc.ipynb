{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5fd29d-878b-4c64-ab23-987e27129ec4",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use a first-visit Monte Carlo control algorithm to find an optimal policy for the Frozen Lake environment from [\"Gymnasium.\"](https://gymnasium.farama.org/index.html#). An example of the environment is shown below. The goal is to move the elf to the gift without entering one of the ice holes. The state is the position on the grid, and we can move the elf into the same four directions in each state.\n",
    "\n",
    "<img src=\"./utils/frozen_lake.gif\" alt=\"Example of Frozen_Lake\" width=\"800\">   \n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e47d7-e4b7-4bd0-ae5c-7245cc9956b9",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1aef66-0371-4847-b774-f3abe8f4d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622e99f-b233-4c4d-8041-6c16914aeb7c",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the Agent\n",
    "\n",
    "Monte Carlo (MC) algorithms are a class of algorithms capable of finding the optimal policy for an environment without needing a model of the environment. In this example, we use the fact, that the action space is independent of the state space. This is a common assumption in reinforcement learning implementations, which allows for a straightforward presentation of the algorithm. However, there are some environments, where this assumption doesn't hold, e.g. the recycling robot from the lecture.\n",
    "\n",
    "Our `Agent` interacts with the environment to obtain an optimal policy via first-visit Monte Carlo control. MC methods sample and average returns for each stateâ€“action pair to estimate the action-value function. Notice that the first-visit characteristic comes from the fact that we always use the return following the *first* visit of a state action pair for estimation. We show the algorithm below.\n",
    "\n",
    "<img src=\"./utils/first_visit_mc.png\" alt=\"Every-visit Monte Carlo control algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Please note that we call the Q-function `action_value_fn`. Just like in the algorithm above, we use an $\\epsilon$-greedy policy via the `epsilon_greedy_policy` method, to ensure exploration. The randomness can be set via the parameter `epsilon`.\n",
    "*Note*: Our implementation of `epsilon_greedy_policy` differs slightly from the pseudocode shown above, however, it results in a similar behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b693c-a9d4-4212-9452-7f264e007d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.ret_sum = defaultdict(float) # Keeping track of sum of reward in each state\n",
    "        self.ret_count = defaultdict(float) # Keeping track of count in each state\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon # choose 0.0 to make a totally greedy policy, 1.0 for a fully random policy\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "    def get_best_action(self, obs):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(),\n",
    "                                                                 rtol=0.01)))\n",
    "        return best_action\n",
    "    def epsilon_greedy_policy(self, obs):\n",
    "        # returns action, choosing a random action with probability epsilon, or the best action\n",
    "        # regarding to Q with probability (1 - epsilon)\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(obs)\n",
    "        return action\n",
    "\n",
    "    def train(self, num_episodes, episode_max_duration=100):\n",
    "        # Run through episodes sampled to improve policy incrementally\n",
    "        for i_episode in range(1, num_episodes + 1):\n",
    "            # Generate an episode, an array of (state, action, reward) tuples\n",
    "            episode = []\n",
    "            obs, info = self.env.reset()\n",
    "            for t in range(episode_max_duration):\n",
    "                action = self.epsilon_greedy_policy(obs)\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                episode.append((obs, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "            episode = np.array(episode)\n",
    "            episode_duration = len(episode[:,:1])\n",
    "            # Calculate returns for the whole episode from the back to save memory and resources\n",
    "            G = np.zeros([episode_duration, ])\n",
    "            for i in range(episode_duration - 1, -1, -1):\n",
    "                if i + 1 > episode_duration - 1:\n",
    "                    G[i] = episode[i][2] # Last step of the episode\n",
    "                else:\n",
    "                    G[i] = episode[i][2] + self.gamma * G[i + 1]  # Every other step #!\n",
    "            # Find indices of first visits of state-action pairs in the episode\n",
    "            first_visit_indices = sorted(np.unique(episode[:,:2], return_index=True, axis=0)[1])\n",
    "            # Update the policy with average over all episodes\n",
    "            for index in first_visit_indices:\n",
    "                state = episode[index][0]\n",
    "                action = episode[index][1]\n",
    "                self.ret_sum[(state, action)] += G[index]\n",
    "                self.ret_count[(state, action)] += 1.0\n",
    "                update = self.ret_sum[(state, action)] / self.ret_count[(state, action)]\n",
    "                self.action_value_fn[int(state)][int(action)] = update\n",
    "            if i_episode % 500 == 0:\n",
    "                self.visualize(i_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd26d64-360a-4dbb-be51-2ecaaf7e16b8",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluation, we require methods that visualize the Q-function (`visualize`), as well as a method that is used to evaluate the resulting policy (`evaluate`). We add these methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984eff2-a53b-4c01-96f2-c83eee231821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self, epoch): \n",
    "    fig, ax=plt.subplots(figsize=(7,5))\n",
    "    lines = 4\n",
    "    rows = 4        \n",
    "    # Define grid positions:\n",
    "    pos_x_left = 0.2\n",
    "    pos_x_mid = 0.5\n",
    "    pos_x_right = 0.8\n",
    "    pos_y_up = 0.2\n",
    "    pos_y_mid = 0.5\n",
    "    pos_y_down = 0.8\n",
    "    grid_size = {'x': lines, 'y': rows}\n",
    "    def gridcreator(pos_x, pos_y):\n",
    "        grid = []\n",
    "        for i in range(grid_size['x']):\n",
    "            for j in range(grid_size['y']):\n",
    "                x = pos_x + j\n",
    "                y = pos_y + i\n",
    "                grid.append((x, y))\n",
    "        return grid\n",
    "    top = self.action_value_fn[:,3].reshape((lines, rows))\n",
    "    top_value_positions = gridcreator(pos_x_mid, pos_y_up)\n",
    "    right = self.action_value_fn[:,2].reshape((lines, rows))\n",
    "    right_value_positions = gridcreator(pos_x_right, pos_y_mid)\n",
    "    bottom = self.action_value_fn[:,1].reshape((lines, rows))\n",
    "    bottom_value_positions = gridcreator(pos_x_mid, pos_y_down)\n",
    "    left= self.action_value_fn[:,0].reshape((lines, rows))\n",
    "    left_value_positions = gridcreator(pos_x_left, pos_y_mid)\n",
    "    # Define triangles\n",
    "    ax.set_ylim(lines, 0)\n",
    "    anchor_points = np.array([[0,0],[0,1],[.5,.5],[1,0],[1,1]]) # Corner coordinates\n",
    "    corner_indizes = np.array([[0,1,2], [0,2,3],[2,3,4],[1,2,4]]) # Corner indices\n",
    "    xy_coordinates = np.zeros((lines * rows * 5,2))\n",
    "    triangles = np.zeros((lines * rows * 4, 3))\n",
    "    for i in range(lines):\n",
    "        for j in range(rows):\n",
    "            k = i*rows+j\n",
    "            xy_coordinates[k*5:(k+1)*5,:] = np.c_[anchor_points[:,0]+j, \n",
    "                                                  anchor_points[:,1]+i]\n",
    "            triangles[k*4:(k+1)*4,:] = corner_indizes + k*5\n",
    "    colours = np.c_[left.flatten(), top.flatten(), \n",
    "            right.flatten(), bottom.flatten()].flatten()\n",
    "    ax.triplot(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "               **{\"color\":\"k\", \"lw\":1})\n",
    "    tripcolor = ax.tripcolor(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "                             facecolors=colours, **{\"cmap\": \"coolwarm\"})\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    fig.colorbar(tripcolor)\n",
    "    # Define text:\n",
    "    textsize = 10\n",
    "    for i, (xi,yi) in enumerate(top_value_positions):\n",
    "        plt.text(xi,yi,round(top.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(right_value_positions):\n",
    "        plt.text(xi,yi,round(right.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(left_value_positions):\n",
    "        plt.text(xi,yi,round(left.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(bottom_value_positions):\n",
    "        plt.text(xi,yi,round(bottom.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    plt.title(\"Q-Function, Epoch \"+str(epoch))\n",
    "    # add lines for separation\n",
    "    for i in range(lines+1):\n",
    "        x = [0, rows]\n",
    "        y = [i, i]\n",
    "        plt.plot(x,y, color='black')\n",
    "    for i in range(rows+1):\n",
    "        x = [i, i]\n",
    "        y = [0, lines]\n",
    "        plt.plot(x,y, color='black')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_best_action(obs) \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            frames.append(out)              \n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d827ad-ddad-457f-b804-8814b508ae78",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier and evaluate its performance.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium` and a custom map. Here, we invite students to explore how the training changes when the position of holes (H) and Frozen plates (F). Then, we set up an Agent for training. Initially, our Q-function only consists of zeros, but changes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14830a7-7f85-48f4-94a1-3bfe8b33f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False)\n",
    "env.reset()\n",
    "agent = Agent(env, gamma=0.9, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d628d6-81d0-412c-9690-1c253d1cc90c",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba1bbc-8034-472e-a1a6-1021e817c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_episodes=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175449b0-8c99-45ea-bed0-2725da5635fa",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Above, we can see the evolution of the Q-function during training. The coloring already indicates what a greedy policy can look like. Below we show a greedy policy in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29fd9c-8e49-4655-979a-b2dc9adca331",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)\n",
    "video = \"first_visit.mp4\"\n",
    "agent.evaluate(env, video, num_runs=5)\n",
    "Video(video, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
