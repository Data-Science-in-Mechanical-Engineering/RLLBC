{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bf438f-3d9f-471f-9912-289a1b45a5a4",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use Dyna-Q to find an optimal policy for the Frozen Lake environment from [\"Gymnasium.\"](https://gymnasium.farama.org/index.html#). An example of the environment is shown below. The goal is to move the elf to the gift without entering one of the ice holes. The state is the position on the grid, and we can move the elf into the same four directions in each state.\n",
    "\n",
    "<img src=\"./utils/frozen_lake.gif\" alt=\"Example of Frozen_Lake\" width=\"800\">   \n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1674a-c06b-419c-8186-3b53e7f39619",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bfb44-2a6b-4c62-a555-b3b938781070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0caf46-8f86-44d9-b899-c707c49380ee",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the Agent\n",
    "\n",
    "Temporal Difference (TD) algorithms are a class of algorithms capable of finding the optimal policy for an environment without needing a model of the environment. In this example, we use the fact, that the action space is independent of the state space. This is a common assumption in reinforcement learning implementations, which allows for a straightforward presentation of the algorithm. However, there are some environments, where this assumption doesn't hold, e.g. the recycling robot from the lecture.\n",
    "\n",
    "Our `Agent` interacts with the environment to obtain an optimal policy via Dyna-Q. To understand Dyna-Q, consider the following: In standard Q-learning, we only improve the value function. However, we can also use the same experience to improve a model in parallel, which we can in turn use for further refinements of the value function. This is the approach Dyna-Q follows. The algorithm is presented below.\n",
    "\n",
    "<img src=\"./utils/dyna-q.png\" alt=\"Dyna-Q algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Please note that we call the Q-function `action_value_fn`. Just like in the algorithm above, we use an $\\epsilon$-greedy policy via the `epsilon_greedy_policy` method, to ensure exploration. The randomness can be set via the parameter `epsilon`. Additionally, there exists a parameter, `learning_rate`, that is used for updates of the Q-function. \n",
    "\n",
    "*Note*: The Agent is similar to the Agent used for Q-learning. However, by adding `self.model` we can use experience for speeding up the training. The parameter `dyn_q_iters` is used to set the rate of replays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf33b0-b021-4fd4-8992-df7d51c18c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1, dyn_q_iters=100):\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n)) # the q-fn\n",
    "        self.model = np.zeros((self.env.observation_space.n, self.env.action_space.n, 2))\n",
    "        class StateActionTracker:\n",
    "            def __init__(self):\n",
    "                self.states = {}\n",
    "                self.actions = {}\n",
    "            def add_state_action_pair(self, state, action):\n",
    "                if state not in self.states:\n",
    "                    self.states[state] = 1\n",
    "                    self.actions[state] = set([action])\n",
    "                else:\n",
    "                    self.states[state] += 1\n",
    "                    if action not in self.actions[state]:\n",
    "                        self.actions[state].add(action)\n",
    "            def get_state(self):\n",
    "                if self.states:\n",
    "                    return np.random.choice(list(self.states.keys()))\n",
    "            def get_action(self, state):\n",
    "                if state in self.actions and self.actions[state]:\n",
    "                    return np.random.choice(list(self.actions[state]))\n",
    "        self.visited_states_and_actions = StateActionTracker()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon # choose 0.0 to make a totally greedy policy, 1.0 for a fully random policy\n",
    "        self.dyn_q_iters = dyn_q_iters\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "    def get_best_action(self, state):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[state], self.action_value_fn[state].max(),\n",
    "                                                                 rtol=0.01)))\n",
    "        return best_action\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        # returns action, choosing a random action with probability epsilon, or the best action\n",
    "        # regarding Q with probability (1 - epsilon)\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(state)\n",
    "        return action\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        # Reset environment and pick first action\n",
    "        for i in range(num_episodes+1):\n",
    "            state, info = self.env.reset()  # most values are replaced in first env.step(),\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Choose action and perform step\n",
    "                action = self.epsilon_greedy_policy(state)\n",
    "                next_state, reward, done, truncated, info = self.env.step(action)\n",
    "                # TD Update\n",
    "                best_next_action = self.get_best_action(next_state)\n",
    "                self.action_value_fn[state][action] = self.action_value_fn[state][action] + self.learning_rate * (reward + self.gamma * self.action_value_fn[next_state][best_next_action] - self.action_value_fn[state][action])\n",
    "                self.model[state][action] = np.array([next_state, reward]) # Assuming deterministic environment\n",
    "                self.visited_states_and_actions.add_state_action_pair(state, action)\n",
    "                state = next_state\n",
    "                for j in range(self.dyn_q_iters):\n",
    "                    prev_state = self.visited_states_and_actions.get_state()\n",
    "                    prev_action = self.visited_states_and_actions.get_action(prev_state)\n",
    "                    prev_next_state, prev_reward = int(self.model[prev_state][prev_action][0]), self.model[prev_state][prev_action][1]\n",
    "                    best_next_action = self.get_best_action(prev_next_state)\n",
    "                    self.action_value_fn[prev_state][prev_action] = self.action_value_fn[prev_state][prev_action] + self.learning_rate * (prev_reward + self.gamma * self.action_value_fn[prev_next_state][best_next_action] - self.action_value_fn[prev_state][prev_action])\n",
    "            if i % 20 == 0:\n",
    "                self.visualize(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab6c07-9e95-4173-a888-c51e4350080c",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluation, we require methods that visualize the Q-function (`visualize`), as well as a method that is used to evaluate the resulting policy (`evaluate`). We add these methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75081fc3-91a3-448b-af3f-a7bc057cc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self, epoch): \n",
    "    fig, ax=plt.subplots(figsize=(7,5))\n",
    "    lines = 4\n",
    "    rows = 4        \n",
    "    # Define grid positions:\n",
    "    pos_x_left = 0.2\n",
    "    pos_x_mid = 0.5\n",
    "    pos_x_right = 0.8\n",
    "    pos_y_up = 0.2\n",
    "    pos_y_mid = 0.5\n",
    "    pos_y_down = 0.8\n",
    "    grid_size = {'x': lines, 'y': rows}\n",
    "    def gridcreator(pos_x, pos_y):\n",
    "        grid = []\n",
    "        for i in range(grid_size['x']):\n",
    "            for j in range(grid_size['y']):\n",
    "                x = pos_x + j\n",
    "                y = pos_y + i\n",
    "                grid.append((x, y))\n",
    "        return grid\n",
    "    top = self.action_value_fn[:,3].reshape((lines, rows))\n",
    "    top_value_positions = gridcreator(pos_x_mid, pos_y_up)\n",
    "    right = self.action_value_fn[:,2].reshape((lines, rows))\n",
    "    right_value_positions = gridcreator(pos_x_right, pos_y_mid)\n",
    "    bottom = self.action_value_fn[:,1].reshape((lines, rows))\n",
    "    bottom_value_positions = gridcreator(pos_x_mid, pos_y_down)\n",
    "    left= self.action_value_fn[:,0].reshape((lines, rows))\n",
    "    left_value_positions = gridcreator(pos_x_left, pos_y_mid)\n",
    "    # Define triangles\n",
    "    ax.set_ylim(lines, 0)\n",
    "    anchor_points = np.array([[0,0],[0,1],[.5,.5],[1,0],[1,1]]) # Corner coordinates\n",
    "    corner_indizes = np.array([[0,1,2], [0,2,3],[2,3,4],[1,2,4]]) # Corner indices\n",
    "    xy_coordinates = np.zeros((lines * rows * 5,2))\n",
    "    triangles = np.zeros((lines * rows * 4, 3))\n",
    "    for i in range(lines):\n",
    "        for j in range(rows):\n",
    "            k = i*rows+j\n",
    "            xy_coordinates[k*5:(k+1)*5,:] = np.c_[anchor_points[:,0]+j, \n",
    "                                                  anchor_points[:,1]+i]\n",
    "            triangles[k*4:(k+1)*4,:] = corner_indizes + k*5\n",
    "    colours = np.c_[left.flatten(), top.flatten(), \n",
    "            right.flatten(), bottom.flatten()].flatten()\n",
    "    ax.triplot(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "               **{\"color\":\"k\", \"lw\":1})\n",
    "    tripcolor = ax.tripcolor(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "                             facecolors=colours, **{\"cmap\": \"coolwarm\"})\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    fig.colorbar(tripcolor)\n",
    "    # Define text:\n",
    "    textsize = 10\n",
    "    for i, (xi,yi) in enumerate(top_value_positions):\n",
    "        plt.text(xi,yi,round(top.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(right_value_positions):\n",
    "        plt.text(xi,yi,round(right.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(left_value_positions):\n",
    "        plt.text(xi,yi,round(left.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(bottom_value_positions):\n",
    "        plt.text(xi,yi,round(bottom.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    plt.title(\"Q-Function, Epoch \"+str(epoch))\n",
    "    # add lines for separation\n",
    "    for i in range(lines+1):\n",
    "        x = [0, rows]\n",
    "        y = [i, i]\n",
    "        plt.plot(x,y, color='black')\n",
    "    for i in range(rows+1):\n",
    "        x = [i, i]\n",
    "        y = [0, lines]\n",
    "        plt.plot(x,y, color='black')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_best_action(obs) \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            frames.append(out)              \n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836720a2-469b-4141-929d-cd4684890dd6",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier and evaluate its performance.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium` and a custom map. Here, we invite students to explore how the training changes when the position of holes (H) and Frozen plates (F). Then, we set up an Agent for training. Initially, our Q-function only consists of zeros, but changes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f62d23-f444-4497-a53d-567fc48f8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False)\n",
    "env.reset()\n",
    "agent = Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ba26c-8e46-4672-ab6e-c17f0924767b",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "*Notice the much lower number of episodes necessary for training compared to Q-learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b692b-567a-43e8-8a5b-1f01934a6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6dc7f7-0420-4453-a534-f55742c48fd5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Above, we can see the evolution of the Q-function during training. Even after 100 epochs, the algorithm should converge to a solution. The coloring already indicates what a greedy policy can look like. Below we show a greedy policy in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3b115-be5b-4d6d-a2c3-c3ffaeff0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)\n",
    "video = \"dyna-q.mp4\"\n",
    "agent.evaluate(env, video, num_runs=5)\n",
    "Video(video, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
