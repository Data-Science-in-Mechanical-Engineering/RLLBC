{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396c0299-2cfc-4a21-b172-6f7100df4b83",
   "metadata": {},
   "source": [
    "![DSME-logo](./utils/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "Notebook Authors: Lukas Kesper\n",
    "\n",
    "In this example, we use SARSA to find an optimal policy for the Cliff Walking environment from [\"Gymnasium.\"](https://gymnasium.farama.org/index.html#). An example of the environment is shown below. The goal is to move the elf to the right side without falling off the cliff. The state is the position on the grid, and we can move the elf into the same four directions in each state.\n",
    "\n",
    "<img src=\"./utils/cliff_walking.gif\" alt=\"Example of Cliff_Walking\" width=\"500\">   \n",
    "\n",
    "Reference: [Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/the-book-2nd.html), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4fa2b-2f8f-4d9b-a8e4-595ad275b328",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d29c90-8c43-4c17-b039-3a027b265c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib inline\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from screeninfo import get_monitors\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d56939-6e6a-4461-b012-0b50a36c9d67",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### Initializing the Agent\n",
    "\n",
    "Temporal Difference (TD) algorithms are a class of algorithms capable of finding the optimal policy for an environment without needing a model of the environment. In this example, we use the fact, that the action space is independent of the state space. This is a common assumption in reinforcement learning implementations, which allows for a straightforward presentation of the algorithm. However, there are some environments, where this assumption doesn't hold, e.g. the recycling robot from the lecture.\n",
    "\n",
    "Our `Agent` interacts with the environment to obtain an optimal policy via SARSA. SARSA is an on-policy algorithm, hence, the learned Q-function directly depends on the current policy, which is updated based on the learned Q-function during training. We show the algorithm below.\n",
    "\n",
    "<img src=\"./utils/SARSA.png\" alt=\"SARSA algorithm\" width=\"500\">   \n",
    "\n",
    "The training loop can be found in the method `train`. Please note that we call the Q-function `action_value_fn`. Just like in the algorithm above, we use an $\\epsilon$-greedy policy via the `epsilon_greedy_policy` method, to ensure exploration. The randomness can be set via the parameter `epsilon`. Additionally, there exists a parameter, `learning_rate`, that is used for updates of the Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51374e6-a4eb-46c7-9b6b-d8902eab0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "\n",
    "    def get_best_action(self, obs):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(),\n",
    "                                                                 rtol=0.01)))\n",
    "        return best_action\n",
    "\n",
    "    def epsilon_greedy_policy(self, obs):\n",
    "        # returns action, choosing a random action with probability epsilon, or the best action\n",
    "        # regarding to Q with probability (1 - epsilon)\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(obs)\n",
    "        return action\n",
    "\n",
    "    def train(self, num_episodes, debug=False):\n",
    "        # Reset environment and pick first action\n",
    "        for i in range(num_episodes+1):\n",
    "            obs, info = self.env.reset()  # most values are replaced in first env.step(),\n",
    "            action = self.epsilon_greedy_policy(obs)\n",
    "            done = False\n",
    "            while not done:\n",
    "                # In this implementation we only use n=1, but we could extend it for n = ... using a numpy array\n",
    "                # Choose action and perform step\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                # TD Update\n",
    "                next_action = self.epsilon_greedy_policy(next_obs)\n",
    "                td_target = reward + self.gamma * self.action_value_fn[next_obs][next_action]\n",
    "                update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target\n",
    "                self.action_value_fn[obs][action] = update\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "            if i % 1000 == 0:\n",
    "                self.visualize(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce251d-5cd6-4d5e-969b-054de8d1d6ab",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation\n",
    "\n",
    "For evaluation, we require methods that visualize the Q-function (`visualize`), as well as a method that is used to evaluate the resulting policy (`evaluate`). We add these methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d690ae9-1204-4a7a-bd02-13aa2addc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self, epoch): \n",
    "    q_fn = np.around(self.action_value_fn, decimals=1)\n",
    "    fig, ax=plt.subplots(figsize=(18,5))\n",
    "    lines = 4\n",
    "    rows = 12\n",
    "    # Define grid positions:\n",
    "    pos_x_left = 0.2\n",
    "    pos_x_mid = 0.5\n",
    "    pos_x_right = 0.8\n",
    "    pos_y_up = 0.2\n",
    "    pos_y_mid = 0.5\n",
    "    pos_y_down = 0.8\n",
    "    grid_size = {'x': lines, 'y': rows}\n",
    "    def gridcreator(pos_x, pos_y):\n",
    "        grid = []\n",
    "        for i in range(grid_size['x']):\n",
    "            for j in range(grid_size['y']):\n",
    "                x = pos_x + j\n",
    "                y = pos_y + i\n",
    "                grid.append((x, y))\n",
    "        return grid\n",
    "    top = q_fn[:,0].reshape((lines,rows))\n",
    "    top_value_positions = gridcreator(pos_x_mid, pos_y_up)\n",
    "    right = q_fn[:,1].reshape((lines,rows))\n",
    "    right_value_positions = gridcreator(pos_x_right, pos_y_mid)\n",
    "    bottom = q_fn[:,2].reshape((lines,rows))\n",
    "    bottom_value_positions = gridcreator(pos_x_mid, pos_y_down)\n",
    "    left= q_fn[:,3].reshape((lines,rows))\n",
    "    left_value_positions = gridcreator(pos_x_left, pos_y_mid)\n",
    "    # Define triangles\n",
    "    ax.set_ylim(lines, 0)\n",
    "    anchor_points = np.array([[0,0],[0,1],[.5,.5],[1,0],[1,1]]) # Corner coordinates\n",
    "    corner_indizes = np.array([[0,1,2], [0,2,3],[2,3,4],[1,2,4]]) # Corner indices\n",
    "    xy_coordinates = np.zeros((lines * rows * 5,2))\n",
    "    triangles = np.zeros((lines * rows * 4, 3))\n",
    "    for i in range(lines):\n",
    "        for j in range(rows):\n",
    "            k = i*rows+j\n",
    "            xy_coordinates[k*5:(k+1)*5,:] = np.c_[anchor_points[:,0]+j, \n",
    "                                                  anchor_points[:,1]+i]\n",
    "            triangles[k*4:(k+1)*4,:] = corner_indizes + k*5\n",
    "    colours = np.c_[left.flatten(), top.flatten(), \n",
    "            right.flatten(), bottom.flatten()].flatten()\n",
    "    ax.triplot(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "               **{\"color\":\"k\", \"lw\":1})\n",
    "    tripcolor = ax.tripcolor(xy_coordinates[:,0], xy_coordinates[:,1], triangles, \n",
    "                             facecolors=colours, **{\"cmap\": \"coolwarm\"}, vmin=-10, vmax=0.0)\n",
    "    ax.margins(0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    fig.colorbar(tripcolor)\n",
    "    # Define text:\n",
    "    textsize = 7\n",
    "    for i, (xi,yi) in enumerate(top_value_positions):\n",
    "        plt.text(xi,yi,round(top.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(right_value_positions):\n",
    "        plt.text(xi,yi,round(right.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(left_value_positions):\n",
    "        plt.text(xi,yi,round(left.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    for i, (xi,yi) in enumerate(bottom_value_positions):\n",
    "        plt.text(xi,yi,round(bottom.flatten()[i],2), size=textsize, color=\"w\", \n",
    "                 ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    plt.title(\"Q-Function, Epoch \"+str(epoch))\n",
    "    # add lines for separation\n",
    "    for i in range(lines+1):\n",
    "        x = [0, rows]\n",
    "        y = [i, i]\n",
    "        plt.plot(x,y, color='black')\n",
    "    for i in range(rows+1):\n",
    "        x = [i, i]\n",
    "        y = [0, lines]\n",
    "        plt.plot(x,y, color='black')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(self, env, file, num_runs=5):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_best_action(obs) \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            out = env.render()\n",
    "            frames.append(out)              \n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "\n",
    "setattr(Agent, 'visualize', visualize)\n",
    "setattr(Agent, 'evaluate', evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33283a-dc2b-4a66-a5c7-648bda555201",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Now we train the agent with the algorithm we implemented earlier and evaluate its performance.\n",
    "\n",
    "### Setting up Agent and Environment\n",
    "We start by setting up the environment, which is part of our custom environments. We create the environment using `Gymnasium`. Then, we set up an Agent for training. Initially, our Q-function only consists of zeros, but changes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54534e87-547e-4be0-8f4b-230f1982b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v1', render_mode=None)\n",
    "env.reset()\n",
    "agent = Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4041c82-9107-4a70-925d-f6d3507f2590",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03b12b-0f84-40bd-9c33-f1c6fa3650ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f118d-ae1c-4f9a-8ac0-73431537d7dd",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Above, we can see the evolution of the Q-function during training. The  coloring already indicates what a greedy policy can look like. Below we show a greedy policy in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff129d59-2fda-40f3-900a-ccf0c838450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "video = \"sarsa.mp4\"\n",
    "agent.evaluate(env, video, num_runs=5)\n",
    "Video(video, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
