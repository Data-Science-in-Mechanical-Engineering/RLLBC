{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c767bde3",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Nonlinear Value Function Approximation for Pole Balancing\n",
    "*(Code adapted from: Reinforcement Learning Course by University of TÃ¼bingen*\n",
    "*https://al.is.mpg.de/pages/course-reinforcement-learning-ws-20-21)*\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aaf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.policy as policy\n",
    "import utils.memory as mem\n",
    "from utils.feedforward import Feedforward\n",
    "import custom_envs\n",
    "import importlib\n",
    "importlib.reload(mem)\n",
    "importlib.reload(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20572e35",
   "metadata": {},
   "source": [
    "## Pendulum Environment\n",
    "\n",
    "The goal is to stabilize the pendulum upside down:\n",
    "\n",
    "<img src=\"./img/pendulum.gif\" alt=\"Pendulum\" width=\"500\">\n",
    "\n",
    "### Dynamics\n",
    "\n",
    "<img src=\"./img/pendulum.png\" alt=\"Pendulum\" width=\"300\">\n",
    "\n",
    "    state = [angle, angular_velocity] (continuous)\n",
    "    action = [torque] (continuous)\n",
    "    reward = (np.cos(state[0]) - 1) - 0.02*np.abs(state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0bd5e",
   "metadata": {},
   "source": [
    "### Utility functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afe879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(value_function):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    xxs =np.linspace(-np.pi/2,np.pi/2)\n",
    "    yys =np.linspace(-8,8)\n",
    "    XX,YY=np.meshgrid(xxs,yys)\n",
    "    dots=np.asarray([XX.ravel(),YY.ravel()]).T\n",
    "    # values = np.asarray(test_func(dots)).reshape(XX.shape)\n",
    "    values =value_function.predict(dots).reshape(XX.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=[10,8])\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    surf = ax.plot_surface(XX, YY, values, cmap=cm.coolwarm,\n",
    "                           linewidth=0, antialiased=False)\n",
    "    ax.set_xlabel('angle')\n",
    "    ax.set_ylabel('angle velocity')\n",
    "    ax.set_zlabel('value')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(plt_fit):\n",
    "    fig_loss = plt.figure()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.plot(range(len(plt_fit)), plt_fit)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930cd4c",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Collect Data using PD Controller Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CustomPendulum-v1')\n",
    "pi = policy.PDPolicy(env)\n",
    "buffer = mem.Memory()\n",
    "\n",
    "s, _ = env.reset()\n",
    "for t in range(100):\n",
    "    #env.render()\n",
    "    a = pi.get_action(s)\n",
    "    s_new, rew, done, _ , _ = env.step(a)\n",
    "    buffer.add_transition([s,a,rew,s_new, done])\n",
    "    s = s_new\n",
    "\n",
    "transitions = buffer.get_all_transitions()\n",
    "states = np.vstack(transitions[:,0])\n",
    "actions = transitions[:,1]\n",
    "rewards = transitions[:,2]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(states[:,0], label=\"position\")\n",
    "plt.plot(states[:,1], label=\"velocity\")\n",
    "plt.plot(actions, label=\"action\")\n",
    "plt.plot(rewards, label=\"reward\")\n",
    "plt.legend()\n",
    "\n",
    "for ep in range(500):\n",
    "    s, _ = env.reset()\n",
    "    for t in range(100):\n",
    "        a = pi.get_action(s)\n",
    "        s_new, rew, done, _, _ = env.step(a)\n",
    "        buffer.add_transition([s,a,rew,s_new, done])\n",
    "        s = s_new\n",
    "\n",
    "buffer.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19de45",
   "metadata": {},
   "source": [
    "### Define Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d787a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(Feedforward):\n",
    "    def __init__(self, observation_dim, hidden_sizes=[100, 100]):\n",
    "        # The V-Network is a fully connected network with 2 hidden layers of size 100 and tanh activation functions.\n",
    "        # It takes the current observation as input and returns the action values of all available actions.\n",
    "        super().__init__(input_size=observation_dim, hidden_sizes=hidden_sizes, output_size=1)\n",
    "        # Adam is a state-of-the-art gradient-based optimizer commonly used for NN training.\n",
    "        # For more information see Kingma & Ba, ADAM: A method for Stochastic Optimization\n",
    "        # You can find the paper on arXiv: https://arxiv.org/pdf/1412.6980.pdf\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005, eps=0.000001)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def fit(self, observations, targets):\n",
    "        self.train()  # put model in training mode\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        pred = self.forward(torch.from_numpy(observations).float())\n",
    "        # Compute Loss\n",
    "        loss = self.loss(pred, torch.from_numpy(targets).float())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2daff",
   "metadata": {},
   "source": [
    "### Learn Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuefunc = ValueFunction(observation_dim=2)\n",
    "plt.show()\n",
    "\n",
    "plt_fit = []\n",
    "\n",
    "iter_fit = 1000\n",
    "gamma = 0.95\n",
    "for i in range(iter_fit):\n",
    "    # sample from the replay buffer\n",
    "    data = buffer.sample(batch=128)\n",
    "    s = np.stack(data[:, 0])  # s_t\n",
    "    sp = np.stack(data[:, 3])  # s_t+1\n",
    "    rew = np.stack(data[:, 2])[:, None]  # rew\n",
    "\n",
    "    values = valuefunc.predict(s)\n",
    "    valuesp = valuefunc.predict(sp)\n",
    "    # target\n",
    "    td_target = rew + gamma * valuesp\n",
    "\n",
    "    # optimize the lsq objective\n",
    "    fit_loss = valuefunc.fit(s, td_target)\n",
    "    plt_fit.append(fit_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c82f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(plt_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed112b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(valuefunc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
