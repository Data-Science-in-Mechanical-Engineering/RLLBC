{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723ff9fb-bc22-47c4-9341-a55a802218a3",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# On-Policy vs. Off-Policy TD-Control for CliffWalking\n",
    "(Adapted from Sutton & Barto Example 6.6)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac3c62-1dea-4207-9a70-e01788883ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import copy\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c9636-1f11-41c9-850f-1db688c9ff5a",
   "metadata": {},
   "source": [
    "## Design of the Agent\n",
    "The CliffWalking environment is a gridworld example that requires the agent to find an optimal path to receive a reward. This is a standard undiscounted, episodic task, with start and goal states, and actions causing movement up, down, right, and left.\n",
    "Reward is −1 on all transitions except those into the \"cliff\", causing the agent to fall. Stepping into this region incurs a reward of −100 and sends the agent instantly back to the start. \n",
    "\n",
    "<img src=\"./img/cliff_walking.gif\" alt=\"Example of Cliff_Walking\" width=\"500\">   \n",
    "\n",
    "The agent is capable of two different learning algorithms:\n",
    "Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dd8a5-8332-4afc-8379-d527573dfaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TD_Agent():\n",
    "    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "    def get_best_action(self, obs):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(),\n",
    "                                                                 rtol=0.01)))\n",
    "        return best_action\n",
    "    def epsilon_greedy_policy(self, obs):\n",
    "        # returns action, choosing a random action with probability epsilon or the best action based on Q(s,a)\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(obs)\n",
    "        return action\n",
    "\n",
    "    def q_learning(self, num_episodes):\n",
    "        returns = []\n",
    "        for i in range(num_episodes):\n",
    "            returns_i = 0\n",
    "            obs, info = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy_policy(obs)\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                best_next_action = self.get_best_action(next_obs)\n",
    "                td_target = reward + self.gamma * self.action_value_fn[next_obs][best_next_action]\n",
    "                update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target\n",
    "                self.action_value_fn[obs][action] = update\n",
    "                obs = next_obs\n",
    "                returns_i += reward\n",
    "            returns.append(returns_i)\n",
    "        return returns\n",
    "\n",
    "    def sarsa(self, num_episodes):\n",
    "        returns = []\n",
    "        for i in range(num_episodes):\n",
    "            returns_i = 0\n",
    "            obs, info = self.env.reset()\n",
    "            action = self.epsilon_greedy_policy(obs)\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                next_action = self.epsilon_greedy_policy(next_obs)\n",
    "                td_target = reward + self.gamma * self.action_value_fn[next_obs][next_action]\n",
    "                update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target\n",
    "                self.action_value_fn[obs][action] = update\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "                returns_i += reward\n",
    "            returns.append(returns_i)\n",
    "        return returns\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5671a-6786-4d66-8ed7-7560598ee32c",
   "metadata": {},
   "source": [
    "## Evaluation of the Training\n",
    "To compare the two methods, both agents are trained with the same properties. Epsilon is set to 0.1, allowing for occasional random behavior of the agent, and the training lasts for 8000 episodes. The function used for comparison can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f87d3-f34f-471a-a470-efd82e9f2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, env, num_runs=1, file=None):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    tot_reward = [0]\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        reward_per_run = 0\n",
    "        while not done:\n",
    "            action = self.get_best_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_per_run += reward\n",
    "            # save frame\n",
    "            out = env.render()\n",
    "            frames.append(out)\n",
    "        tot_reward.append(reward_per_run + tot_reward[-1])\n",
    "\n",
    "    # create animtation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            # plt.show()\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "        video_created = True\n",
    "\n",
    "    return tot_reward, video_created\n",
    "    \n",
    "setattr(TD_Agent, \"evaluate\", evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92041c72-1ae1-41b1-9113-1137fb5e8b8f",
   "metadata": {},
   "source": [
    "We show the return of both agents over the episodes. To allow for improved visualization, the returns are smoothed using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c7d6f-3011-4dc0-9b41-e014480d2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(array, size):\n",
    "    window_size = size\n",
    "    filter = np.ones(window_size) / window_size\n",
    "    smoothed_array = np.convolve(array, filter, mode='same')\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df052d33-02c8-4f0c-9cb9-9a678bba0633",
   "metadata": {},
   "source": [
    "In the diagram below, the smoothed returns are displayed. Although both agents learn and improve their policy over time, Q-learning receives significantly less reward during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5da3e-0d52-4126-8566-48d7668de6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agents\n",
    "env = gym.make('CliffWalking-v1', render_mode=None)\n",
    "num_episodes = 8100\n",
    "filter = 100\n",
    "env.reset()\n",
    "agent = TD_Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.1)\n",
    "q_learning_training_history = smooth(agent.q_learning(num_episodes=num_episodes), filter)\n",
    "q_learning_agent = copy.deepcopy(agent)\n",
    "agent.reset()\n",
    "sarsa_training_history = smooth(agent.sarsa(num_episodes=num_episodes), filter)\n",
    "sarsa_agent = copy.deepcopy(agent)\n",
    "agent.reset()\n",
    "\n",
    "# Compare the returns\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "ax.plot(range(num_episodes), q_learning_training_history, label=\"Q-Learning\")\n",
    "ax.plot(range(num_episodes), sarsa_training_history, label=\"SARSA\")\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_title(\"Comparison of Q-Learning and SARSA\")\n",
    "ax.set_ylim(-100, 0)\n",
    "ax.set_xlim(0, num_episodes-filter)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c873203-0314-4566-952b-f61c860852ed",
   "metadata": {},
   "source": [
    "This significant difference in the returns results from the epsilon-greedy training and the on-/off-policy characteristics of the agents. Below, we visualize the final policies in the environment, were exploratory actions are no longer taken.<br>\n",
    "The agent trained using Q-learning learns the values for the target policy, which travels right along the edge of the cliff. Unfortunately, this results in\n",
    "its occasionally falling off the cliff during training due to the ε-greedy action exploration, which causes the reduced returns during training.<br>\n",
    "Sarsa, on the other hand, learns values for the behavior policy and thus takes random exploratory actions into account. Therefore, it learns the longer but safer path through the upper part of the grid. \n",
    "\n",
    "# Q-learning agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e1d7a-e148-45ef-b3a8-cc16da732462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_env = gym.make('CliffWalking-v1', render_mode='rgb_array')\n",
    "print('Q-Learning agent:')\n",
    "video_file_1 = \"q_learning.mp4\"\n",
    "q_learning_agent.evaluate(render_env, 5, video_file_1)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7da403-d5f5-4fe2-924b-964b968d928f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('SARSA agent:')\n",
    "video_file_2 = \"sarsa.mp4\"\n",
    "sarsa_agent.evaluate(render_env, 5, video_file_2)\n",
    "Video(video_file_2, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
