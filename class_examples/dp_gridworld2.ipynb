{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2417fbb5",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651b81f",
   "metadata": {},
   "source": [
    "# Dynamic Programming in a Gridworld Example II\n",
    "<img src=\"./img/exp_4_1.png\" alt=\"gridworld_example\" width=\"500\">   \n",
    "\n",
    "(Implementation of Example 4.1 in Sutton & Barto textbook, 2nd edition.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cef8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ef4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.num_states = 16\n",
    "        self.num_actions = 4\n",
    "        self.trans_prop = np.array([[0, \"u\", 0, 0, 1], [0, \"d\", 0, 0, 1], [0, \"r\", 0, 0, 1], [0, \"l\", 0, 0, 1],\n",
    "                                    [1, \"u\", 1, -1, 1], [1, \"d\", 5, -1, 1], [1, \"r\", 2, -1, 1], [1, \"l\", 0, -1, 1],\n",
    "                                    [2, \"u\", 2, -1, 1], [2, \"d\", 6, -1, 1], [2, \"r\", 3, -1, 1], [2, \"l\", 1, -1, 1],\n",
    "                                    [3, \"u\", 3, -1, 1], [3, \"d\", 7, -1, 1], [3, \"r\", 3, -1, 1], [3, \"l\", 2, -1, 1],\n",
    "                                    [4, \"u\", 0, -1, 1], [4, \"d\", 8, -1, 1], [4, \"r\", 5, -1, 1], [4, \"l\", 4, -1, 1],\n",
    "                                    [5, \"u\", 1, -1, 1], [5, \"d\", 9, -1, 1], [5, \"r\", 6, -1, 1], [5, \"l\", 4, -1, 1],\n",
    "                                    [6, \"u\", 2, -1, 1], [6, \"d\", 10, -1, 1], [6, \"r\", 7, -1, 1], [6, \"l\", 5, -1, 1],\n",
    "                                    [7, \"u\", 3, -1, 1], [7, \"d\", 11, -1, 1], [7, \"r\", 7, -1, 1], [7, \"l\", 6, -1, 1],\n",
    "                                    [8, \"u\", 4, -1, 1], [8, \"d\", 12, -1, 1], [8, \"r\", 9, -1, 1], [8, \"l\", 8, -1, 1],\n",
    "                                    [9, \"u\", 5, -1, 1], [9, \"d\", 13, -1, 1], [9, \"r\", 10, -1, 1], [9, \"l\", 8, -1, 1],\n",
    "                                    [10, \"u\", 6, -1, 1], [10, \"d\", 14, -1, 1], [10, \"r\", 11, -1, 1],\n",
    "                                    [10, \"l\", 9, -1, 1],\n",
    "                                    [11, \"u\", 7, -1, 1], [11, \"d\", 0, -1, 1], [11, \"r\", 11, -1, 1],\n",
    "                                    [11, \"l\", 10, -1, 1],\n",
    "                                    [12, \"u\", 8, -1, 1], [12, \"d\", 12, -1, 1], [12, \"r\", 13, -1, 1],\n",
    "                                    [12, \"l\", 12, -1, 1],\n",
    "                                    [13, \"u\", 9, -1, 1], [13, \"d\", 13, -1, 1], [13, \"r\", 14, -1, 1],\n",
    "                                    [13, \"l\", 12, -1, 1],\n",
    "                                    [14, \"u\", 10, -1, 1], [14, \"d\", 14, -1, 1], [14, \"r\", 0, -1, 1],\n",
    "                                    [14, \"l\", 13, -1, 1],\n",
    "                                    [15, \"u\", 15, 0, 1], [15, \"d\", 15, 0, 1], [15, \"r\", 15, 0, 1], [15, \"l\", 15, 0, 1]\n",
    "                                    ])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, gamma=1.0):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.old_policy = None\n",
    "        self.policy = None\n",
    "        self.old_value_function = None\n",
    "        self.value_function = None\n",
    "        self.reset_policy()\n",
    "        self.reset_old_policy()\n",
    "        self.reset_value_function()\n",
    "        self.reset_old_value_function()\n",
    "\n",
    "    def policy_evaluation(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.old_value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.old_value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.old_value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.old_value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = self.policy[i, 0] * float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1) + \\\n",
    "                                     self.policy[i, 1] * float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) + \\\n",
    "                                     self.policy[i, 2] * float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3) + \\\n",
    "                                     self.policy[i, 3] * float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)\n",
    "\n",
    "    def policy_evaluation_ipu(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "        '''\n",
    "        We only keep the second value function to reuse the convergence cirterion for stopping as in the case with two value functions.\n",
    "        However this value function is not required in the algorithm and could be omitted likewise.\n",
    "        '''\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = self.policy[i, 0] * float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1) + \\\n",
    "                                     self.policy[i, 1] * float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) + \\\n",
    "                                     self.policy[i, 2] * float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3) + \\\n",
    "                                     self.policy[i, 3] * float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)\n",
    "\n",
    "    def value_iteration(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.old_value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.old_value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.old_value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.old_value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = np.max(np.array([float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1), float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) , float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3), float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)]))\n",
    "\n",
    "    def value_iteration_ipu(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = np.max(np.array([float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1), float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) , float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3), float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)]))\n",
    "\n",
    "    def policy_improvement(self, mdp):\n",
    "        self.old_policy = np.copy(self.policy)\n",
    "        values = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(1, 15):\n",
    "            neighborValues = [0, 0, 0, 0]\n",
    "            neighbors = mdp.trans_prop[i * 4:i * 4 + 4:1, 2]\n",
    "            for j in range(4):\n",
    "                neighborValues[j] = float(mdp.trans_prop[i * 4 + j, 3]) + self.gamma * float(values[int(neighbors[j])])\n",
    "\n",
    "            policyValues = np.flatnonzero(neighborValues == np.max(neighborValues))\n",
    "            for k in range(self.num_actions):\n",
    "                if k in policyValues:\n",
    "                    self.policy[i, k] = 1 / len(policyValues)\n",
    "                else:\n",
    "                    self.policy[i, k] = 0\n",
    "\n",
    "    def render_value_function(self):\n",
    "        test = False\n",
    "        if abs(np.min(self.value_function)) > 10:\n",
    "            test = True\n",
    "        if test:\n",
    "            print('-------------------------------------')\n",
    "        else:\n",
    "            print('---------------------------------')\n",
    "        out = '| '\n",
    "        for i in range(16):\n",
    "            token = float(np.round(self.value_function[i], 2))\n",
    "            convertedToken = \"{:.2f}\".format(token)\n",
    "            if token == 0:\n",
    "                out += \" \"\n",
    "            if test and token > -10:\n",
    "                out += \" \"\n",
    "            out += str(convertedToken) + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                if test:\n",
    "                    print('-------------------------------------')\n",
    "                else:\n",
    "                    print('---------------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def render_policy(self):\n",
    "        pol = np.copy(self.policy)\n",
    "        print('-----------------------------')\n",
    "        out = '| '\n",
    "        for i in range(self.num_states):\n",
    "            token = \"\"\n",
    "            if pol[i, 3] > 0:  # left\n",
    "                token += \"\\u2190\"\n",
    "            if pol[i, 0] > 0:  # up\n",
    "                token += \"\\u2191\"\n",
    "            if pol[i, 1] > 0:  # down\n",
    "                token += \"\\u2193\"\n",
    "            if pol[i, 2] > 0:  # right\n",
    "                token += \"\\u2192\"\n",
    "            if token == \"\":  # empty\n",
    "                token += \"  \"\n",
    "\n",
    "            if len(token) == 1:\n",
    "                token += '  '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 2:\n",
    "                token += ' '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 3:\n",
    "                token += ' '\n",
    "\n",
    "            out += token + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                print('-----------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def reset_value_function(self):\n",
    "        self.value_function = np.zeros((self.num_states, 1))\n",
    "\n",
    "    def reset_old_value_function(self):\n",
    "        self.old_value_function = np.ones((self.num_states, 1)) * 1e9\n",
    "\n",
    "    def reset_policy(self):\n",
    "        self.policy = np.ones((self.num_states, self.num_actions)) * (1.0 / self.num_actions)\n",
    "        self.policy[0, :] = 0\n",
    "        self.policy[-1, :] = 0\n",
    "\n",
    "    def reset_old_policy(self):\n",
    "        self.old_policy = np.zeros((self.num_states, self.num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfb0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP()\n",
    "agent = Agent(num_states=mdp.num_states, num_actions=mdp.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb801d3",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede32bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 89 iterations.\n",
      "Value Function:\n",
      "-------------------------------------\n",
      "|   0.00 | -13.90 | -19.84 | -21.83 | \n",
      "-------------------------------------\n",
      "| -13.90 | -17.86 | -19.85 | -19.84 | \n",
      "-------------------------------------\n",
      "| -19.84 | -19.85 | -17.86 | -13.90 | \n",
      "-------------------------------------\n",
      "| -21.83 | -19.84 | -13.90 |   0.00 | \n",
      "-------------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑   |  ←↓  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑→  |  ↓   |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 4 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 1 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "policy_stable = False\n",
    "threshold = 0.01\n",
    "j = 0\n",
    "while not policy_stable:\n",
    "    j += 1\n",
    "    print('*************************************************************************************')\n",
    "    print('Iteration ' + str(j))\n",
    "    print('*************************************************************************************')\n",
    "    i = 0\n",
    "    while True:\n",
    "        agent.policy_evaluation(mdp=mdp)\n",
    "        i += 1\n",
    "        if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold:\n",
    "            break\n",
    "    print(f'Policy evaluation converged after {i} iterations.')\n",
    "    print('Value Function:')\n",
    "    agent.render_value_function()\n",
    "\n",
    "    agent.policy_improvement(mdp=mdp)\n",
    "\n",
    "    if np.max(np.abs(agent.old_policy - agent.policy)) == 0:\n",
    "        policy_stable = True\n",
    "    print('Policy:')\n",
    "    agent.render_policy()\n",
    "\n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6bdfb",
   "metadata": {},
   "source": [
    "## Policy Iteration with in-place updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288c4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 62 iterations.\n",
      "Value Function:\n",
      "-------------------------------------\n",
      "|   0.00 | -13.93 | -19.91 | -21.90 | \n",
      "-------------------------------------\n",
      "| -13.93 | -17.92 | -19.91 | -19.91 | \n",
      "-------------------------------------\n",
      "| -19.91 | -19.91 | -17.93 | -13.95 | \n",
      "-------------------------------------\n",
      "| -21.90 | -19.91 | -13.95 |   0.00 | \n",
      "-------------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←   | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  |  ←   |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑   |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 3 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 1 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "policy_stable = False\n",
    "threshold = 0.01\n",
    "j = 0\n",
    "while not policy_stable:\n",
    "    j += 1\n",
    "    print('*************************************************************************************')\n",
    "    print('Iteration ' + str(j))\n",
    "    print('*************************************************************************************')\n",
    "    i = 0\n",
    "    while True:\n",
    "        agent.policy_evaluation_ipu(mdp=mdp)\n",
    "        i += 1\n",
    "        if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold:\n",
    "            break\n",
    "    print(f'Policy evaluation converged after {i} iterations.')\n",
    "    print('Value Function:')\n",
    "    agent.render_value_function()\n",
    "\n",
    "    agent.policy_improvement(mdp=mdp)\n",
    "\n",
    "    if np.max(np.abs(agent.old_policy - agent.policy)) == 0:\n",
    "        policy_stable = True\n",
    "    print('Policy:')\n",
    "    agent.render_policy()\n",
    "\n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190faa1d",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9386f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "Value iteration converged after 4 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "threshold = 0.01\n",
    "i = 0\n",
    "while True:\n",
    "    agent.value_iteration(mdp=mdp)\n",
    "    i += 1\n",
    "    if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold:\n",
    "        break\n",
    "print(f'Value iteration converged after {i} iterations.')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "'''\n",
    "This policy improvement step is not required by the algorithm.\n",
    "We just do it to update our policy matrix so we can plot the policy nicely.\n",
    "'''\n",
    "agent.policy_improvement(mdp=mdp)\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b4b52",
   "metadata": {},
   "source": [
    "## Value Iteration with in-place updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512f1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "Value iteration converged after 4 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "threshold = 0.01\n",
    "i = 0\n",
    "while True:\n",
    "    agent.value_iteration(mdp=mdp)\n",
    "    i += 1\n",
    "    if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold:\n",
    "        break\n",
    "print(f'Value iteration converged after {i} iterations.')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "'''\n",
    "This policy improvement step is not required by the algorithm.\n",
    "We just do it to update our policy matrix so we can plot the policy nicely.\n",
    "'''\n",
    "agent.policy_improvement(mdp=mdp)\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6146b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllbc_v1",
   "language": "python",
   "name": "rllbc_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
