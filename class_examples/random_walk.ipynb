{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67ad422",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Linear Value Function Approximation\n",
    "*Code adapted from: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b52aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833561f",
   "metadata": {},
   "source": [
    "## Random Walk Example\n",
    "<center><img width=\"800\" src=./img/random_walk.png /></center>\n",
    "\n",
    "**Example 9.1 in Sutton & Barto:**\n",
    "Consider a\n",
    "1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and\n",
    "144). The states are numbered from 1 to 1000, left to right, and all episodes begin near\n",
    "the center, in state 500. State transitions are from the current state to one of the 100\n",
    "neighboring states to its left, or to one of the 100 neighboring states to its right, all with\n",
    "equal probability. Of course, if the current state is near an edge, then there may be fewer\n",
    "than 100 neighbors on that side of it. In this case, all the probability that would have\n",
    "gone into those missing neighbors goes into the probability of terminating on that side\n",
    "(thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance\n",
    "of terminating on the right). As usual, termination on the left produces a reward of -1\n",
    ", and termination on the right produces a reward of +1. All other transitions have a\n",
    "reward of zero. We use this task as a running example throughout this section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07cde61",
   "metadata": {},
   "source": [
    "Environment Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a98e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set rewards and temporal discounting\n",
    "REWARD_LEFT = -1\n",
    "REWARD_RIGHT = 1\n",
    "REWARD_BETWEEN = 0\n",
    "\n",
    "GAMMA = 1\n",
    "\n",
    "# # of states except for terminal states\n",
    "N_STATES = 1000\n",
    "# all states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "# start from a central state\n",
    "START_STATE = 500\n",
    "# terminal states\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# possible actions\n",
    "ACTION_LEFT = -1\n",
    "ACTION_RIGHT = 1\n",
    "ACTIONS = [ACTION_LEFT, ACTION_RIGHT]\n",
    "# maximum stride for an action\n",
    "STEP_RANGE = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db32e3",
   "metadata": {},
   "source": [
    "## Tabular Dynamic Programming to compute 'true' Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_true_value():\n",
    "\n",
    "    true_value = np.zeros((N_STATES + 2))\n",
    "\n",
    "    # Dynamic programming to find the true state values\n",
    "    while True:\n",
    "        old_value = np.copy(true_value)\n",
    "        for state in STATES:\n",
    "            true_value[state] = 0\n",
    "            for action in ACTIONS:\n",
    "                for step in range(1, STEP_RANGE + 1):\n",
    "                    step *= action\n",
    "                    next_state = state + step\n",
    "                    next_state = max(min(next_state, N_STATES + 1), 0)\n",
    "                    # asynchronous update for faster convergence\n",
    "                    if next_state == 0:\n",
    "                        true_value[state] += 1.0 / (2 * STEP_RANGE) * (REWARD_LEFT)\n",
    "                    elif next_state == N_STATES + 1:\n",
    "                        true_value[state] += 1.0 / (2 * STEP_RANGE) * (REWARD_RIGHT)\n",
    "                    else:\n",
    "                        true_value[state] += 1.0 / (2 * STEP_RANGE) * (REWARD_BETWEEN + GAMMA * true_value[next_state])\n",
    "\n",
    "        error = np.sum(np.abs(old_value - true_value))\n",
    "        print('The remaining error is: ' + str(np.round(error, 3)))\n",
    "        if error < 1e-2:\n",
    "            break\n",
    "    # correct the state value for terminal states to 0\n",
    "    true_value[0] = true_value[-1] = 0\n",
    "\n",
    "    return true_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73896d",
   "metadata": {},
   "source": [
    "## Random Walk Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an @action at @state, return new state and reward for this transition\n",
    "def step(state, action):\n",
    "    step = np.random.randint(1, STEP_RANGE + 1)\n",
    "    step *= action\n",
    "    state += step\n",
    "    state = max(min(state, N_STATES + 1), 0)\n",
    "    if state == 0:\n",
    "        reward = REWARD_LEFT\n",
    "    elif state == N_STATES + 1:\n",
    "        reward = REWARD_RIGHT\n",
    "    else:\n",
    "        reward = REWARD_BETWEEN\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb76a3",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an action, following random policy\n",
    "def get_action():\n",
    "    if np.random.binomial(1, 0.5) == 1:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b368d7d",
   "metadata": {},
   "source": [
    "## Different Types of Value Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b174096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a wrapper class for aggregation value function\n",
    "class ValueFunction:\n",
    "    # @num_of_groups: # of aggregations\n",
    "    def __init__(self, num_of_groups):\n",
    "        self.num_of_groups = num_of_groups\n",
    "        self.group_size = N_STATES // num_of_groups\n",
    "\n",
    "        # thetas\n",
    "        self.params = np.zeros(num_of_groups)\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        if state in END_STATES:\n",
    "            return 0\n",
    "        group_index = (state - 1) // self.group_size\n",
    "        return self.params[group_index]\n",
    "\n",
    "    # update parameters\n",
    "    # @delta: step size * (target - old estimation)\n",
    "    # @state: state of current sample\n",
    "    def update(self, delta, state):\n",
    "        group_index = (state - 1) // self.group_size\n",
    "        self.params[group_index] += delta\n",
    "\n",
    "# a wrapper class for tile coding value function\n",
    "class TilingsValueFunction:\n",
    "    # @num_of_tilings: # of tilings\n",
    "    # @tileWidth: each tiling has several tiles, this parameter specifies the width of each tile\n",
    "    # @tilingOffset: specifies how tilings are put together\n",
    "    def __init__(self, numOfTilings, tileWidth, tilingOffset):\n",
    "        self.numOfTilings = numOfTilings\n",
    "        self.tileWidth = tileWidth\n",
    "        self.tilingOffset = tilingOffset\n",
    "\n",
    "        # To make sure that each sate is covered by same number of tiles,\n",
    "        # we need one more tile for each tiling\n",
    "        self.tilingSize = N_STATES // tileWidth + 1\n",
    "\n",
    "        # weight for each tile\n",
    "        self.params = np.zeros((self.numOfTilings, self.tilingSize))\n",
    "\n",
    "        # For performance, only track the starting position for each tiling\n",
    "        # As we have one more tile for each tiling, the starting position will be negative\n",
    "        self.tilings = np.arange(-tileWidth + 1, 0, tilingOffset)\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        stateValue = 0.0\n",
    "        # go through all the tilings\n",
    "        for tilingIndex in range(0, len(self.tilings)):\n",
    "            # find the active tile in current tiling\n",
    "            tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth\n",
    "            stateValue += self.params[tilingIndex, tileIndex]\n",
    "        return stateValue\n",
    "\n",
    "    # update parameters\n",
    "    # @delta: step size * (target - old estimation)\n",
    "    # @state: state of current sample\n",
    "    def update(self, delta, state):\n",
    "\n",
    "        # each state is covered by same number of tilings\n",
    "        # so the delta should be divided equally into each tiling (tile)\n",
    "        delta /= self.numOfTilings\n",
    "\n",
    "        # go through all the tilings\n",
    "        for tilingIndex in range(0, len(self.tilings)):\n",
    "            # find the active tile in current tiling\n",
    "            tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth\n",
    "            self.params[tilingIndex, tileIndex] += delta\n",
    "\n",
    "# a wrapper class for polynomial / Fourier -based value function\n",
    "POLYNOMIAL_BASES = 0\n",
    "FOURIER_BASES = 1\n",
    "class BasesValueFunction:\n",
    "    # @order: # of bases, each function also has one more constant parameter (called bias in machine learning)\n",
    "    # @type: polynomial bases or Fourier bases\n",
    "    def __init__(self, order, type):\n",
    "        self.order = order\n",
    "        self.weights = np.zeros(order + 1)\n",
    "\n",
    "        # set up bases function\n",
    "        self.bases = []\n",
    "        if type == POLYNOMIAL_BASES:\n",
    "            for i in range(0, order + 1):\n",
    "                self.bases.append(lambda s, i=i: pow(s, i))\n",
    "        elif type == FOURIER_BASES:\n",
    "            for i in range(0, order + 1):\n",
    "                self.bases.append(lambda s, i=i: np.cos(i * np.pi * s))\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        # map the state space into [0, 1]\n",
    "        state /= float(N_STATES)\n",
    "        # get the feature vector\n",
    "        feature = np.asarray([func(state) for func in self.bases])\n",
    "        return np.dot(self.weights, feature)\n",
    "\n",
    "    def update(self, delta, state):\n",
    "        # map the state space into [0, 1]\n",
    "        state /= float(N_STATES)\n",
    "        # get derivative value\n",
    "        derivative_value = np.asarray([func(state) for func in self.bases])\n",
    "        self.weights += delta * derivative_value\n",
    "\n",
    "class PolynomialValueFunction:\n",
    "    def __init__(self, order):\n",
    "        self.order = order\n",
    "        self.weights = np.zeros(order +1)\n",
    "\n",
    "    def featurize(self, state):\n",
    "        feature = np.zeros(self.order + 1)\n",
    "        state /= float(N_STATES)\n",
    "        for i in range(self.order +1):\n",
    "            feature[i] = state ** i\n",
    "        return feature\n",
    "\n",
    "    def value(self, state):\n",
    "        return np.dot(self.featurize(state), self.weights)\n",
    "\n",
    "    def update(self, delta, state):\n",
    "        grad_v = self.featurize(state)\n",
    "        self.weights += delta * grad_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43121cce",
   "metadata": {},
   "source": [
    "## Gradient Monte-Carlo Algorithm \n",
    "(Sutton & Barto page 202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_monte_carlo(value_function, alpha, distribution=None):\n",
    "    state = START_STATE\n",
    "    trajectory = np.array([[state, 0. ]])\n",
    "\n",
    "    # We assume gamma = 1, so return is just the same as the latest reward\n",
    "    #reward = 0.0\n",
    "    while state not in END_STATES:\n",
    "        action = get_action()\n",
    "        next_state, reward = step(state, action)\n",
    "        trajectory = np.append(trajectory, np.array([[next_state, reward]]), axis=0)\n",
    "        state = next_state\n",
    "\n",
    "    # Gradient update for each state in this trajectory\n",
    "\n",
    "    ret = 0.0\n",
    "    traj_len = trajectory.shape[0]\n",
    "    for i in range(traj_len-1):\n",
    "        state = int(trajectory[traj_len - 2 - i, 0])\n",
    "        reward = int(trajectory[traj_len -1 - i, 1])\n",
    "        ret = reward + GAMMA*ret\n",
    "        delta = alpha * (ret - value_function.value(state))\n",
    "        value_function.update(delta, state)\n",
    "        if distribution is not None:\n",
    "            distribution[state] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ebdc5",
   "metadata": {},
   "source": [
    "## Semi-gradient TD(0) Algorithm\n",
    "(Sutton & Barto page 203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_td_0(value_function, alpha, distribution=None):\n",
    "    state = START_STATE\n",
    "\n",
    "    while not state in END_STATES:\n",
    "        action = get_action()\n",
    "        next_state, reward = step(state, action)\n",
    "        if next_state == 0:\n",
    "            delta = alpha*(reward  - value_function.value(state))\n",
    "        elif next_state == N_STATES+ 1:\n",
    "            delta = alpha*(reward  - value_function.value(state))\n",
    "        else:\n",
    "            delta = alpha*(reward + GAMMA * value_function.value(next_state) - value_function.value(state))\n",
    "        value_function.update(delta, state)\n",
    "        if distribution is not None:\n",
    "            distribution[state] += 1\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144346c",
   "metadata": {},
   "source": [
    "### Compute true state value function using tabular dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98576d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_value = compute_true_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48d44b",
   "metadata": {},
   "source": [
    "### MC learning with state aggregation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = int(1e5)\n",
    "alpha = 5e-5\n",
    "\n",
    "# we have 10 aggregations in this example, each has 100 states\n",
    "value_function = ValueFunction(10)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    gradient_monte_carlo(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(STATES, state_values, label='Approximate MC value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(STATES, distribution[1: -1], label='State distribution')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f8317",
   "metadata": {},
   "source": [
    "### TD Learning with state aggregation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = int(1e5)\n",
    "alpha = 5e-5\n",
    "\n",
    "# we have 10 aggregations in this example, each has 100 states\n",
    "value_function = ValueFunction(10)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    semi_gradient_td_0(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(STATES, state_values, label='Approximate TD value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(STATES, distribution[1: -1], label='State distribution')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44afb4",
   "metadata": {},
   "source": [
    "### MC learning with polynomial feature (affine function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = int(5e5)\n",
    "alpha = 2e-5\n",
    "\n",
    "# polynomial of order 1\n",
    "value_function = BasesValueFunction(1, POLYNOMIAL_BASES)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    gradient_monte_carlo(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(STATES, state_values, label='Approximate MC value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf17449",
   "metadata": {},
   "source": [
    "### TD learning with polynomial feature (affine function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457feb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = int(5e5)\n",
    "alpha = 2e-5\n",
    "\n",
    "# polynomial of order 1\n",
    "value_function = BasesValueFunction(1, POLYNOMIAL_BASES)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    semi_gradient_td_0(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(STATES, state_values, label='Approximate TD value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "true_value = compute_true_value()\n",
    "episodes = int(5e5)\n",
    "alpha = 2e-5\n",
    "\n",
    "# polynomial of order 1\n",
    "value_function = BasesValueFunction(1, POLYNOMIAL_BASES)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    semi_gradient_td_0(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(STATES, state_values, label='Approximate TD value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e93993",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = int(1e6)\n",
    "alpha = 1e-4\n",
    "\n",
    "# polynomial of order 5\n",
    "value_function = BasesValueFunction(5, POLYNOMIAL_BASES)\n",
    "\n",
    "distribution = np.zeros(N_STATES + 2)\n",
    "for ep in tqdm(range(episodes)):\n",
    "    semi_gradient_td_0(value_function, alpha, distribution)\n",
    "    \n",
    "distribution /= np.sum(distribution)\n",
    "state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(STATES, state_values, label='Approximate TD value')\n",
    "plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
