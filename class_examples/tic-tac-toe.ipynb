{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2306869b-5605-4781-922d-1c0ab33ed7e7",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29193c92-aafc-4e2a-bcc3-0a46fcc1d1e3",
   "metadata": {},
   "source": [
    "# Tic-Tac-Toe\n",
    "*(Adapted from: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4b506-6687-4ecd-9f9b-1c5ccf8a454b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)           #\n",
    "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649360b8-2d43-41f7-af4e-0e4f9b2018ef",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python Libraries we need to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5491d-fcbf-47bb-a016-fcc10ccdea6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pygame\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621fce3-3930-4050-a107-857386411565",
   "metadata": {},
   "source": [
    "# Source Code\n",
    "We will make sense of this in the upcoming lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc209373-89c1-4390-9ef2-584dbd3ecbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
    "\n",
    "HIST_LEN = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af80f-324a-4322-8177-c5dd76b095c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self):\n",
    "        # the board is represented by an n * n array,\n",
    "        # 1 represents a chessman of the player who moves first,\n",
    "        # -1 represents a chessman of another player\n",
    "        # 0 represents an empty position\n",
    "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.winner = None\n",
    "        self.hash_val = None\n",
    "        self.end = None\n",
    "        self.window_surface = None\n",
    "        self.window_size = [480, 480]\n",
    "        self.cell_size = [160, 160]\n",
    "        self.metadata = {\"render_fps\": 10}\n",
    "\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.render_mode = 'rgb_array' #'human' #'human' #\n",
    "\n",
    "\n",
    "        rel_path = 'custom_envs/tabular_envs/envs/img'\n",
    "        img_dir = os.path.join(Path(os.getcwd()), 'img')\n",
    "\n",
    "\n",
    "        file_name = file_name = os.path.join(img_dir,  \"ttt_empty.png\")\n",
    "        self.empty_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        file_name = os.path.join(img_dir, \"ttt_cross.png\")\n",
    "        self.cross_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "\n",
    "        file_name = os.path.join(img_dir, \"ttt_circle.png\")\n",
    "        self.circle_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "    # compute the hash value for one state, it's unique\n",
    "    def hash(self):\n",
    "        if self.hash_val is None:\n",
    "            self.hash_val = 0\n",
    "            for i in np.nditer(self.data):\n",
    "                self.hash_val = self.hash_val * 3 + i + 1\n",
    "        return self.hash_val\n",
    "\n",
    "    # check whether a player has won the game, or it's a tie\n",
    "    def is_end(self):\n",
    "        if self.end is not None:\n",
    "            return self.end\n",
    "        results = []\n",
    "        # check row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results.append(np.sum(self.data[i, :]))\n",
    "        # check columns\n",
    "        for i in range(BOARD_COLS):\n",
    "            results.append(np.sum(self.data[:, i]))\n",
    "\n",
    "        # check diagonals\n",
    "        trace = 0\n",
    "        reverse_trace = 0\n",
    "        for i in range(BOARD_ROWS):\n",
    "            trace += self.data[i, i]\n",
    "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n",
    "        results.append(trace)\n",
    "        results.append(reverse_trace)\n",
    "\n",
    "        for result in results:\n",
    "            if result == 3:\n",
    "                self.winner = 1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "            if result == -3:\n",
    "                self.winner = -1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "\n",
    "        # whether it's a tie\n",
    "        sum_values = np.sum(np.abs(self.data))\n",
    "        if sum_values == BOARD_SIZE:\n",
    "            self.winner = 0\n",
    "            self.end = True\n",
    "            return self.end\n",
    "\n",
    "        # game is still going on\n",
    "        self.end = False\n",
    "        return self.end\n",
    "\n",
    "    # @symbol: 1 or -1\n",
    "    # put chessman symbol in position (i, j)\n",
    "    def next_state(self, i, j, symbol):\n",
    "        new_state = State()\n",
    "        new_state.data = np.copy(self.data)\n",
    "        new_state.data[i, j] = symbol\n",
    "        return new_state\n",
    "\n",
    "    # print the board\n",
    "    def render_state(self):\n",
    "        mode = self.render_mode\n",
    "        if self.window_surface is None:\n",
    "            pygame.init()\n",
    "\n",
    "            if mode == 'human':\n",
    "                pygame.display.init()\n",
    "                pygame.display.set_caption('Tic-Tac-Toe')\n",
    "                self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "            elif mode == 'rgb_array':\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "        assert (\n",
    "                self.window_surface is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "\n",
    "\n",
    "        for y in range(BOARD_ROWS):\n",
    "            for x in range(BOARD_COLS):\n",
    "                pos = (y * self.cell_size[0], x * self.cell_size[1])\n",
    "                rect = (*pos, *self.cell_size)\n",
    "\n",
    "\n",
    "                if self.data[x, y] == 1:\n",
    "                    self.window_surface.blit(self.circle_img, pos)\n",
    "                elif self.data[x, y] == -1:\n",
    "                    self.window_surface.blit(self.cross_img, pos)\n",
    "                elif self.data[x, y] == 0:\n",
    "                    self.window_surface.blit(self.empty_img, pos)\n",
    "\n",
    "                pygame.draw.rect(self.window_surface, (0, 0, 0), rect, 5)\n",
    "        if mode == 'human':\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        elif mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window_surface)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "def get_all_states_impl(current_state, current_symbol, all_states):\n",
    "    for i in range(BOARD_ROWS):\n",
    "        for j in range(BOARD_COLS):\n",
    "            if current_state.data[i][j] == 0:\n",
    "                new_state = current_state.next_state(i, j, current_symbol)\n",
    "                new_hash = new_state.hash()\n",
    "                if new_hash not in all_states:\n",
    "                    is_end = new_state.is_end()\n",
    "                    all_states[new_hash] = (new_state, is_end)\n",
    "                    if not is_end:\n",
    "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
    "\n",
    "\n",
    "def get_all_states():\n",
    "    current_symbol = 1\n",
    "    current_state = State()\n",
    "    all_states = dict()\n",
    "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
    "    get_all_states_impl(current_state, current_symbol, all_states)\n",
    "    return all_states\n",
    "\n",
    "\n",
    "# all possible board configurations\n",
    "all_states = get_all_states()\n",
    "\n",
    "\n",
    "class Judger:\n",
    "    # @player1: the player who will move first, its chessman will be 1\n",
    "    # @player2: another player with a chessman -1\n",
    "    def __init__(self, player1, player2):\n",
    "        self.p1 = player1\n",
    "        self.p2 = player2\n",
    "        self.current_player = None\n",
    "        self.p1_symbol = 1\n",
    "        self.p2_symbol = -1\n",
    "        self.p1.set_symbol(self.p1_symbol)\n",
    "        self.p2.set_symbol(self.p2_symbol)\n",
    "        self.current_state = State()\n",
    "\n",
    "    def reset(self):\n",
    "        self.p1.reset()\n",
    "        self.p2.reset()\n",
    "\n",
    "    def alternate(self):\n",
    "        while True:\n",
    "            yield self.p1\n",
    "            yield self.p2\n",
    "\n",
    "    #render_state: if True, print each board during the game\n",
    "    def play(self, render_state=False, render_final_state=False, show_game=False):\n",
    "        alternator = self.alternate()\n",
    "        self.reset()\n",
    "        current_state = State()\n",
    "        if current_state.render_mode == 'rgb_array' and render_state:\n",
    "            self.create_fig()\n",
    "        self.p1.set_state(current_state)\n",
    "        self.p2.set_state(current_state)\n",
    "        if render_state:\n",
    "            out = current_state.render_state()\n",
    "            if out is not None:\n",
    "                self.update_fig(out)\n",
    "        while True:\n",
    "            player = next(alternator)\n",
    "            i, j, symbol = player.act()\n",
    "            while i == -1:  # -1 if a key is pressed, that is not in the keylist\n",
    "                i, j, symbol = player.act()\n",
    "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
    "            current_state, is_end = all_states[next_state_hash]\n",
    "            self.p1.set_state(current_state)\n",
    "            self.p2.set_state(current_state)\n",
    "            if show_game:\n",
    "                time.sleep(0.5)\n",
    "            if render_state:\n",
    "                out = current_state.render_state()\n",
    "                if out is not None:\n",
    "                    self.update_fig(out)\n",
    "            if is_end:\n",
    "                if render_final_state:\n",
    "                    out = current_state.render_state()\n",
    "                    if out is not None:\n",
    "                        self.update_fig(out)\n",
    "                return current_state.winner\n",
    "\n",
    "    def create_fig(self):\n",
    "        self.fig = plt.figure(figsize=(5, 5))\n",
    "        self.hfig = display(self.fig, display_id=True)\n",
    "\n",
    "    def update_fig(self, frame):\n",
    "        plt.clf()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(frame)\n",
    "        self.hfig.update(self.fig)\n",
    "        time.sleep(0.25 / 1)\n",
    "\n",
    "\n",
    "\n",
    "# AI player\n",
    "class Player:\n",
    "    # @step_size: the step size to update estimations\n",
    "    # @epsilon: the probability to explore\n",
    "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
    "        self.estimations = dict()\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "        self.symbol = 0\n",
    "        self.cell_values = []\n",
    "        self.win_rates = []\n",
    "        self.results = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.states.append(state)\n",
    "        self.greedy.append(True)\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "        for hash_val in all_states:\n",
    "            state, is_end = all_states[hash_val]\n",
    "            if is_end:\n",
    "                if state.winner == self.symbol:\n",
    "                    self.estimations[hash_val] = 1.0\n",
    "                elif state.winner == 0:\n",
    "                    # we need to distinguish between a tie and a lose\n",
    "                    self.estimations[hash_val] = 0.5\n",
    "                else:\n",
    "                    self.estimations[hash_val] = 0\n",
    "            else:\n",
    "                self.estimations[hash_val] = 0.5\n",
    "\n",
    "    # update value estimation\n",
    "    def backup(self):\n",
    "        states = [state.hash() for state in self.states]\n",
    "\n",
    "        for i in reversed(range(len(states) - 1)):\n",
    "            state = states[i]\n",
    "            td_error = self.greedy[i] * (\n",
    "                    self.estimations[states[i + 1]] - self.estimations[state]\n",
    "            )\n",
    "            self.estimations[state] += self.step_size * td_error\n",
    "\n",
    "    # choose an action based on the state\n",
    "    def act(self):\n",
    "        state = self.states[-1]\n",
    "        next_states = []\n",
    "        next_positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if state.data[i, j] == 0:\n",
    "                    next_positions.append([i, j])\n",
    "                    next_states.append(state.next_state(\n",
    "                        i, j, self.symbol).hash())\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = next_positions[np.random.randint(len(next_positions))]\n",
    "            action.append(self.symbol)\n",
    "            self.greedy[-1] = False\n",
    "            return action\n",
    "\n",
    "        values = []\n",
    "        for hash_val, pos in zip(next_states, next_positions):\n",
    "            values.append((self.estimations[hash_val], pos))\n",
    "\n",
    "        self.cell_values = values\n",
    "        # to select one of the actions of equal value at random due to Python's sort is stable\n",
    "        np.random.shuffle(values)\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "        action = values[0][1]\n",
    "        action.append(self.symbol)\n",
    "        return action\n",
    "\n",
    "    def save_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
    "            pickle.dump(self.estimations, f)\n",
    "\n",
    "    def load_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
    "            self.estimations = pickle.load(f)\n",
    "\n",
    "    def load_pretrained_policy(self):\n",
    "        with open('./utils/policy_pretrained.bin', 'rb') as f:\n",
    "            self.estimations = pickle.load(f)\n",
    "\n",
    "    def save_metrics(self):\n",
    "        with open('metrics_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
    "            metrics = {'win_rates': self.win_rates, 'results': self.results}\n",
    "            pickle.dump(metrics, f)\n",
    "\n",
    "    def load_metrics(self):\n",
    "        with open('metrics_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "            self.win_rates = metrics['win_rates']\n",
    "            self.results = metrics['results']\n",
    "\n",
    "\n",
    "# human interface\n",
    "# input a number to put a chessman\n",
    "# | q | w | e |\n",
    "# | a | s | d |\n",
    "# | z | x | c |\n",
    "class HumanPlayer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.symbol = None\n",
    "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def act(self):\n",
    "        self.state.render_state()\n",
    "        key = input(\"Input your position:\")\n",
    "        if key in self.keys:  # if a legal key was pressed\n",
    "            data = self.keys.index(key)\n",
    "        else:\n",
    "            print(\"Invalid input! Please try again!\")\n",
    "            return -1, -1, self.symbol\n",
    "\n",
    "        i = data // BOARD_COLS\n",
    "        j = data % BOARD_COLS\n",
    "\n",
    "        if self.state.data[i][j] == 0:  # if field was not chosen before\n",
    "            return i, j, self.symbol\n",
    "        else:\n",
    "            print(\"This field is already occupied! Please try again!\")\n",
    "            return -1, -1, self.symbol\n",
    "\n",
    "\n",
    "''' Trains the players '''\n",
    "def train(epochs, print_every_n=5000):\n",
    "    player1 = Player(epsilon=0.01)\n",
    "    player2 = Player(epsilon=0.01)\n",
    "    judger = Judger(player1, player2)\n",
    "    for i in tqdm(range(1, epochs + 1)):\n",
    "        winner = judger.play(render_state=False)\n",
    "        if winner == 1:\n",
    "            player1.results.append(1)\n",
    "            player2.results.append(0)\n",
    "        elif winner == -1:\n",
    "            player1.results.append(0)\n",
    "            player2.results.append(1)\n",
    "        else:\n",
    "            player1.results.append(0)\n",
    "            player2.results.append(0)\n",
    "        if i % print_every_n == 0:\n",
    "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1.win_rates[-1], player2.win_rates[-1]))\n",
    "\n",
    "        hist = min(len(player1.results), HIST_LEN)\n",
    "        player1.win_rates.append(sum(player1.results[-hist:])/hist)\n",
    "        player2.win_rates.append(sum(player2.results[-hist:])/hist)\n",
    "\n",
    "        player1.backup()\n",
    "        player2.backup()\n",
    "        judger.reset()\n",
    "    player1.save_policy()\n",
    "    player1.save_metrics()\n",
    "    player2.save_policy()\n",
    "    player2.save_metrics()\n",
    "    return player1, player2\n",
    "\n",
    "\n",
    "def retrain(epochs, print_every_n=5000):\n",
    "    player1 = Player(epsilon=0.01)\n",
    "    player2 = Player(epsilon=0.01)\n",
    "    judger = Judger(player1, player2)\n",
    "    player1.load_policy()\n",
    "    player1.load_metrics()\n",
    "    player2.load_policy()\n",
    "    player2.load_metrics()\n",
    "    for i in tqdm(range(1, epochs + 1)):\n",
    "        winner = judger.play(render_state=False)\n",
    "        if winner == 1:\n",
    "            player1.results.append(1)\n",
    "            player2.results.append(0)\n",
    "        elif winner == -1:\n",
    "            player1.results.append(0)\n",
    "            player2.results.append(1)\n",
    "        else:\n",
    "            player1.results.append(0)\n",
    "            player2.results.append(0)\n",
    "        if i % print_every_n == 0:\n",
    "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1.win_rates[-1], player2.win_rates[-1]))\n",
    "\n",
    "        hist = min(len(player1.results), HIST_LEN)\n",
    "        player1.win_rates.append(sum(player1.results[-hist:]) / hist)\n",
    "        player2.win_rates.append(sum(player2.results[-hist:]) / hist)\n",
    "\n",
    "        player1.backup()\n",
    "        player2.backup()\n",
    "        judger.reset()\n",
    "    player1.save_policy()\n",
    "    player1.save_metrics()\n",
    "    player2.save_policy()\n",
    "    player2.save_metrics()\n",
    "    return player1, player2\n",
    "\n",
    "\n",
    "''' Lets the players play against each other '''\n",
    "def compete(turns):\n",
    "    player1 = Player(epsilon=0)\n",
    "    player2 = Player(epsilon=0)\n",
    "    judger = Judger(player1, player2)\n",
    "    player1.load_policy()\n",
    "    player2.load_policy()\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for _ in range(turns):\n",
    "        winner = judger.play()\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        judger.reset()\n",
    "    print('%d turns, player 1 winrate %.02f, player 2 winrate %.02f' % (turns, player1_win / turns, player2_win / turns))\n",
    "\n",
    "\n",
    "# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n",
    "# So we test whether the AI can guarantee at least a tie if it goes second.\n",
    "def play(load_pretrained=False):\n",
    "    cont = True\n",
    "    while cont:\n",
    "        player1 = HumanPlayer()\n",
    "        player2 = Player(epsilon=0)\n",
    "        judger = Judger(player1, player2)\n",
    "        if load_pretrained:\n",
    "            player2.load_pretrained_policy()\n",
    "        else:\n",
    "            player2.load_policy()\n",
    "        winner = judger.play(render_state=True, render_final_state=True, show_game=True)\n",
    "        if winner == player2.symbol:\n",
    "            print(\"You lose!\")\n",
    "            cont = continue_input()\n",
    "        elif winner == player1.symbol:\n",
    "            print(\"You win!\")\n",
    "            cont = continue_input()\n",
    "        else:\n",
    "            print(\"It is a tie!\")\n",
    "            cont = continue_input()\n",
    "\n",
    "def continue_input():\n",
    "    key = input(\"Press 'c' to continue or 'q' to quit:\")\n",
    "    if key == 'c':\n",
    "        return True\n",
    "    elif key == 'q':\n",
    "        return False\n",
    "    else:\n",
    "        print('Faulty input!')\n",
    "        continue_input()\n",
    "\n",
    "        \n",
    "def render_training(winrate1, winrate2, nbr_games):\n",
    "    fig = plt.figure()\n",
    "    # plot of player 1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.linspace(0, nbr_games, nbr_games), winrate1, color=\"blue\")\n",
    "    plt.xlabel(\"Number of Runs\")\n",
    "    plt.ylabel(\"Winrate\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\"Player 1\")\n",
    "    # Plot of player 2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.linspace(0, nbr_games, nbr_games), winrate2, color=\"red\")\n",
    "    plt.xlabel(\"Number of Runs\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\"Player 2\")\n",
    "    #fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef214634-98b4-4a21-9ac3-c71cc83060c2",
   "metadata": {},
   "source": [
    "# Test Setup\n",
    "Two Tic-Tac-Toe agents are trained in self play for 100,000 games.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e4c1c-0b7a-4658-8f24-db4b6d1dd1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbr_initial_training_games = 50\n",
    "nbr_retraining_games = 99950\n",
    "nbr_evaluation_games = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6b53d-53c9-4dd5-894a-359711deb55e",
   "metadata": {},
   "source": [
    "### Initial Training\n",
    "\n",
    "We first train the agents for 50 games and check what they have learned.\n",
    "\n",
    "We firstly plot the win rate during training, meaning the percentage of games the respective agent won. It is important to note, that if both agents act optimally, nether of them can win the game and all games will result in a tie. Thus, optimal behavior corresponds to both agents have a win rate of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8bd0f-149c-4f46-a276-ddd450071e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train agents for ' + str(nbr_initial_training_games) + ' games.')\n",
    "player1, player2 = train(nbr_initial_training_games)\n",
    "render_training(player1.win_rates, player2.win_rates, nbr_initial_training_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0992b3-7bda-42db-985b-14cb06b84105",
   "metadata": {},
   "source": [
    "### Evaluation in self-play\n",
    "\n",
    "Now we check the winrates of both agents, if we let them compete for 100 games without learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38540ebd-e6c0-40a0-a6a4-ed343917b1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Evaluate for ' + str(nbr_evaluation_games) + ' games of self play.')\n",
    "compete(nbr_evaluation_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f106005-3f9a-4a0f-9974-94629a5ed51f",
   "metadata": {},
   "source": [
    "### Evaluation with human interaction\n",
    "\n",
    "Let's try it ourselves! We play against agent 2. Our token is the blue circle, the agent's token is the red cross.\n",
    "\n",
    "*Note*: The game is based on the QWERTY-layout, so to use the bottom-left corner with a QWERTZ-layout, you have to press *z* instead of *y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd4f9f-e277-4e1c-bf30-f55f5625736e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Play against agent trained for ' + str(nbr_initial_training_games) + ' games.')\n",
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ee2a4-398a-46a3-b67b-eaaea24f25dd",
   "metadata": {},
   "source": [
    "### Continuation of Training\n",
    "\n",
    "Well, seems the agent hasn't really gotten the game, yet. Let's train both agents for more games and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da686bd-1721-40ea-998a-3295ceb0d0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Continue training agents for ' + str(nbr_retraining_games) + ' games.')\n",
    "player1, player2 = retrain(nbr_retraining_games)  # train a lot\n",
    "render_training(player1.win_rates, player2.win_rates, nbr_initial_training_games + nbr_retraining_games)\n",
    "print('Evaluate for ' + str(nbr_evaluation_games) + ' games of self play.')\n",
    "compete(nbr_evaluation_games)\n",
    "print('Play against agent trained for ' + str(nbr_initial_training_games + nbr_retraining_games) + ' games.')\n",
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5649f-a74f-42fc-bfaa-949c6770fcac",
   "metadata": {},
   "source": [
    "### Play against pretrained agent\n",
    "To speed things up a little, we have prepared a pretrained agent to play against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49a217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c304c9b-637b-4909-8424-7775977f96aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "play(load_pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
