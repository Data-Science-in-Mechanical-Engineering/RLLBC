{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e4388b",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1244e",
   "metadata": {},
   "source": [
    "# Bellman Optimality Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2afcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d365c3",
   "metadata": {},
   "source": [
    "## Bellman optimality equations - Recycling Robot\n",
    "<img src=\"./img/recycling_bot.png\" alt=\"Recycling_Bot\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65df67f",
   "metadata": {},
   "source": [
    "MDP Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0be5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_S = 3\n",
    "R_W = 1\n",
    "R_E = -30\n",
    "\n",
    "ALPHA = 0.9\n",
    "BETA = 0.5\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b35ca",
   "metadata": {},
   "source": [
    "Formulate Bellman optimality equations as a optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e402ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell_opt_eq_rb(v):\n",
    "    v_h = v[0]\n",
    "    v_l = v[1]\n",
    "\n",
    "    F = np.empty(2)\n",
    "    F[0] = v_h - max(R_S + GAMMA*(ALPHA*v_h + (1-ALPHA)*v_l), R_W + GAMMA * v_h)\n",
    "    F[1] = v_l - max(BETA*R_S + R_E*(1-BETA) + GAMMA*((1-BETA)*v_h + BETA*v_l), R_W + GAMMA*v_l, GAMMA*v_h)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb27bf2",
   "metadata": {},
   "source": [
    "Solve for the optimal value funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fafe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal state value high: 27.52\n",
      "optimal state value low: 24.77\n",
      "remaining_optimization_loss: 0.0\n",
      "optimal action in state high: search\n",
      "optimal action in state low: recharge\n"
     ]
    }
   ],
   "source": [
    "v_init = np.array([0., 0.])\n",
    "v = fsolve(bell_opt_eq_rb, v_init)\n",
    "print('optimal state value high: ' + str(np.round(v[0], 2)))\n",
    "print('optimal state value low: ' + str(np.round(v[1], 2)))\n",
    "print('remaining_optimization_loss: ' + str(np.sum(np.abs(bell_opt_eq_rb(v)))))\n",
    "\n",
    "acts_h = ['search', 'wait']\n",
    "qs_h = np.array([R_S + GAMMA*(ALPHA*v[0] + (1-ALPHA)*v[1]), R_W + GAMMA * v[0]])\n",
    "print('optimal action in state high: ' + acts_h[np.argmax(qs_h)])\n",
    "\n",
    "acts_l = ['search', 'wait', 'recharge']\n",
    "qs_l = np.array([BETA*R_S + R_E*(1-BETA) + GAMMA*((1-BETA)*v[0] + BETA*v[1]), R_W + GAMMA*v[1], GAMMA*v[0]])\n",
    "print('optimal action in state low: ' + acts_l[np.argmax(qs_l)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f43a5",
   "metadata": {},
   "source": [
    "## Bellman optimality equations - Gridworld\n",
    "<img src=\"./img/exp_4_1.png\" alt=\"gridworld_example\" width=\"500\">   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa467601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_value_function(v):\n",
    "    test = True\n",
    "    if test:\n",
    "        print('-------------------------------------')\n",
    "    else:\n",
    "        print('---------------------------------')\n",
    "    out = '| '\n",
    "    for i in range(16):\n",
    "        token = float(np.round(v[i], 2))\n",
    "        convertedToken = \"{:.2f}\".format(token)\n",
    "        if test and token > -10:\n",
    "            out += \" \"\n",
    "        out += str(convertedToken) + ' | '\n",
    "        if (i + 1) % 4 == 0:\n",
    "            print(out)\n",
    "            if test:\n",
    "                print('-------------------------------------')\n",
    "            else:\n",
    "                print('---------------------------------')\n",
    "            out = '| '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be98a6",
   "metadata": {},
   "source": [
    "MDP Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f5c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = -1\n",
    "GAMMA = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1c4fd",
   "metadata": {},
   "source": [
    "Formulate Bellman optimality equations as a optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432f00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell_opt_eq_gw(v):\n",
    "    v_T = v[0]\n",
    "    v_1 = v[1]\n",
    "    v_2 = v[2]\n",
    "    v_3 = v[3]\n",
    "    v_4 = v[4]\n",
    "    v_5 = v[5]\n",
    "    v_6 = v[6]\n",
    "    v_7 = v[7]\n",
    "    v_8 = v[8]\n",
    "    v_9 = v[9]\n",
    "    v_10 = v[10]\n",
    "    v_11 = v[11]\n",
    "    v_12 = v[12]\n",
    "    v_13 = v[13]\n",
    "    v_14 = v[14]\n",
    "\n",
    "    F = np.empty(15)\n",
    "    F[0] = v_T\n",
    "    F[1] = v_1 - max(R + GAMMA * v_1, R + GAMMA * v_2, R + GAMMA * v_5, R + GAMMA * v_T)\n",
    "    F[2] = v_2 - max(R + GAMMA * v_2, R + GAMMA * v_3, R + GAMMA * v_6, R + GAMMA * v_1)\n",
    "    F[3] = v_3 - max(R + GAMMA * v_3, R + GAMMA * v_3, R + GAMMA * v_7, R + GAMMA * v_2)\n",
    "    F[4] = v_4 - max(R + GAMMA * v_T, R + GAMMA * v_5, R + GAMMA * v_8, R + GAMMA * v_4)\n",
    "    F[5] = v_5 - max(R + GAMMA * v_1, R + GAMMA * v_6, R + GAMMA * v_9, R + GAMMA * v_4)\n",
    "    F[6] = v_6 - max(R + GAMMA * v_2, R + GAMMA * v_7, R + GAMMA * v_10, R + GAMMA * v_5)\n",
    "    F[7] = v_7 - max(R + GAMMA * v_3, R + GAMMA * v_7, R + GAMMA * v_11, R + GAMMA * v_6)\n",
    "    F[8] = v_8 - max(R + GAMMA * v_4, R + GAMMA * v_9, R + GAMMA * v_12, R + GAMMA * v_8)\n",
    "    F[9] = v_9 - max(R + GAMMA * v_5, R + GAMMA * v_10, R + GAMMA * v_13, R + GAMMA * v_8)\n",
    "    F[10] = v_10 - max(R + GAMMA * v_6, R + GAMMA * v_11, R + GAMMA * v_14, R + GAMMA * v_9)\n",
    "    F[11] = v_11 - max(R + GAMMA * v_7, R + GAMMA * v_11, R + GAMMA * v_T, R + GAMMA * v_10)\n",
    "    F[12] = v_12 - max(R + GAMMA * v_8, R + GAMMA * v_13, R + GAMMA * v_12, R + GAMMA * v_12)\n",
    "    F[13] = v_13 - max(R + GAMMA * v_9, R + GAMMA * v_14, R + GAMMA * v_13, R + GAMMA * v_12)\n",
    "    F[14] = v_14 - max(R + GAMMA * v_10, R + GAMMA * v_T, R + GAMMA * v_14, R + GAMMA * v_13)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1de2cc",
   "metadata": {},
   "source": [
    "Solve for the optimal value funciton. (tends to get stuck in local optima, check remaining optimization loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3823dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|  0.00 |  -1.00 |  -2.00 |  -3.00 | \n",
      "-------------------------------------\n",
      "|  -1.00 |  -2.00 |  -3.00 |  -2.00 | \n",
      "-------------------------------------\n",
      "|  -2.00 |  -3.00 |  -2.00 |  -1.00 | \n",
      "-------------------------------------\n",
      "|  -3.00 |  -2.00 |  -1.00 |  0.00 | \n",
      "-------------------------------------\n",
      "remaining_optimization_loss: 1.554312234622245e-13\n"
     ]
    }
   ],
   "source": [
    "v_init = np.ones(15)*-5\n",
    "v = fsolve(bell_opt_eq_gw, v_init, xtol=1e-12)\n",
    "v = np.concatenate((v, v[:1]))\n",
    "render_value_function(v)\n",
    "print('remaining_optimization_loss: ' + str(np.sum(np.abs(bell_opt_eq_gw(v)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a17d3c",
   "metadata": {},
   "source": [
    "## Compare to result of dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41827622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 89 iterations.\n",
      "Value Function:\n",
      "-------------------------------------\n",
      "|   0.00 | -13.90 | -19.84 | -21.83 | \n",
      "-------------------------------------\n",
      "| -13.90 | -17.86 | -19.85 | -19.84 | \n",
      "-------------------------------------\n",
      "| -19.84 | -19.85 | -17.86 | -13.90 | \n",
      "-------------------------------------\n",
      "| -21.83 | -19.84 | -13.90 |   0.00 | \n",
      "-------------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑   |  ←↓  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑→  |  ↓   |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 4 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 1 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/9k75n5096nd4yfw16gvwvv680000gn/T/ipykernel_88110/2110959919.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  token = float(np.round(self.value_function[i], 2))\n",
      "/var/folders/8d/9k75n5096nd4yfw16gvwvv680000gn/T/ipykernel_88110/2110959919.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  neighborValues[j] = float(mdp.trans_prop[i*4+j, 3]) + self.gamma*float(values[int(neighbors[j])])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.num_states = 16\n",
    "        self.num_actions = 4\n",
    "        self.trans_prop = np.array([[0, \"u\", 0, 0, 1], [0, \"d\", 0, 0, 1], [0, \"r\", 0, 0, 1], [0, \"l\", 0, 0, 1],\n",
    "                                    [1, \"u\", 1, -1, 1], [1, \"d\", 5, -1, 1], [1, \"r\", 2, -1, 1], [1, \"l\", 0, -1, 1],\n",
    "                                    [2, \"u\", 2, -1, 1], [2, \"d\", 6, -1, 1], [2, \"r\", 3, -1, 1], [2, \"l\", 1, -1, 1],\n",
    "                                    [3, \"u\", 3, -1, 1], [3, \"d\", 7, -1, 1], [3, \"r\", 3, -1, 1], [3, \"l\", 2, -1, 1],\n",
    "                                    [4, \"u\", 0, -1, 1], [4, \"d\", 8, -1, 1], [4, \"r\", 5, -1, 1], [4, \"l\", 4, -1, 1],\n",
    "                                    [5, \"u\", 1, -1, 1], [5, \"d\", 9, -1, 1], [5, \"r\", 6, -1, 1], [5, \"l\", 4, -1, 1],\n",
    "                                    [6, \"u\", 2, -1, 1], [6, \"d\", 10, -1, 1], [6, \"r\", 7, -1, 1], [6, \"l\", 5, -1, 1],\n",
    "                                    [7, \"u\", 3, -1, 1], [7, \"d\", 11, -1, 1], [7, \"r\", 7, -1, 1], [7, \"l\", 6, -1, 1],\n",
    "                                    [8, \"u\", 4, -1, 1], [8, \"d\", 12, -1, 1], [8, \"r\", 9, -1, 1], [8, \"l\", 8, -1, 1],\n",
    "                                    [9, \"u\", 5, -1, 1], [9, \"d\", 13, -1, 1], [9, \"r\", 10, -1, 1], [9, \"l\", 8, -1, 1],\n",
    "                                    [10, \"u\", 6, -1, 1], [10, \"d\", 14, -1, 1], [10, \"r\", 11, -1, 1],[10, \"l\", 9, -1, 1],\n",
    "                                    [11, \"u\", 7, -1, 1], [11, \"d\", 0, -1, 1], [11, \"r\", 11, -1, 1], [11, \"l\", 10, -1, 1],\n",
    "                                    [12, \"u\", 8, -1, 1], [12, \"d\", 12, -1, 1], [12, \"r\", 13, -1, 1], [12, \"l\", 12, -1, 1],\n",
    "                                    [13, \"u\", 9, -1, 1], [13, \"d\", 13, -1, 1], [13, \"r\", 14, -1, 1], [13, \"l\", 12, -1, 1],\n",
    "                                    [14, \"u\", 10, -1, 1], [14, \"d\", 14, -1, 1], [14, \"r\", 0, -1, 1], [14, \"l\", 13, -1, 1],\n",
    "                                    [15, \"u\", 15, 0, 1], [15, \"d\", 15, 0, 1], [15, \"r\", 15, 0, 1], [15, \"l\", 15, 0, 1]\n",
    "                                    ])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, gamma=1.0):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.old_policy = None\n",
    "        self.policy = None\n",
    "        self.old_value_function = None\n",
    "        self.value_function = None\n",
    "        self.reset_policy()\n",
    "        self.reset_old_policy()\n",
    "        self.reset_value_function()\n",
    "        self.reset_old_value_function()\n",
    "\n",
    "    def policy_evaluation(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.old_value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.old_value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.old_value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.old_value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = self.policy[i, 0] * float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1) + \\\n",
    "                                     self.policy[i, 1] * float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) + \\\n",
    "                                     self.policy[i, 2] * float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3) + \\\n",
    "                                     self.policy[i, 3] * float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)\n",
    "\n",
    "    def policy_improvement(self, mdp):\n",
    "        self.old_policy = np.copy(self.policy)\n",
    "        values = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(1, 15):\n",
    "            neighborValues = [0, 0, 0, 0]\n",
    "            neighbors = mdp.trans_prop[i*4:i*4+4:1, 2]\n",
    "            for j in range(4):\n",
    "                neighborValues[j] = float(mdp.trans_prop[i*4+j, 3]) + self.gamma*float(values[int(neighbors[j])])\n",
    "\n",
    "            policyValues = np.flatnonzero(neighborValues == np.max(neighborValues))\n",
    "            for k in range(self.num_actions):\n",
    "                if k in policyValues:\n",
    "                    self.policy[i, k] = 1 / len(policyValues)\n",
    "                else:\n",
    "                    self.policy[i, k] = 0\n",
    "\n",
    "    def render_value_function(self):\n",
    "        test = False\n",
    "        if abs(np.min(self.value_function)) > 10:\n",
    "            test = True\n",
    "        if test:\n",
    "            print('-------------------------------------')\n",
    "        else:\n",
    "            print('---------------------------------')\n",
    "        out = '| '\n",
    "        for i in range(16):\n",
    "            token = float(np.round(self.value_function[i], 2))\n",
    "            convertedToken = \"{:.2f}\".format(token)\n",
    "            if token == 0:\n",
    "                out += \" \"\n",
    "            if test and token > -10:\n",
    "                out += \" \"\n",
    "            out += str(convertedToken) + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                if test:\n",
    "                    print('-------------------------------------')\n",
    "                else:\n",
    "                    print('---------------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def render_policy(self):\n",
    "        pol = np.copy(self.policy)\n",
    "        print('-----------------------------')\n",
    "        out = '| '\n",
    "        for i in range(self.num_states):\n",
    "            token = \"\"\n",
    "            if pol[i, 3] > 0:   # left\n",
    "                token += \"\\u2190\"\n",
    "            if pol[i, 0] > 0:   # up\n",
    "                token += \"\\u2191\"\n",
    "            if pol[i, 1] > 0:   # down\n",
    "                token += \"\\u2193\"\n",
    "            if pol[i, 2] > 0:   # right\n",
    "                token += \"\\u2192\"\n",
    "            if token == \"\":     # empty\n",
    "                token += \"  \"\n",
    "\n",
    "            if len(token) == 1:\n",
    "                token += '  '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 2:\n",
    "                token += ' '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 3:\n",
    "                token += ' '\n",
    "\n",
    "            out += token + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                print('-----------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def reset_value_function(self):\n",
    "        self.value_function = np.zeros((self.num_states, 1))\n",
    "        \n",
    "    def reset_old_value_function(self):\n",
    "        self.old_value_function = np.ones((self.num_states, 1))*1e9\n",
    "\n",
    "    def reset_policy(self):\n",
    "        self.policy = np.ones((self.num_states, self.num_actions)) * (1.0 / self.num_actions)\n",
    "        self.policy[0, :] = 0\n",
    "        self.policy[-1, :] = 0\n",
    "\n",
    "    def reset_old_policy(self):\n",
    "        self.old_policy = np.zeros((self.num_states, self.num_actions))\n",
    "        \n",
    "mdp = MDP()\n",
    "agent = Agent(num_states=mdp.num_states, num_actions=mdp.num_actions)\n",
    "\n",
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "policy_stable = False\n",
    "threshold = 0.01\n",
    "j = 0\n",
    "while not policy_stable:\n",
    "    j += 1\n",
    "    print('*************************************************************************************')\n",
    "    print('Iteration ' + str(j))\n",
    "    print('*************************************************************************************')\n",
    "    i = 0\n",
    "    while True:\n",
    "        agent.policy_evaluation(mdp=mdp)\n",
    "        i += 1\n",
    "        if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold :\n",
    "            break\n",
    "    print(f'Policy evaluation converged after {i} iterations.')\n",
    "    print('Value Function:')\n",
    "    agent.render_value_function()\n",
    "    \n",
    "    agent.policy_improvement(mdp=mdp)\n",
    "\n",
    "    if np.max(np.abs(agent.old_policy - agent.policy)) == 0 :\n",
    "            policy_stable = True\n",
    "    print('Policy:')\n",
    "    agent.render_policy()\n",
    "    \n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb6787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllbc-library",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
