{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bb7499-c96b-4b12-afb6-8c940081cb23",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# TD-Methods vs. MC-Methods for Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602df6c-a90c-4a01-b195-d6939b77a949",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5ce7a-5272-4d01-9b3b-e8d8af93800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import gymnasium as gym\n",
    "from tqdm import notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce6f12-6dff-41a1-a66a-dbb34d1d4f18",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "\n",
    "We compare two different types of methods: TD methods, represented by SARSA, and MC methods, represented by First-Visit Monte-Carlo Control. We use the methods to train agents, whereupon we compare the results and the reward achieved during sampling. For the comparison of both methods, the environment \"FrozenLake\" is used, in which an agent has to find the correct path through a grid world. An example is shown below.\n",
    "\n",
    "<img src=\"./img/frozen_lake.gif\" alt=\"Environment\" width=\"800\" />\n",
    "\n",
    "Both methods gain experience from training with an &epsilon;-greedy policy, where the action is either random or determined based on an action value function. Learning the value function is iterative so that the policy continues to improve over time. Monte Carlo methods perform updates of the action value function based on the full episode. In contrast, TD methods perform updates during sampling. \n",
    "\n",
    "### Agent Class\n",
    "\n",
    "First, we design the class `Agent`. The class consists of the action value function, which is used to select actions. To select the actions during training, we use an &epsilon;-greedy policy. For this purpose, the class `Agent` has the method`epsilon_greedy_policy`, which calls the method `get_random_action` with a probability `epsilon` of choosing a random action. Otherwise, the agent uses the action that promises the maximum reward via `get_best_action`. This way, additional experience can be generated despite greedy sampling. Sampling from the optimal policy can happen by only using `get_best_action` after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bec75-d012-42d1-8f81-eac37f707277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1, learning_rate=0.05):\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon \n",
    "        self.rtol = 0.08\n",
    "        # Only necessary for first-visit MC:\n",
    "        self.returns = defaultdict(float) \n",
    "        self.returns_count = defaultdict(float)\n",
    "        # Only necessary for SARSA\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "    \n",
    "    def get_best_action(self, obs):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(),\n",
    "                                                                 self.rtol))) \n",
    "        return best_action\n",
    "    \n",
    "    def epsilon_greedy_policy(self, obs):\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(obs)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05be0e1-d484-497f-ae97-babc34cdce1c",
   "metadata": {},
   "source": [
    "A special feature of the implementation is that in the case of similar action values for several actions (based on a tolerance `self.rtol`) in a state, these actions are chosen randomly. To keep the resulting policy performant, a discount factor `self.gamma` is used. \n",
    "\n",
    "Next, we add the two algorithms used for learning. Both algorithms use the parameter `num_episodes`, which defines the number of episodes until the end of the training. At the end of the training, the training methods return an array containing the total reward achieved in the episodes so that they can be compared.\n",
    "\n",
    "### MC-Method - First-Visit Monte Carlo\n",
    "\n",
    "In Monte Carlo methods, the evaluation of state and action pairs is performed based on completed episodes. In First-Visit methods, the action value of a state is calculated as the average return over all episodes in which the state-action pair was visited for the first time. This method guarantees that each visit is scored only once, preventing double weighting of episodes that visit the same state-action pair. Beyond that, there exist Every-Visit methods, in which each passing through a state-action pair is evaluated. An overview of the algorithm is provided below:\n",
    "\n",
    "<img src=\"./img/first_visit_MC.png\" alt=\"First-Visit MC\" width=\"500\" />\n",
    "\n",
    "The training is realized in the `first_visit_MC` method. By using the learning method, the action value function of the class `Agent` is improved over episodes. Based on this, the resulting policy is also improved with each episode. It is worth noting that the policy updates happen implicitly through the use of the `epsilon_greedy_policy` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5e05c-c160-482e-8c75-ebaec0eb81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC(self, num_episodes=2000):\n",
    "    returns_log = [] # Used for logging\n",
    "    # Run through episodes sampled to improve policy incrementally\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Generate an episode (state, action, reward)\n",
    "        episode = []\n",
    "        returns_i = 0\n",
    "        obs, info = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.epsilon_greedy_policy(obs)\n",
    "            next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "            episode.append((obs, action, reward))\n",
    "            returns_i += reward\n",
    "            obs = next_obs\n",
    "        returns_log.append(returns_i)\n",
    "        episode = np.array(episode)\n",
    "        episode_duration = len(episode[:,:1])\n",
    "        # Calculate returns for the whole episode from the back to save memory and resources\n",
    "        G = np.zeros([episode_duration, ])\n",
    "        for i in range(episode_duration - 1, -1, -1):\n",
    "            if i + 1 > episode_duration - 1:\n",
    "                G[i] = episode[i][2]\n",
    "            else:\n",
    "                G[i] = episode[i][2] + self.gamma * G[i + 1]\n",
    "        # Find indices of first visits of state-action pairs in the episode\n",
    "        first_visit_indices = sorted(np.unique(episode[:,:1], return_index=True)[1])\n",
    "        # Update the policy with average over all episodes\n",
    "        for index in first_visit_indices:\n",
    "            state = episode[index][0]\n",
    "            action = episode[index][1]\n",
    "            self.returns[(state, action)] += G[index]\n",
    "            self.returns_count[(state, action)] += 1.0\n",
    "            update = self.returns[(state, action)] / self.returns_count[(state, action)]\n",
    "            self.action_value_fn[int(state)][int(action)] = update\n",
    "    return returns_log\n",
    "\n",
    "setattr(Agent, \"first_visit_MC\", first_visit_MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0127dc-4c64-4035-8b92-661e5a09da42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TD-Method - SARSA\n",
    "\n",
    "Temporal Difference (TD) learning methods update the action value function without having to finish the episode. SARSA stands for \"State-Action-Reward-State-Action\". It is an on-policy form of TD-learning, since it updates its action values using the action value of the next state and the current policies action. An off-policy alternative is Q-learning, where action values are updated using the action value of the next state and a greedy action (`get_best_action`). An overview of the algorithm is provided below:\n",
    "\n",
    "<img src=\"./img/sarsa.png\" alt=\"First-Visit MC\" width=\"500\" />\n",
    "\n",
    "The algorithm is implemented in the `sarsa` method. As for `first_visit_MC`, the implementation follows lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1521e8-a62b-4aa6-8172-991e8ef1acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(self, num_episodes=2000):\n",
    "    returns_log = [] # Used for logging\n",
    "    # Run through episodes\n",
    "    for i in range(num_episodes):\n",
    "        returns_i = 0\n",
    "        obs, info = self.env.reset()\n",
    "        action = self.epsilon_greedy_policy(obs)\n",
    "        done = False\n",
    "        while not done:\n",
    "            # In each step, update the action value function\n",
    "            next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "            next_action = self.epsilon_greedy_policy(next_obs)\n",
    "            td_target = reward + self.gamma * self.action_value_fn[next_obs][next_action]\n",
    "            update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target\n",
    "            self.action_value_fn[obs][action] = update\n",
    "            obs = next_obs\n",
    "            action = next_action\n",
    "            returns_i += reward\n",
    "        returns_log.append(returns_i)\n",
    "    return returns_log\n",
    "\n",
    "setattr(Agent, \"sarsa\", sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceaac4d-8933-45cd-af41-2689569b7799",
   "metadata": {},
   "source": [
    "This defines both agents so that they can be trained.\n",
    "\n",
    "## Training the Agents \n",
    "\n",
    "For this purpose, the environment \"FrozenLake\" is used, and a map is defined. In the environment, the goal is to choose correct movements on a grid for reaching a gift. Reaching the gift generates a reward of \"1\", while falling into a hole causes the episode to end without a reward.\n",
    "\n",
    "We define a `map` for the environment and initially do not use `render_mode` to speed up the training. In addition, we set `is_slippery = False` to prevent agents from moving randomly. After initializing the environment, necessary parameters are defined, followed by creating the agents and training them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779a19f-8c66-438a-8f3e-d87e8958c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "map = [\"SFFH\", \"FFFH\", \"HFFH\", \"HFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) # render mode is None to speed up learning\n",
    "env.reset()\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.1\n",
    "train_episodes = 2000\n",
    "\n",
    "# Initialize agents\n",
    "mc_agent = Agent(env, gamma, epsilon)\n",
    "td_agent = Agent(env, gamma, epsilon, learning_rate)\n",
    "\n",
    "# Train agents\n",
    "mc_training_history = mc_agent.first_visit_MC(train_episodes)\n",
    "td_training_history = td_agent.sarsa(train_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfbc4d-83ae-4f21-9528-6175a0ba736d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparing the Acheived Rewards during Training\n",
    "\n",
    "The achieved total rewards of the episodes during the training are shown below above the episodes. Since the reward can be either \"1\" or \"0\", we use the `smooth` method that gives the mean over a number of episodes defined by the `size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301b849-b5a6-4c11-aca6-12f9c8cf69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(array, size):\n",
    "    window_size = size\n",
    "    filter = np.ones(window_size) / window_size\n",
    "    smoothed_array = np.convolve(array, filter, mode='same')\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802ccfb-fbe3-46a1-b28b-b55d816e2e19",
   "metadata": {},
   "source": [
    "Using this method results in the following graphs for the achieved reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01823581-6517-4af7-833a-7b88c8a050a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size for smoothing\n",
    "size = 100\n",
    "\n",
    "# Compare the returns\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "ax.plot(range(train_episodes), smooth(mc_training_history, size), label=\"MC-Agent\")\n",
    "ax.plot(range(train_episodes), smooth(td_training_history, size), label=\"TD-Agent\")\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Total Rewards')\n",
    "ax.set_title(\"Comparison of MC and TD Training\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(0, train_episodes-size)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba801f-7e2e-4344-8b01-6be77535b3df",
   "metadata": {},
   "source": [
    "Now we can evaluate the final policy.\n",
    "\n",
    "## Comparing the Trained Agents\n",
    "\n",
    "For this purpose, we add another method to the `Agent` class, which can be used to display the policy. This method `evaluate` lets the agent sample the best possible actions for a number of episodes based on the `num_runs` parameter, using `get_best_action`. A video is recorded, which is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172e468-f47f-4297-8234-6c98323a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, env, num_runs=10, file=None):\n",
    "    frames = []  # collect rgb_image of agent env interaction\n",
    "    video_created = False\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        reward_per_run = 0\n",
    "        out = env.render()\n",
    "        frames.append(out)\n",
    "        while not done:\n",
    "            action = self.get_best_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_per_run += reward\n",
    "            # save frame\n",
    "            out = env.render()\n",
    "            frames.append(out)\n",
    "\n",
    "    # create animation out of saved frames\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)\n",
    "        plt.close()\n",
    "        anim.save(file, writer=\"ffmpeg\", fps=5)\n",
    "        video_created = True\n",
    "\n",
    "    return video_created\n",
    "\n",
    "setattr(Agent, \"evaluate\", evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f5999-44c6-453b-a429-9ccd315f68da",
   "metadata": {
    "tags": []
   },
   "source": [
    "To display the policies, first, the `render_mode` of the environment must be changed so that a rgb array can be stored. To achieve this, we replace the current environment. Additionally, we define the number of runs we want to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d9eab-44e2-4cc6-8794-b89e3f6cbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching the render_mode to rgb_array\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)\n",
    "\n",
    "# Defining the number of runs for the evaluation\n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6075d0-2b2c-423b-ba93-4687bb731bc1",
   "metadata": {},
   "source": [
    "We now use the `evaluate` method to store and replay a video for both agents, displaying the policies on the grid.\n",
    "\n",
    "### Policy of the MC-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c086c5f-697f-4ada-9b64-4b1967117b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_video_file = \"mc_trained.mp4\"\n",
    "mc_agent.evaluate(env, num_runs, mc_video_file)\n",
    "Video(mc_video_file, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b627ed-68ff-439d-8ac4-1f14a0f172a9",
   "metadata": {},
   "source": [
    "### Policy of the TD-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f0490-240a-492d-a955-049e08978fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "td_video_file = \"td_trained.mp4\"\n",
    "td_agent.evaluate(env, num_runs, td_video_file)\n",
    "Video(td_video_file, html_attributes=\"loop autoplay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
