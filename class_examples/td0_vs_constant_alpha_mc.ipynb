{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61909d6-1d7b-4bd9-bd8b-8c64bce12703",
   "metadata": {
    "tags": []
   },
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Comparing the abilities of TD(0) and constant-&alpha; MC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c15b16-870a-40d7-a64e-b0daea35febb",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a68c8-25b9-46c3-8853-8e40b70354d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212f4f8-0861-4ed9-8140-a66c941d09bf",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "\n",
    "In this example, we compare the prediction abilities of TD(0) and constant-&alpha; MC. To do this, we use both methods for learning state value functions based on a given policy and compare how the deviation to the true state value function changes over the training episodes. For the comparison, we use Frozenlake, with a simple map of the form:\n",
    "\n",
    "<img src=\"./img/env.png\" alt=\"Environment\" width=\"400\" height=\"400\" />\n",
    "\n",
    "The agent's goal is to achieve the gift. Upon reaching the gift, the agent receives a reward of \"1\"; on any other space, the agent receives a reward of \"0\".\n",
    "\n",
    "\n",
    "### Obtaining Policy and State Value Function\n",
    "\n",
    "To compare the two methods, we first need a policy, as well as the associated value function. Here, we use policy iteration to determine both. Dynamic Programming is particularly suitable for determining the state value function that we use for reference, since it can determine it stably with adjustable accuracy, specified in `update_threshold`. The details of the implementation of the agent that performs the policy iteration are assumed to be given here. For more details on Dynamic Programming, we refer to previous lectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ceb18-2f77-4056-90c2-fdbd18108286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, num_states, dynamics_fn):\n",
    "        self.num_states = num_states\n",
    "        self.actions_per_state = [] # list containing the actions available per state\n",
    "        for state in dynamics_fn:\n",
    "            actions = list(dynamics_fn[state].keys())\n",
    "            self.actions_per_state.append(actions)\n",
    "        self.P = dynamics_fn # P[s][a] represents a list of possible transitions and rewards given state s and a\n",
    "\n",
    "class DP_Agent():\n",
    "    def __init__(self, mdp, gamma=0.9, update_threshold=0.05):\n",
    "        self.mdp = mdp # contains state transition function, num_states and actions\n",
    "        self.update_threshold = update_threshold # stopping distance as criteria for stopping policy evaluation\n",
    "        self.state_value_fn = np.zeros(self.mdp.num_states) # a table leading from state to value expectations\n",
    "        self.policy = [[1 for _ in actions] for actions in self.mdp.actions_per_state]\n",
    "        self.policy = [[action / len(actions) for action in actions] for actions in self.policy]\n",
    "        self.gamma = gamma # discount rate for return\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Choose action based on the probabilities defined within the policy\n",
    "        action = np.random.choice(np.flatnonzero(np.isclose(self.policy[state], max(self.policy[state]))))\n",
    "        return action\n",
    "\n",
    "    def get_action_value(self, state, action):\n",
    "        # Calculate action value for state action pair based on the transition probabilities of the mdp\n",
    "        action_value = 0\n",
    "        for transition in self.mdp.P[state][action]:\n",
    "            transition_prob = transition[0]  # prob of next state\n",
    "            state_value_obs = transition[1]  # id of next state\n",
    "            reward = transition[2]  # reward of next state\n",
    "            action_value += transition_prob * (reward + self.gamma * self.state_value_fn[state_value_obs])\n",
    "        return action_value\n",
    "\n",
    "    def train(self):\n",
    "        policy_stable = False\n",
    "        while not policy_stable:\n",
    "            # Use policy evaluation to update the value function\n",
    "            self.policy_evaluation()\n",
    "            # Use new value function to update the policy\n",
    "            policy_stable = self.improve_policy() # if the policy stops changing, this ends the training process\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        # Policy evaluation for all states\n",
    "        max_update = self.update_threshold + 1\n",
    "        while max_update > self.update_threshold:\n",
    "            max_update = 0\n",
    "            for state in range(self.mdp.num_states):\n",
    "                old_state_value = self.state_value_fn[state]\n",
    "                new_state_value = 0\n",
    "                actions = np.nonzero(self.policy[state])[0]\n",
    "                for action in actions:\n",
    "                    action_prob = self.policy[state][action]\n",
    "                    new_state_value += action_prob * self.get_action_value(state, action)\n",
    "                self.state_value_fn[state] = max(new_state_value, old_state_value)\n",
    "                max_update = max(max_update, np.abs(old_state_value - self.state_value_fn[state]))\n",
    "\n",
    "    def improve_policy(self):\n",
    "        # Initiates improving the policy\n",
    "        policy_stable = True\n",
    "        # Cache current policy\n",
    "        current_policy = self.policy\n",
    "        # Create empty policy to store the best possible actions based on current value function\n",
    "        best_policy = [[0 for _ in actions] for actions in self.mdp.actions_per_state]\n",
    "        for state in range(self.mdp.num_states):\n",
    "            # Calculate best possible policy based on current value function\n",
    "            action_values = []\n",
    "            for action in range(len(self.mdp.actions_per_state[state])):\n",
    "                action_values.append(self.get_action_value(state, action))  # calculate q(s,a)\n",
    "            action_value_max = max(action_values)\n",
    "            best_actions = np.where(action_values == action_value_max)[0] # find indices where maximum value occurs\n",
    "            for index in best_actions:\n",
    "                best_policy[state][index] = 1\n",
    "            best_policy[state] = [best_policy[state][action] / len(best_actions)\n",
    "                                  for action in self.mdp.actions_per_state[state]]\n",
    "            # If the current policy is not the best policy, update it\n",
    "            if not np.array_equal(current_policy[state], best_policy[state]):\n",
    "                policy_stable = False\n",
    "                self.policy[state] = best_policy[state]\n",
    "        return policy_stable\n",
    "\n",
    "    def visualize(self):\n",
    "        print('State value function of the agent:')\n",
    "        x_axis = 4\n",
    "        y_axis = 4\n",
    "        vmin = 0\n",
    "        vmax = 1\n",
    "        X1 = np.reshape(self.state_value_fn, (x_axis, y_axis))\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        cmap = cm.get_cmap(\"Blues_r\")\n",
    "        cmap.set_under(\"black\")\n",
    "        img = ax.imshow(X1, interpolation=\"nearest\", vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.axis('off')\n",
    "        for i in range(x_axis):\n",
    "            for j in range(y_axis):\n",
    "                ax.text(j, i, str(X1[i][j])[:4], fontsize=12, color='black', ha='center', va='center')\n",
    "        plt.show()\n",
    "\n",
    "    def render_policy(self):\n",
    "        pol = np.copy(self.policy)\n",
    "        print('Policy of the agent:')\n",
    "        print('-----------------------------')\n",
    "        out = '| '\n",
    "        for i in range(self.mdp.num_states):\n",
    "            token = \"\"\n",
    "            if pol[i, 3] > 0:   # left\n",
    "                token += \"\\u2190\"\n",
    "            if pol[i, 0] > 0:   # up\n",
    "                token += \"\\u2191\"\n",
    "            if pol[i, 1] > 0:   # down\n",
    "                token += \"\\u2193\"\n",
    "            if pol[i, 2] > 0:   # right\n",
    "                token += \"\\u2192\"\n",
    "            if token == \"\":     # empty\n",
    "                token += \"  \"\n",
    "\n",
    "            if len(token) == 1:\n",
    "                token += '  '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 2:\n",
    "                token += ' '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 3:\n",
    "                token += ' '\n",
    "\n",
    "            out += token + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                print('-----------------------------')\n",
    "                out = '| '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d012b1-70ad-43e0-82d7-a95d61d1112f",
   "metadata": {},
   "source": [
    "We now create the map, the environment and the agent, let the agent train until the optimal policy and the corresponding value function is found and then extract the state value function and policy. Under `target_state_value_fn` the state value function determined via DP now serves as a reference for TD(0) and constant-&alpha; MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343904b-2189-417c-874a-cd118515722e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up map, environment, and agent\n",
    "map = [\"SFFF\", \"FFFF\", \"FFFF\", \"FFFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) # set render_mode=None for speeding up learning\n",
    "env.reset()\n",
    "gamma = 0.9\n",
    "mdp = MarkovDecisionProcess(env.observation_space.n, env.unwrapped.P) # in our case contains dynamics function\n",
    "agent = DP_Agent(mdp, update_threshold=0.00001)\n",
    "\n",
    "# Train the agent.\n",
    "agent.train()\n",
    "\n",
    "# Extract the necessary functions for comparison\n",
    "policy = agent.policy\n",
    "target_state_value_fn = agent.state_value_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbf5e-bb67-494f-95b4-9e061cd933ab",
   "metadata": {},
   "source": [
    "Below, we show both the state value function, and the resulting policy. The optimal policy in this environment always leads down-right and thus towards the gift. Since we use a discount factor &gamma; with the optimal policy, the state-value decreases from the terminal state to the initial state for each field by the factor &gamma;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31de65-d7d8-4d7c-9b39-98d09e9c050a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the policy\n",
    "agent.render_policy()\n",
    "\n",
    "# Show the value function\n",
    "agent.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c9556-f7aa-40e8-95c0-c8b3f95a2554",
   "metadata": {},
   "source": [
    "Now that the policy and the target value function are ready, we can turn our attention to the methods for determining the state value function.\n",
    "\n",
    "We implement the two methods in a way that returns a measure of the progress of learning for each episode. We use the root mean squared (RMS) error between the learned value function and the target value function. The methods return an array containing the RMS error of all learning episodes.\n",
    "\n",
    "### Constant-&alpha; MC\n",
    "\n",
    "With constant alpha MC, the update of the state value function is determined by comparing the return of a state with its current state value. In case there are deviations, the difference multiplied by the factor &alpha; is added to the state value.\n",
    "Updates are always performed after the end of the episode for all passed states. In this implementation, updates are performed for every visit, whereby each visit is also a first visit due to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae883773-a60d-4f21-bafa-c17c139f90fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def constant_alpha_MC(gamma, alpha, env, num_episodes, target_state_value_fn, policy):\n",
    "    # Initialize state value function\n",
    "    state_value_fn = np.zeros(mdp.num_states)\n",
    "    rms_errors = [] \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Generate an episode using the policy\n",
    "        episode = []\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.choice(len(policy[obs]), p=policy[obs])\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            episode.append((obs, reward))\n",
    "            obs = next_obs\n",
    "        #print('NEW EPISODE')\n",
    "        episode = np.array(episode)\n",
    "        episode_duration = len(episode[:,:1])\n",
    "        # Calculate returns for the whole episode from the back to save memory and resources\n",
    "        G = np.zeros([episode_duration, ])\n",
    "        for i in range(episode_duration - 1, -1, -1):\n",
    "            if i + 1 > episode_duration - 1:\n",
    "                G[i] = episode[i][1] # Last step of the episode\n",
    "            else:\n",
    "                G[i] = episode[i][1] + gamma * G[i + 1] # Every other step\n",
    "        # Update the policy with constant-alpha MC\n",
    "        for index in range(episode_duration):\n",
    "            obs = int(episode[index][0])\n",
    "            #print('state:', obs)\n",
    "            #print('old value fn', state_value_fn[obs])\n",
    "            state_value_fn[obs] = state_value_fn[obs] + alpha * (G[index] - state_value_fn[obs])\n",
    "            #print('new value fn', state_value_fn[obs])\n",
    "        # Calculate root mean squared error for the episode\n",
    "        rms_error = np.sqrt(np.mean(np.square(state_value_fn - target_state_value_fn)))\n",
    "        rms_errors.append(rms_error)\n",
    "    return rms_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad2151-f102-43b7-88f2-702de68a20bd",
   "metadata": {},
   "source": [
    "### TD(0)\n",
    "\n",
    "In TD methods, the updates for the state value function are performed using the TD error. This error is the deviation between the state value of a state and the immediate reward, plus the discounted state value of the subsequent state. As with constant-&alpha; MC, in case of deviation, the error is added to the state value of a state with a factor alpha, however, not at the end of the episode but immediately. For TD(0), it is sufficient to observe a state, its reward, and subsequent state to determine the updates. The opposite of this behavior is TD(1), where the entire episode would be used for the error, so that updates could only be made at the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdca5e-0878-40d2-aeba-b6e5d1ab921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_0(gamma, alpha, env, num_episodes, target_state_value_fn, policy):\n",
    "    # Initialize state value function\n",
    "    state_value_fn = np.zeros(mdp.num_states)\n",
    "    rms_errors = []\n",
    "    for i in range(num_episodes):\n",
    "        # Update the value function based on the policy\n",
    "        obs, info = env.reset() # Initial state\n",
    "        done = False\n",
    "        #print('NEW EPISODE')\n",
    "        while not done:\n",
    "            action = np.random.choice(len(policy[obs]), p=policy[obs]) # Get action from policy\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            #print('state:', obs)\n",
    "            #print('old value fn', state_value_fn[obs])\n",
    "            state_value_fn[obs] = state_value_fn[obs] + alpha * (reward + ( gamma * state_value_fn[next_obs] ) - state_value_fn[obs])\n",
    "            #print('old value fn', state_value_fn[obs])\n",
    "            obs = next_obs\n",
    "        # Calculate root mean squared error for the episode\n",
    "        rms_error = np.sqrt(np.mean(np.square(state_value_fn - target_state_value_fn)))\n",
    "        rms_errors.append(rms_error)\n",
    "    return rms_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ce951-9abf-4126-96d5-be62ce7e2cc0",
   "metadata": {},
   "source": [
    "## Learning the State Value Functions\n",
    "\n",
    "To obtain meaningful values, we perform the learning of the state value functions for both methods with multiple values of &alpha;. In addition, we use multiple runs for each value of &alpha; and average their values for the episodes using the `data_generator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b96f59-c994-4b77-8281-c8824d112e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(func, n_repeats, gamma, alpha, env, evaluation_episodes, target_state_value_fn, policy):\n",
    "    data = []\n",
    "    # Generate the data\n",
    "    for i in range(n_repeats):\n",
    "        data.append(func(gamma, alpha, env, evaluation_episodes, target_state_value_fn, policy))\n",
    "    # Take the mean over the number of repeats\n",
    "    mean_data = np.mean(data, axis=0)\n",
    "    return mean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70fd0a-08e6-4612-bec6-9a2de26d1f6d",
   "metadata": {},
   "source": [
    "Now we use the `data_generator` to learn the state value functions for several values of &alpha; TD(0) and constant-&alpha; MC to get a plot of the mean RMS errors for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad84c-46c8-46f1-845a-5de7b2d0faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 50\n",
    "evaluation_episodes = 300\n",
    "\n",
    "# Generate MC runs\n",
    "mc_10 = data_generator(constant_alpha_MC, n_repeats, gamma, 0.1, env, evaluation_episodes, target_state_value_fn, policy)\n",
    "mc_20 = data_generator(constant_alpha_MC, n_repeats, gamma, 0.2, env, evaluation_episodes, target_state_value_fn, policy)\n",
    "mc_30 = data_generator(constant_alpha_MC, n_repeats, gamma, 0.3, env, evaluation_episodes, target_state_value_fn, policy)\n",
    "\n",
    "# Generate TD runs\n",
    "td_10 = data_generator(td_0, n_repeats, gamma, 0.1, env, evaluation_episodes, target_state_value_fn, policy)\n",
    "td_20 = data_generator(td_0, n_repeats, gamma, 0.2, env, evaluation_episodes, target_state_value_fn, policy)\n",
    "td_30 = data_generator(td_0, n_repeats, gamma, 0.3, env, evaluation_episodes, target_state_value_fn, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbb6c3-88f7-4856-a9af-cb5764c39bf7",
   "metadata": {},
   "source": [
    "## Comparing the Results\n",
    "\n",
    "Below we plot the RMS errors over the episodes. The dashed graphs represent constant-&alpha; MC, the solid ones represent TD(0). \n",
    "Both methods succeed in approximating the target value function, which is reflected in the reduction of the RMS error to almost zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a0bb0-6940-4705-92ee-6f83a6b3a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "ax.plot(range(evaluation_episodes), mc_10, label=\"constant-alpha MC - alpha=0.10\", linestyle='dashed', color='blue')\n",
    "ax.plot(range(evaluation_episodes), mc_20, label=\"constant-alpha MC - alpha=0.20\", linestyle='dashed', color='red')\n",
    "ax.plot(range(evaluation_episodes), mc_30, label=\"constant-alpha MC - alpha=0.30\", linestyle='dashed', color='orange')\n",
    "ax.plot(range(evaluation_episodes), td_10, label=\"TD(0) - alpha=0.10\", color='blue')\n",
    "ax.plot(range(evaluation_episodes), td_20, label=\"TD(0) - alpha=0.20\", color='red')\n",
    "ax.plot(range(evaluation_episodes), td_30, label=\"TD(0) - alpha=0.30\", color='orange')\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Root Mean Squared Error')\n",
    "ax.set_title('Root Mean Squared Error Averaged Over States')\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_xlim(0, evaluation_episodes)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
