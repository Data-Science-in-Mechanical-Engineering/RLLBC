{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723ff9fb-bc22-47c4-9341-a55a802218a3",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Dyna-Q vs Q-Learning\n",
    "*(Adapted from Sutton & Barto, Chapter 8)*\n",
    "\n",
    "Generating experience can be a resource-intensive task. Consider a physical system where each run generates a monetary cost. In such a case, money can be a limiting factor and the experience generated must be used efficiently. To improve resource efficiency, we have to consider what we use our experience for.\n",
    "Fundamentally, there are two possible use-cases for the experience we generate: We can learn a value function, or we can use it to improve a model. \n",
    "In standard Q-Learning we only improve the value function. However, we can also use the same experience to improve a model in parallel, which we can in turn use for further improvements of the value function. This is the approach Dyna-Q follows. The algorithm is presented below and compared with Q-Learning, to show the improvements in sampling-efficiency.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac3c62-1dea-4207-9a70-e01788883ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import custom_envs\n",
    "import copy\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from utils.visuals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c9636-1f11-41c9-850f-1db688c9ff5a",
   "metadata": {},
   "source": [
    "## Design of the Agent\n",
    "The algorithm for Dyna-Q equals the Q-Learning algorithm, however, there is an addition. Each step is used to improve a deterministic model of the environment, from which random samples are generated to improve the value function. The algorithm is presented below:\n",
    "\n",
    "<img src=\"./img/dyna-q.png\" alt=\"Environment\" width=\"500\" />\n",
    "\n",
    "The agent is similar to the already presented Q-Learning agent , with two additions:\n",
    "* `self.model` is a model of the environment, containing the subsequent state and reward for each state-action pair\n",
    "+ `self.visited_states_and_actions` keeps track of the visited state-action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dd8a5-8332-4afc-8379-d527573dfaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TD_Agent():\n",
    "    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1, dyn_q_iters=25):\n",
    "        self.env = env\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n)) # the q-fn\n",
    "        self.model = np.zeros((self.env.observation_space.n, self.env.action_space.n, 2))\n",
    "        class StateActionTracker:\n",
    "            def __init__(self):\n",
    "                self.states = {}\n",
    "                self.actions = {}\n",
    "            def add_state_action_pair(self, state, action):\n",
    "                if state not in self.states:\n",
    "                    self.states[state] = 1\n",
    "                    self.actions[state] = set([action])\n",
    "                else:\n",
    "                    self.states[state] += 1\n",
    "                    if action not in self.actions[state]:\n",
    "                        self.actions[state].add(action)\n",
    "            def get_state(self):\n",
    "                if self.states:\n",
    "                    return np.random.choice(list(self.states.keys()))\n",
    "            def get_action(self, state):\n",
    "                if state in self.actions and self.actions[state]:\n",
    "                    return np.random.choice(list(self.actions[state]))\n",
    "        self.visited_states_and_actions = StateActionTracker()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.dyn_q_iters = dyn_q_iters\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "    def get_best_action(self, obs):\n",
    "        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(), rtol=0.01)))\n",
    "        return best_action\n",
    "    \n",
    "    def epsilon_greedy_policy(self, obs):\n",
    "        # returns action, choosing a random action with probability epsilon or the best action based on Q(s,a)\n",
    "        randomly = np.random.random() < self.epsilon\n",
    "        if randomly:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_best_action(obs)\n",
    "        return action\n",
    "    \n",
    "    def q_learning_update(self, obs, action, next_obs, reward):\n",
    "        best_next_action = self.get_best_action(next_obs)\n",
    "        td_target = reward + self.gamma * self.action_value_fn[next_obs][best_next_action]\n",
    "        update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target\n",
    "        self.action_value_fn[obs][action] = update\n",
    "        \n",
    "    def q_learning(self, num_episodes):\n",
    "        returns = []\n",
    "        for i in range(num_episodes):\n",
    "            returns_i = 0\n",
    "            obs, info = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Generate next_obs and reward based on state-action pair\n",
    "                action = self.epsilon_greedy_policy(obs)\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                # Update the value function\n",
    "                self.q_learning_update(obs, action, next_obs, reward)\n",
    "                obs = next_obs\n",
    "                returns_i += reward\n",
    "            returns.append(returns_i)\n",
    "        return returns\n",
    "    \n",
    "    def dyna_q(self, num_episodes):\n",
    "        returns = []\n",
    "        for i in range(num_episodes):\n",
    "            returns_i = 0\n",
    "            obs, info = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Generate next_obs and reward based on state-action pair\n",
    "                action = self.epsilon_greedy_policy(obs)\n",
    "                next_obs, reward, done, truncated, info = env.step(action)\n",
    "                # Update the value function\n",
    "                self.q_learning_update(obs, action, next_obs, reward)\n",
    "                # Update the model\n",
    "                self.model[obs][action] = np.array([next_obs, reward]) # Assuming deterministic environment\n",
    "                self.visited_states_and_actions.add_state_action_pair(obs, action)\n",
    "                returns_i += reward\n",
    "                obs = next_obs\n",
    "                # Perform more updates using the model\n",
    "                for j in range(self.dyn_q_iters):\n",
    "                    mod_state = self.visited_states_and_actions.get_state()\n",
    "                    mod_action = self.visited_states_and_actions.get_action(mod_state)\n",
    "                    mod_next_state, mod_reward = int(self.model[mod_state][mod_action][0]), self.model[mod_state][mod_action][1]\n",
    "                    self.q_learning_update(mod_state, mod_action, mod_next_state, mod_reward)\n",
    "            returns.append(returns_i)\n",
    "        return returns\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5671a-6786-4d66-8ed7-7560598ee32c",
   "metadata": {},
   "source": [
    "## Evaluation of the Training\n",
    "To compare the two methods, both agents are trained with the same properties. Epsilon is set to 0.2, allowing for occasional random behavior of the agent. We show the return of both agents over the episodes. To allow for improved visualization, the returns are smoothed using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c7d6f-3011-4dc0-9b41-e014480d2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(array, size):\n",
    "    window_size = size\n",
    "    filter = np.ones(window_size) / window_size\n",
    "    smoothed_array = np.convolve(array, filter, mode='same')\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0d227-5ce3-4382-a164-d17905d29570",
   "metadata": {},
   "source": [
    "We train the agents and prove that the learned value functions lead to comparable policies. Additionally we evaluate the policy within the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe8884-689f-425a-ad96-210d5412a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization\n",
    "setattr(TD_Agent, \"evaluate\", evaluate)\n",
    "\n",
    "# Set values\n",
    "map = [\"SFHH\", \"HFFH\", \"HFFH\", \"HHFG\"]\n",
    "env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False)\n",
    "render_env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)\n",
    "filter = 50\n",
    "num_episodes = 10000\n",
    "env.reset()\n",
    "\n",
    "agent_1 = TD_Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.2)\n",
    "q_learning_training_history = smooth(agent_1.q_learning(num_episodes=num_episodes), filter)\n",
    "plot_action_value(agent_1.action_value_fn, \"Q-Learning\")\n",
    "video_file_1 = \"q_agent.mp4\"\n",
    "agent_1.evaluate(render_env, 1, video_file_1)\n",
    "Video(video_file_1, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442621d-4b30-4e38-af5f-483e98eacfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2 = TD_Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.2)\n",
    "dyna_q_training_history = smooth(agent_2.dyna_q(num_episodes=num_episodes), filter)\n",
    "plot_action_value(agent_2.action_value_fn, \"Dyna-Q\")\n",
    "video_file_2 = \"dyna_q_agent.mp4\"\n",
    "agent_2.evaluate(render_env, 1, video_file_2)\n",
    "Video(video_file_2, html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df052d33-02c8-4f0c-9cb9-9a678bba0633",
   "metadata": {},
   "source": [
    "Lastly, we show how much more efficient the learning with Dyna-Q is, compared to Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be36a8-b826-44f1-a702-af31b4042c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "ax.plot(range(num_episodes), q_learning_training_history, label=\"Q-Learning\")\n",
    "ax.plot(range(num_episodes), dyna_q_training_history, label=\"Dyna-Q\")\n",
    "ax.grid(False)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Returns')\n",
    "ax.set_title(\"Comparison of Dyna-Q and Q-Learning\")\n",
    "ax.set_ylim(-1, 2)\n",
    "ax.set_xlim(0, 1000)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
