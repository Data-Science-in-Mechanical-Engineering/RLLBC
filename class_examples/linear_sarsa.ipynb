{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db817fe",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc87975",
   "metadata": {},
   "source": [
    "# On-Policy Control (Sarsa) with Linear Value Function Approximation\n",
    "(*Code adapted from: https://github.com/SamKirkiles/mountain-car-SARSA-AC/blob/master/mountain_car.py*)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1e383",
   "metadata": {},
   "source": [
    "## Mountain Car Environment\n",
    "\n",
    "<img src=\"./img/mountain_car.gif\" alt=\"Mountain Car\" width=\"500\">\n",
    "\n",
    "\n",
    "The agent (a car) is started at the bottom of a valley. For any given\n",
    "state the agent may choose to accelerate to the left, right or cease\n",
    "any acceleration.\n",
    "\n",
    "    Source:\n",
    "        The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(2)\n",
    "        Num    Observation               Min            Max\n",
    "        0      Car Position              -1.2           0.6\n",
    "        1      Car Velocity              -0.07          0.07\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(3)\n",
    "        Num    Action\n",
    "        0      Accelerate to the Left\n",
    "        1      Don't accelerate\n",
    "        2      Accelerate to the Right\n",
    "\n",
    "        Note: This does not affect the amount of velocity affected by the\n",
    "        gravitational pull acting on the car.\n",
    "\n",
    "    Reward:\n",
    "         Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n",
    "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "\n",
    "    Starting State:\n",
    "         The position of the car is assigned a uniform random value in [-0.6 , -0.4].\n",
    "         The starting velocity of the car is always assigned to 0.\n",
    "\n",
    "    Episode Termination:\n",
    "         The car position is more than 0.5\n",
    "         Episode length is greater than 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5746703",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "num_episodes_render = 10\n",
    "discount_factor = 1.0\n",
    "alpha = 0.01\n",
    "nA = env.action_space.n\n",
    "\n",
    "# Parameter vector define number of parameters per action based on featurizer size\n",
    "w = np.zeros((nA, 400))\n",
    "\n",
    "# Plots\n",
    "plt_actions = np.zeros(nA)\n",
    "episode_rewards = np.zeros(num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7463f6",
   "metadata": {},
   "source": [
    "## Features\n",
    "Here: Radial Basis Functions. For more information check Sutton & Barto Chap. 9.5.5\n",
    "\n",
    "<img src=\"./img/radial_basis_function.png\" alt=\"Radial Basis Function\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get satistics over observation space samples for normalization\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Create radial basis function sampler to convert states to features for nonlinear function approx\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "    (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "    (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "    (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "])\n",
    "# Fit featurizer to our scaled inputs\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "\n",
    "# Normalize and turn into feature\n",
    "def featurize_state_rbf(state):\n",
    "    # Transform data\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac0ed8",
   "metadata": {},
   "source": [
    "## Episodic Semi-gradient Sarsa Algorithm with Linear Function Approximation\n",
    "Sutton & Barto page 244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(state, action, w):\n",
    "    # Our linear model takes the state and gives us the action value function for all possible actions\n",
    "    # (accelerate to the left / don't accelerate / accelerate to the right) so to evaluate the q-function\n",
    "    # only use the weights that are corresponding to the action we take.\n",
    "    value = state.dot(w[action])\n",
    "    return value\n",
    "\n",
    "\n",
    "# Epsilon greedy policy\n",
    "def policy(state, weight, epsilon=0.1):\n",
    "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    best_action = np.argmax([Q(state, a, w) for a in range(nA)])\n",
    "    A[best_action] += (1.0 - epsilon)\n",
    "    sample = np.random.choice(nA, p=A)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in tqdm(range(num_episodes)):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = featurize_state_rbf(state)\n",
    "\n",
    "    while True:\n",
    "        if np.mod(e, num_episodes_render) == 0:\n",
    "            env.render()\n",
    "        # Sample from policy\n",
    "        action = policy(state, w)\n",
    "        # Staistic for graphing\n",
    "        plt_actions[action] += 1\n",
    "        # Step environment and get next state and make it a feature\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = featurize_state_rbf(next_state)\n",
    "\n",
    "        # Figure out what our policy tells us to do for the next state\n",
    "        next_action = policy(next_state, w)\n",
    "\n",
    "        # Statistic for graphing\n",
    "        episode_rewards[e] += reward\n",
    "\n",
    "        # Figure out target and td error\n",
    "        target = reward + discount_factor * Q(next_state, next_action, w)\n",
    "        td_error = Q(state, action, w) - target\n",
    "\n",
    "        # Update weights\n",
    "        dw = (td_error).dot(state)\n",
    "        w[action] -= alpha * dw\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        # update our state\n",
    "        state = next_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bb45b",
   "metadata": {},
   "source": [
    "## Evaluate Run\n",
    "Take a look at the return per episode the agent could achieve. <br>\n",
    "Take a look at the Cost-to-go function ($-\\max_a \\hat{Q}(s, a, w)$) the agent learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_to_go_mountain_car(num_tiles=20):\n",
    "    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_tiles)\n",
    "    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_tiles)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.apply_along_axis(lambda _: -np.max([Q(featurize_state_rbf(_), a, w) for a in range(nA)]), 2, np.dstack([X, Y]))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                           cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Velocity')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title(\"Mountain Car Cost-to-Go Function\")\n",
    "    fig.colorbar(surf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84227dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_result():\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(num_episodes), episode_rewards)\n",
    "    plt.title('Training Result')\n",
    "    plt.ylabel('Return')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f36ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plot_cost_to_go_mountain_car()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
