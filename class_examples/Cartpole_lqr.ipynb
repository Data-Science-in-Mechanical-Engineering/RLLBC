{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a607d3b-5b74-4230-a32f-a22180c25eb0",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Linear Quadratic Regulator for Cartpole Control\n",
    "\n",
    "![cartpole](./img/cartpole.png)\n",
    "\n",
    "In this notebook we take a look at the Linear Quadratic Regulator (LQR) applied to the cartpole depicted above. <br> \n",
    "The cartpole has the coordinates $x$ and $\\theta$, cart mass $m_c$, pole mas $m_p$ and pole length $2l$. <br> \n",
    "The pole is controlled by applying a force $F$ to the cart. The control task is to balance the pole in an upright position. This represents an unstable equilibrium.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf549b8-7a52-429b-8709-c23f9c501296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import control as ct\n",
    "import math\n",
    "import custom_envs\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Union\n",
    "from IPython.display import Video\n",
    "from IPython.display import display\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138da008-0d0e-4fd5-bb43-71ab4990b335",
   "metadata": {},
   "source": [
    "## Custom Cartpole Environment\n",
    "As we need to know the system dynamics for doing controller design, the first step is to model the equations of motion for the cartpole system.<br>\n",
    "Luckily there is a paper available, where someone has already done the job: https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "\n",
    "![cart_pole.gif](./img/cart_pole.gif)\n",
    "\n",
    "To fit the framework we typically use in RL and to visualize controller performance, we combined this formulation with the farama gym cartpole environment: https://gymnasium.farama.org/environments/classic_control/cart_pole/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ab046-3474-41d5-8e1e-d7ec214471f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('CustomCartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09c988-aba6-4ede-bf4c-15ffd22f06df",
   "metadata": {},
   "source": [
    "## Linearize System\n",
    "For controller synthesis we will make use of the Python Control Systems Library.\n",
    "If you are interested in looking deeper into this, a good tutorial by Richard Murray can be found here: https://www.cds.caltech.edu/~murray/courses/cds112/wi2023/W1_python-control.pdf <br>\n",
    "First, we simply incorporate the nonlinear equations of motion of the cartpole system, we used in the cartpole environment into a nonlinear control system. <br>\n",
    "For LQR synthesis, however, we need a LTI (linear time-invariant) system. The equations of motion are already time-invariant, as they don't depend on the time. But we need to linearize them. The linear system representation we choose is a state space model of the form: $\\dot{x}=A x+B u$ <br>\n",
    "Therfore, we define the control as the force acting on the cart $u = F$ <br>\n",
    "and the state as the cart position, the pole angle and their first derivative \n",
    "<br> $x=\\left(\\begin{array}{l}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\\end{array}\\right)$ <br>\n",
    "Consequently, our control system will look like this \n",
    "<br> $\\left(\\begin{array}{c}\\dot{x} \\\\ \\ddot{x} \\\\ \\dot{\\theta} \\\\ \\ddot{\\theta}\\end{array}\\right)=\\left(\\begin{array}{llll}a_{11} & a_{12} & a_{13} & a_{14} \\\\ a_{21} & a_{22} & a_{23} & a_{24} \\\\ a_{31} & a_{32} & a_{33} & a_{34} \\\\ a_{41} & a_{42} & a_{43} & a_{44}\\end{array}\\right)\\left(\\begin{array}{l}x \\\\ \\dot{x} \\\\ \\theta \\\\ \\dot{\\theta}\\end{array}\\right)+\\left(\\begin{array}{l}b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4\\end{array}\\right) F$ <br>\n",
    "So the angluar acceleration of the pole will be descibed like this: <br>\n",
    "$\\ddot{\\theta}=a_{41} x+a_{42} \\dot{x}+a_{43} \\theta+a_{44} \\dot{\\theta}+b_4 F$ <br>\n",
    "We derive those linear coefficients by performing a first order Talyor approximation $T_1$ on the nonlinear equations of motion <br>\n",
    "$ T_1 f\\left(x, x_0\\right)=f\\left(x_0\\right)+\\frac{\\partial f(x)}{\\partial x}\\left(x-x_0\\right) +\\frac{\\partial f(x)}{\\partial \\dot{x}}\\left(\\dot{x}-\\dot{x}_0\\right)+\\frac{\\partial f(x)}{\\partial \\theta}\\left(\\theta-\\theta_0\\right) +\\frac{\\partial f(x)}{\\partial \\dot{\\theta}}\\left(\\dot{\\theta}-\\dot{\\theta}_0\\right)$<br>\n",
    "In fact, the *linearize* function does this for us. We only need to provide a state to linearize around, we choose the upper euqilibrium of the cart pole, where the cart is at the origin, the pole stands upright and nothing moves. This corresponds to all state variables being 0.<br>\n",
    "As we do control in discrete time, we need to reformulate the system matrices, such that we result in a model formulation of\n",
    "<br>$x_{t+1}=A_d x_t+B_d u_t$. <br>\n",
    "The *sample* function does this for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720c5a7-0a31-4732-83dc-27ede653d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole = ct.NonlinearIOSystem(env.ct_sys_update, env.ct_sys_output, states=4, name='cartpole', inputs=['action'], outputs=['x', 'x_dot', 'theta', 'theta_dot'])\n",
    "linsys = cartpole.linearize(x0=np.array([0., 0., 0., 0.]), u0=np.array([0.]))\n",
    "linsys_d = linsys.sample(env.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee5c9d-7ae8-4c06-9872-2207f61aa80e",
   "metadata": {},
   "source": [
    "## LQR Synthesis\n",
    "To stabilize the upper equilibrium, we synthesize a linear quadratic regulator (LQR). The idea is to design an optimal controller, such that cost for each start state $x_t$ <br>\n",
    "$ J = \\sum_{k=t}^{\\infty}\\left[ x_k^T Q x_k + u_k^T R u_k \\right] $ is minimized. <br>\n",
    "Therefore, we need to fomulate the matrices $Q$ and $R$ that penalize deviations of the state from the equilibrium state and control effort. <br>\n",
    "The result is a feedback controller $u_t = K x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b048bb3-3138-4032-87d4-292cbdc527bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost_x = 5\n",
    "cost_x_dot = 1\n",
    "cost_theta = 10\n",
    "cost_theta_dot = 1\n",
    "cost_control = 1\n",
    "\n",
    "Q = np.diag([cost_x, cost_x_dot, cost_theta, cost_theta_dot])\n",
    "R = np.diag([cost_control])\n",
    "\n",
    "K, _, _ = ct.lqr(linsys_d, Q, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4adb7-89b5-4062-895f-ff706d21ab4d",
   "metadata": {},
   "source": [
    "Define simulation timesteps and a start state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65c3f8-3d80-4daf-8f4d-a7dfb61811e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_steps = 500\n",
    "start_state = np.array([0.5, 0, 20 * 2 * math.pi / 360, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bbfb7-06e2-4d83-aedc-b637bb88f7b8",
   "metadata": {},
   "source": [
    "Let's use the LQR to control the nonlinear cartpole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93877652-f4ac-40f0-b3dd-c48fb5ea7ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "steps = 0\n",
    "state, _ = env.reset(start_state=start_state.copy())\n",
    "\n",
    "cont_arr_lqr = np.zeros(max_steps)\n",
    "state_arr_lqr = np.zeros([max_steps, 4])\n",
    "\n",
    "frames = []  # collect rgb_image of agent env interaction\n",
    "\n",
    "while not done:\n",
    "    action = -np.matmul(K, state)[0]\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    cont_arr_lqr[steps] = action\n",
    "    state_arr_lqr[steps, :] = state\n",
    "    out = env.render()\n",
    "    frames.append(out)\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "    if steps == max_steps:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ace1e-8560-41ce-ba93-050233ac0b91",
   "metadata": {},
   "source": [
    "Visualization of the simulation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cb543-a56e-45f7-a70e-e6b7ac89d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(frames, title):\n",
    "    if all(frame is not None for frame in frames):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(frames[0])\n",
    "\n",
    "        def animate(index):\n",
    "            img.set_data(frames[index])\n",
    "            # plt.show()\n",
    "            return [img]\n",
    "        anim = FuncAnimation(fig, animate, frames=len(frames), interval=10)\n",
    "        plt.close()\n",
    "        anim.save(title, writer=\"ffmpeg\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4ffd2-629a-451b-b801-b339930a446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video(frames, 'cartpole_lqr.mp4')\n",
    "Video(\"cartpole_lqr.mp4\", html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a5d6c-83b8-4c4d-b30c-92f2dccf9cdf",
   "metadata": {},
   "source": [
    "## Linearization Error\n",
    "Now we use the same LQR to control the linearized system from the same start state. <br>\n",
    "Then we compare the state and control trajectories of the original and the linearized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13b207-c151-4ea3-ad98-bd80d9590dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_arr_lin_lqr = np.zeros(max_steps)\n",
    "state_arr_lin_lqr = np.zeros([max_steps, 4])\n",
    "state = start_state.copy()\n",
    "\n",
    "for steps in range(max_steps):\n",
    "    action = -np.matmul(K, state)[0]\n",
    "    next_state = linsys_d.dynamics(env.tau, state, action)\n",
    "    cont_arr_lin_lqr[steps] = action\n",
    "    state_arr_lin_lqr[steps, :] = state\n",
    "    state = next_state\n",
    "\n",
    "plt.plot(state_arr_lin_lqr[:, 0], label='$x_{lin}$', color='gold')\n",
    "plt.plot(state_arr_lqr[:, 0], label='$x$', color='darkorange')\n",
    "plt.plot(state_arr_lin_lqr[:, 1], label='$\\dot{x}_{lin}$', color='limegreen')\n",
    "plt.plot(state_arr_lqr[:, 1], label='$\\dot{x}$', color='darkgreen')\n",
    "plt.plot(state_arr_lin_lqr[:, 2], label='$\\\\theta_{lin}$', color='lightsteelblue')\n",
    "plt.plot(state_arr_lqr[:, 2], label='$\\\\theta$', color='darkblue')\n",
    "plt.plot(state_arr_lin_lqr[:, 3], label='$\\dot{\\\\theta}_{lin}$', color='tomato')\n",
    "plt.plot(state_arr_lqr[:, 3], label='$\\dot{\\\\theta}$', color='darkred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5f259-fe3f-401c-862d-5f17e6feb861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(cont_arr_lin_lqr, label='$u_{lin}$', color='gold')\n",
    "plt.plot(cont_arr_lqr, label='$u$', color='darkorange')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a3ac7-3fb8-4c33-a591-12fc0b1d8fea",
   "metadata": {},
   "source": [
    "We see deviations that are attributed to the linearization error we make. However, due to feedback control, the LQR is capable of stabilizing the nonlinear cartpole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3a9ba-52bf-4474-846d-83e025b989f6",
   "metadata": {},
   "source": [
    "## Feedforward Control\n",
    "Let's now try feedforward control. Here, instead of reacting to the current state of the system, we use a precomputed control sequence to stabilize the cartpole.<br>\n",
    "For the computation of this control sequence we rely on our prior knowledge about the linearized system. Hence we use the precomputed LQR sequence on the linearized system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5c7c9-c07f-45f6-95cf-2d267a1dec2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_steps = 500\n",
    "\n",
    "done = False\n",
    "steps = 0\n",
    "state, _ = env.reset(start_state=start_state.copy())\n",
    "\n",
    "cont_arr_ff = np.zeros(max_steps)\n",
    "state_arr_ff = np.zeros([max_steps, 4])\n",
    "\n",
    "frames = []  # collect rgb_image of agent env interaction\n",
    "\n",
    "while not done:\n",
    "    action = cont_arr_lin_lqr[steps]\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    cont_arr_ff[steps] = action\n",
    "    state_arr_ff[steps, :] = state\n",
    "    out = env.render()\n",
    "    frames.append(out)\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "    if steps == max_steps:\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f108101-3098-483d-ac34-92ea775950c2",
   "metadata": {},
   "source": [
    "We see in this case, the controller fails as there is no compensation of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdf6a8-7078-4ddf-8a6a-a668108a14b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_video(frames, 'cartpole_feedforward.mp4')\n",
    "Video(\"cartpole_feedforward.mp4\", html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631beb9c-2c55-4bbb-94c3-c0a13a44657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state_arr_ff[:, 0], label='$x$', color='darkorange')\n",
    "plt.plot(state_arr_ff[:, 1], label='$\\dot{x}$', color='darkgreen')\n",
    "plt.plot(state_arr_ff[:, 2], label='$\\\\theta$', color='darkblue')\n",
    "plt.plot(state_arr_ff[:, 3], label='$\\dot{\\\\theta}$', color='darkred')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d8f1e-227f-4d9b-ab98-7a633574e58f",
   "metadata": {},
   "source": [
    "However, we see that feedforward control is not only a problem in case of linearization errors. <br>\n",
    "It works, if we use the exact same system and start state again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97392163-793c-45a1-bee0-ddc5e66af611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_arr_lin_ff = np.zeros(max_steps)\n",
    "state_arr_lin_ff = np.zeros([max_steps, 4])\n",
    "state = start_state.copy()\n",
    "\n",
    "for steps in range(max_steps):\n",
    "    action = cont_arr_lin_lqr[steps]\n",
    "    next_state = linsys_d.dynamics(env.tau, state, action)\n",
    "    cont_arr_lin_ff[steps] = action\n",
    "    state_arr_lin_ff[steps, :] = state\n",
    "    state = next_state\n",
    "\n",
    "plt.plot(state_arr_lin_ff[:, 0], label='$x_{lin}$', color='gold')\n",
    "plt.plot(state_arr_lin_ff[:, 1], label='$\\dot{x}_{lin}$', color='limegreen')\n",
    "plt.plot(state_arr_lin_ff[:, 2], label='$\\\\theta_{lin}$', color='lightsteelblue')\n",
    "plt.plot(state_arr_lin_ff[:, 3], label='$\\dot{\\\\theta}_{lin}$', color='tomato')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501b7ce-1ada-4984-a7bc-1925080b2151",
   "metadata": {},
   "source": [
    "But as soon as we have a slight change in the initial conditions, the feedforward controller fails again </br>\n",
    "The same would be true in case of noise or slight modelling errors of the linear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ea188-8228-495b-ae80-166f381dee36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_arr_lin_ff = np.zeros(max_steps)\n",
    "state_arr_lin_ff = np.zeros([max_steps, 4])\n",
    "state = start_state.copy()+np.array([0., 0., 1e-15, 0.])\n",
    "\n",
    "for steps in range(max_steps):\n",
    "    action = cont_arr_lin_lqr[steps]\n",
    "    next_state = linsys_d.dynamics(env.tau, state, action)\n",
    "    cont_arr_lin_ff[steps] = action\n",
    "    state_arr_lin_ff[steps, :] = state\n",
    "    state = next_state\n",
    "\n",
    "plt.plot(state_arr_lin_ff[:, 0], label='$x_{lin}$', color='gold')\n",
    "plt.plot(state_arr_lin_ff[:, 1], label='$\\dot{x}_{lin}$', color='limegreen')\n",
    "plt.plot(state_arr_lin_ff[:, 2], label='$\\\\theta_{lin}$', color='lightsteelblue')\n",
    "plt.plot(state_arr_lin_ff[:, 3], label='$\\dot{\\\\theta}_{lin}$', color='tomato')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
