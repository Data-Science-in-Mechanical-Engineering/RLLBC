{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf672d84-3b54-4f58-af33-a0ca67caf614",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# **BlackJack**\n",
    "The example uses First-Visit Monte-Carlo estimation with exploring starts for controlling the BlackJack environment from Sutton and Barto. \n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95eeb31a-b6d0-471e-b154-bd09c82ee0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import custom_envs\n",
    "import numpy as np\n",
    "import warnings\n",
    "# Ignore numpy depraction warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca34448-3fbe-4dea-af87-86b7144d370d",
   "metadata": {},
   "source": [
    "## Design of the Agent\n",
    "The BlackJack environment is based on a simplified version from Sutton and Barto, which was taken from the ToyText package from Gymnasium. In BlackJack, the goal is to get a score of 21. The player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealerâ€™s cards is faceup and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the\n",
    "game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). The agent represents the player and is designed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09617f23-084f-476a-9b77-16c4a6490ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MC_Agent():\n",
    "    def __init__(self, env, gamma=1.0):\n",
    "        self.env = env\n",
    "        self.returns = defaultdict(float) # Summing up returns in each state\n",
    "        self.returns_count = defaultdict(float) # Keeping track of count in each state\n",
    "        self.action_value_fn = np.zeros((self.env.observation_space[0].n, self.env.observation_space[1].n, self.env.observation_space[2].n, self.env.action_space.n))\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_random_action(self):\n",
    "        random_action = np.random.choice(range(self.env.action_space.n))\n",
    "        return random_action\n",
    "\n",
    "    def get_greedy_action(self, obs):\n",
    "        action_probability = np.isclose(self.action_value_fn[obs],self.action_value_fn[obs].max(),rtol=0.01)\n",
    "        actions = np.flatnonzero(action_probability)\n",
    "        best_action = np.random.choice(actions)\n",
    "        return best_action\n",
    "\n",
    "    def train(self, num_episodes=2000, episode_max_duration=100):\n",
    "        # Run through episodes sampled to improve policy incrementally\n",
    "        for i_episode in range(1, num_episodes + 1):\n",
    "            # Generate an episode (state, action, reward)\n",
    "            episode = []\n",
    "            obs, info = env.reset()\n",
    "            for t in range(episode_max_duration):\n",
    "                # For realizing exploring starts, a random action is first chosen...\n",
    "                if t == 0:\n",
    "                    action = self.get_random_action()\n",
    "                # ... and a greedy action is chosen in all subsequent states\n",
    "                else:\n",
    "                    action = self.get_greedy_action(obs)\n",
    "                next_obs, reward, done, truncated, info = env.step(action)\n",
    "                episode.append((obs, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "            episode = np.array(episode)\n",
    "            episode_duration = len(episode[:,:1])\n",
    "            # Calculate returns for the whole episode from the back to save memory and resources\n",
    "            G = np.zeros([episode_duration, ])\n",
    "            for i in range(episode_duration - 1, -1, -1):\n",
    "                if i + 1 > episode_duration - 1:\n",
    "                    G[i] = episode[i][2] # Last step of the episode\n",
    "                else:\n",
    "                    G[i] = episode[i][2] + self.gamma * G[i + 1]  # Every other step #!\n",
    "            # Find indices of first visits of state-action pairs in the episode\n",
    "            first_visit_indices = sorted(np.unique(episode[:,:1], return_index=True)[1])\n",
    "            # Update the policy with average over all episodes\n",
    "            for index in first_visit_indices:\n",
    "                state = episode[index][0]\n",
    "                action = episode[index][1]\n",
    "                self.returns[(state, action)] += G[index]\n",
    "                self.returns_count[(state, action)] += 1.0\n",
    "                update = self.returns[(state, action)] / self.returns_count[(state, action)]\n",
    "                self.action_value_fn[state][int(action)] = update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727720f-fff4-4b78-89fa-5af517a79864",
   "metadata": {},
   "source": [
    "## Evaluation of the Training\n",
    "Since BlackJack is a game of chance, two agents are compared to evaluate success. One of the agents is untrained and acts randomly, the other agent has been trained and acts greedy. The necessary function for comparison can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c6c5ad-875e-4e95-8d73-8dd326b4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, env, num_runs=1):\n",
    "    tot_reward = [0]\n",
    "    for _ in range(num_runs):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        reward_per_run = 0\n",
    "        while not done:\n",
    "            action = self.get_greedy_action(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_per_run += reward\n",
    "        tot_reward.append(reward_per_run + tot_reward[-1])\n",
    "    return tot_reward\n",
    "\n",
    "setattr(MC_Agent, \"evaluate\", evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7af170-7917-4f5b-b75f-1513045d4692",
   "metadata": {},
   "source": [
    "Two agents are created for evaluation. While one of the agents is trained, the other one acts random. Both agents play BlackJack for some episodes, then the cumulative reward over all evaluation episodes of each agent is compared to show which agent is more successful in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a88f91-2f8e-4c35-a3f8-5a6f0028df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BlackJack-v1', render_mode=None, sab=True)  # set render_mode=None for speeding up learning\n",
    "num_runs = 500\n",
    "env.reset()\n",
    "\n",
    "# Evaluating a random agent\n",
    "random_agent = MC_Agent(env, gamma=0.9)\n",
    "random_results = random_agent.evaluate(env, num_runs=num_runs)\n",
    "\n",
    "# Evaluating a trained agent\n",
    "env.reset()\n",
    "trained_agent = MC_Agent(env, gamma=0.9)\n",
    "trained_agent.train(num_episodes=1000000)\n",
    "trained_results = trained_agent.evaluate(env, num_runs=num_runs)\n",
    "\n",
    "# Comparing both agents\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
    "ax.plot(range(num_runs + 1), random_results, label=\"Random Agent\")\n",
    "ax.plot(range(num_runs + 1), trained_results, label=\"Trained Agent\")\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e99452-8e2c-406b-8009-6c19051165ba",
   "metadata": {},
   "source": [
    "## Analysis of the Results\n",
    "The reason for the different behavior is the knowledge that the agent has obtained about the environment through the Monte Carlo estimation. In order to show this, the value function, based on the obtained action value function, and the resulting policy can be used. For this purpose, methods for showing both functions are provided first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3031419-221c-4757-9a22-7f1f7e4f3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(self):\n",
    "    player_sum = np.array(['12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])\n",
    "    dealer_showing = np.array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n",
    "    x11 = []\n",
    "    y11 = []\n",
    "    x12 = []\n",
    "    y12 = []\n",
    "    x21 = []\n",
    "    y21 = []\n",
    "    x22 = []\n",
    "    y22 = []\n",
    "    # Only a part of the value function array is necessary for showing the policy. This is sliced following S&B:\n",
    "    sliced_value_fn = self.action_value_fn[12:22, 1:11, :, 0]\n",
    "    # We use this array to retrieve the indices for player current sum, dealer value, and usable ace\n",
    "    for i in range(sliced_value_fn.shape[0]):\n",
    "        for j in range(sliced_value_fn.shape[1]):\n",
    "            for k in range(sliced_value_fn.shape[2]):\n",
    "                # Collect values for the policy\n",
    "                orig_i = i + 12\n",
    "                orig_j = j + 1\n",
    "                orig_k = k\n",
    "                if orig_k == 1:\n",
    "                    if self.get_greedy_action((orig_i, orig_j, orig_k)) == 0:\n",
    "                        x11.append(j)\n",
    "                        y11.append(i)\n",
    "                    else:\n",
    "                        x12.append(j)\n",
    "                        y12.append(i)\n",
    "                else:\n",
    "                    if self.get_greedy_action((orig_i, orig_j, orig_k)) == 0:\n",
    "                        x21.append(j)\n",
    "                        y21.append(i)\n",
    "                    else:\n",
    "                        x22.append(j)\n",
    "                        y22.append(i)\n",
    "    # Print the policy\n",
    "    plt.figure(0)\n",
    "    plt.title('With Usable Ace')\n",
    "    plt.scatter(x11, y11, color='red')\n",
    "    plt.scatter(x12, y12, color='blue')\n",
    "    plt.xticks(range(len(dealer_showing)), dealer_showing)\n",
    "    plt.yticks(range(len(player_sum)), player_sum)\n",
    "    plt.xlabel('Dealer Card')\n",
    "    plt.ylabel('Player Sum')\n",
    "    plt.figure(1)\n",
    "    plt.title('Without Usable Ace')\n",
    "    plt.scatter(x21, y21, color='red')\n",
    "    plt.scatter(x22, y22, color='blue')\n",
    "    plt.xticks(range(len(dealer_showing)), dealer_showing)\n",
    "    plt.yticks(range(len(player_sum)), player_sum)\n",
    "    plt.xlabel('Dealer Card')\n",
    "    plt.ylabel('Player Sum')\n",
    "    \n",
    "setattr(MC_Agent, \"visualize_policy\", visualize_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b9160-240f-41f1-a5c1-226b16cd3594",
   "metadata": {},
   "source": [
    "The resulting policy for the agent is shown below. Here, red dots signal \"Stick\" and blue dots signal \"Hit\". It can be seen that different game behavior is shown depending on the availability of an ace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29051c27-c523-450c-84ad-deffefb665b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_agent.visualize_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c3613-74ac-429b-89d1-61d5eedef383",
   "metadata": {},
   "source": [
    "By determining the action chosen in each case, or all actions in the case of equal expectation of reward, the state value function can be formed. To represent this, a function is first given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d0f9b-2046-4f24-8fc7-7c4ca2eb603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_function(self):\n",
    "    player_sum = np.array(['12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])\n",
    "    dealer_showing = np.array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n",
    "    # Only a part of the value function array is necessary for showing the policy. This is sliced following S&B:\n",
    "    sliced_value_fn = self.action_value_fn[12:22, 1:11, :, 0]\n",
    "    state_value_fn = np.zeros((10, 10, 2))\n",
    "    # We use this array to retrieve the indices for player current sum, dealer value, and usable ace\n",
    "    for i in range(sliced_value_fn.shape[0]):\n",
    "        for j in range(sliced_value_fn.shape[1]):\n",
    "            for k in range(sliced_value_fn.shape[2]):\n",
    "                # Collect values for the policy\n",
    "                orig_i = i + 12\n",
    "                orig_j = j + 1\n",
    "                orig_k = k\n",
    "                # Compute the state_value function\n",
    "                if np.allclose(self.action_value_fn[orig_i, orig_j, orig_k],\n",
    "                               self.action_value_fn[orig_i, orig_j, orig_k][0], rtol=0.01, atol=0.01):\n",
    "                    state_value_fn[i, j, k] = sum(self.action_value_fn[\n",
    "                        orig_i, orig_j, orig_k])/2\n",
    "                else:\n",
    "                    state_value_fn[i, j, k] = self.action_value_fn[\n",
    "                        orig_i, orig_j, orig_k, self.get_greedy_action((orig_i, orig_j, orig_k))]\n",
    "    # Print the value function\n",
    "    fig, axes = plt.subplots(nrows=2, figsize=(5, 8), subplot_kw={'projection': '3d'})\n",
    "    axes[0].set_title('Without Usable Ace')\n",
    "    axes[1].set_title('With Usable Ace')\n",
    "    X, Y = np.meshgrid(dealer_showing.astype(int), player_sum.astype(int))\n",
    "    axes[0].plot_wireframe(X, Y, state_value_fn[:, :, 0])\n",
    "    axes[1].plot_wireframe(X, Y, state_value_fn[:, :, 1])\n",
    "    for ax in axes[0], axes[1]:\n",
    "        ax.set_zlim(-1, 1)\n",
    "        ax.set_ylabel('Player Sum')\n",
    "        ax.set_xlabel('Dealer Showing')\n",
    "        ax.set_zlabel('State-Value')\n",
    "    plt.show()\n",
    "\n",
    "setattr(MC_Agent, \"visualize_value_function\", visualize_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031129f-8dd0-40d7-9cf6-af21358abc3c",
   "metadata": {},
   "source": [
    "The state value function represents the learning success of the agent. This could be learned without the need for a model, purely based on experience. Again, there is a difference between the learned value functions depending on the availability of an ace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01771cd3-b037-463e-8adb-a02cf827c59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_agent.visualize_value_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLLBC-grader",
   "language": "python",
   "name": "rllbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
