{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552c80fd",
   "metadata": {},
   "source": [
    "![DSME-logo](./img/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering (DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee951583",
   "metadata": {},
   "source": [
    "# Dynamic Programming in a Gridworld Example\n",
    "\n",
    "<img src=\"./img/exp_4_1.png\" alt=\"gridworld_example\" width=\"500\">   \n",
    "\n",
    "(Implementation of Example 4.1 in Sutton & Barto textbook, 2nd edition.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24b0aa",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfcdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84b922",
   "metadata": {},
   "source": [
    "## MDP and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005a3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.num_states = 16\n",
    "        self.num_actions = 4\n",
    "        self.trans_prop = np.array([[0, \"u\", 0, 0, 1], [0, \"d\", 0, 0, 1], [0, \"r\", 0, 0, 1], [0, \"l\", 0, 0, 1],\n",
    "                                    [1, \"u\", 1, -1, 1], [1, \"d\", 5, -1, 1], [1, \"r\", 2, -1, 1], [1, \"l\", 0, -1, 1],\n",
    "                                    [2, \"u\", 2, -1, 1], [2, \"d\", 6, -1, 1], [2, \"r\", 3, -1, 1], [2, \"l\", 1, -1, 1],\n",
    "                                    [3, \"u\", 3, -1, 1], [3, \"d\", 7, -1, 1], [3, \"r\", 3, -1, 1], [3, \"l\", 2, -1, 1],\n",
    "                                    [4, \"u\", 0, -1, 1], [4, \"d\", 8, -1, 1], [4, \"r\", 5, -1, 1], [4, \"l\", 4, -1, 1],\n",
    "                                    [5, \"u\", 1, -1, 1], [5, \"d\", 9, -1, 1], [5, \"r\", 6, -1, 1], [5, \"l\", 4, -1, 1],\n",
    "                                    [6, \"u\", 2, -1, 1], [6, \"d\", 10, -1, 1], [6, \"r\", 7, -1, 1], [6, \"l\", 5, -1, 1],\n",
    "                                    [7, \"u\", 3, -1, 1], [7, \"d\", 11, -1, 1], [7, \"r\", 7, -1, 1], [7, \"l\", 6, -1, 1],\n",
    "                                    [8, \"u\", 4, -1, 1], [8, \"d\", 12, -1, 1], [8, \"r\", 9, -1, 1], [8, \"l\", 8, -1, 1],\n",
    "                                    [9, \"u\", 5, -1, 1], [9, \"d\", 13, -1, 1], [9, \"r\", 10, -1, 1], [9, \"l\", 8, -1, 1],\n",
    "                                    [10, \"u\", 6, -1, 1], [10, \"d\", 14, -1, 1], [10, \"r\", 11, -1, 1],[10, \"l\", 9, -1, 1],\n",
    "                                    [11, \"u\", 7, -1, 1], [11, \"d\", 0, -1, 1], [11, \"r\", 11, -1, 1], [11, \"l\", 10, -1, 1],\n",
    "                                    [12, \"u\", 8, -1, 1], [12, \"d\", 12, -1, 1], [12, \"r\", 13, -1, 1], [12, \"l\", 12, -1, 1],\n",
    "                                    [13, \"u\", 9, -1, 1], [13, \"d\", 13, -1, 1], [13, \"r\", 14, -1, 1], [13, \"l\", 12, -1, 1],\n",
    "                                    [14, \"u\", 10, -1, 1], [14, \"d\", 14, -1, 1], [14, \"r\", 0, -1, 1], [14, \"l\", 13, -1, 1],\n",
    "                                    [15, \"u\", 15, 0, 1], [15, \"d\", 15, 0, 1], [15, \"r\", 15, 0, 1], [15, \"l\", 15, 0, 1]\n",
    "                                    ])\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, gamma=1.0):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.old_policy = None\n",
    "        self.policy = None\n",
    "        self.old_value_function = None\n",
    "        self.value_function = None\n",
    "        self.reset_policy()\n",
    "        self.reset_old_policy()\n",
    "        self.reset_value_function()\n",
    "        self.reset_old_value_function()\n",
    "\n",
    "    def policy_evaluation(self, mdp):\n",
    "        self.old_value_function = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(mdp.num_states):\n",
    "            o1 = self.old_value_function[int(mdp.trans_prop[4 * i, 2]), 0]  # old values for state i\n",
    "            o2 = self.old_value_function[int(mdp.trans_prop[4 * i + 1, 2]), 0]\n",
    "            o3 = self.old_value_function[int(mdp.trans_prop[4 * i + 2, 2]), 0]\n",
    "            o4 = self.old_value_function[int(mdp.trans_prop[4 * i + 3, 2]), 0]\n",
    "\n",
    "            r1 = float(mdp.trans_prop[4 * i + 0, 3])  # rewards for action a in state s\n",
    "            r2 = float(mdp.trans_prop[4 * i + 1, 3])\n",
    "            r3 = float(mdp.trans_prop[4 * i + 2, 3])\n",
    "            r4 = float(mdp.trans_prop[4 * i + 3, 3])\n",
    "            # As the mdp we consider here is deterministic, the sum over subsequent states and rewards only consists of one element\n",
    "            self.value_function[i] = self.policy[i, 0] * float(mdp.trans_prop[4 * i + 0, 4]) * (r1 + self.gamma * o1) + \\\n",
    "                                     self.policy[i, 1] * float(mdp.trans_prop[4 * i + 1, 4]) * (r2 + self.gamma * o2) + \\\n",
    "                                     self.policy[i, 2] * float(mdp.trans_prop[4 * i + 2, 4]) * (r3 + self.gamma * o3) + \\\n",
    "                                     self.policy[i, 3] * float(mdp.trans_prop[4 * i + 3, 4]) * (r4 + self.gamma * o4)\n",
    "\n",
    "    def policy_improvement(self, mdp):\n",
    "        self.old_policy = np.copy(self.policy)\n",
    "        values = np.copy(self.value_function)\n",
    "\n",
    "        for i in range(1, 15):\n",
    "            neighborValues = [0, 0, 0, 0]\n",
    "            neighbors = mdp.trans_prop[i*4:i*4+4:1, 2]\n",
    "            for j in range(4):\n",
    "                neighborValues[j] = float(mdp.trans_prop[i*4+j, 3]) + self.gamma*float(values[int(neighbors[j])])\n",
    "\n",
    "            policyValues = np.flatnonzero(neighborValues == np.max(neighborValues))\n",
    "            for k in range(self.num_actions):\n",
    "                if k in policyValues:\n",
    "                    self.policy[i, k] = 1 / len(policyValues)\n",
    "                else:\n",
    "                    self.policy[i, k] = 0\n",
    "\n",
    "    def render_value_function(self):\n",
    "        test = False\n",
    "        if abs(np.min(self.value_function)) > 10:\n",
    "            test = True\n",
    "        if test:\n",
    "            print('-------------------------------------')\n",
    "        else:\n",
    "            print('---------------------------------')\n",
    "        out = '| '\n",
    "        for i in range(16):\n",
    "            token = float(np.round(self.value_function[i], 2))\n",
    "            convertedToken = \"{:.2f}\".format(token)\n",
    "            if token == 0:\n",
    "                out += \" \"\n",
    "            if test and token > -10:\n",
    "                out += \" \"\n",
    "            out += str(convertedToken) + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                if test:\n",
    "                    print('-------------------------------------')\n",
    "                else:\n",
    "                    print('---------------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def render_policy(self):\n",
    "        pol = np.copy(self.policy)\n",
    "        print('-----------------------------')\n",
    "        out = '| '\n",
    "        for i in range(self.num_states):\n",
    "            token = \"\"\n",
    "            if pol[i, 3] > 0:   # left\n",
    "                token += \"\\u2190\"\n",
    "            if pol[i, 0] > 0:   # up\n",
    "                token += \"\\u2191\"\n",
    "            if pol[i, 1] > 0:   # down\n",
    "                token += \"\\u2193\"\n",
    "            if pol[i, 2] > 0:   # right\n",
    "                token += \"\\u2192\"\n",
    "            if token == \"\":     # empty\n",
    "                token += \"  \"\n",
    "\n",
    "            if len(token) == 1:\n",
    "                token += '  '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 2:\n",
    "                token += ' '\n",
    "                token = ' ' + token\n",
    "            elif len(token) == 3:\n",
    "                token += ' '\n",
    "\n",
    "            out += token + ' | '\n",
    "            if (i + 1) % 4 == 0:\n",
    "                print(out)\n",
    "                print('-----------------------------')\n",
    "                out = '| '\n",
    "\n",
    "    def reset_value_function(self):\n",
    "        self.value_function = np.zeros((self.num_states, 1))\n",
    "        \n",
    "    def reset_old_value_function(self):\n",
    "        self.old_value_function = np.ones((self.num_states, 1))*1e9\n",
    "\n",
    "    def reset_policy(self):\n",
    "        self.policy = np.ones((self.num_states, self.num_actions)) * (1.0 / self.num_actions)\n",
    "        self.policy[0, :] = 0\n",
    "        self.policy[-1, :] = 0\n",
    "\n",
    "    def reset_old_policy(self):\n",
    "        self.old_policy = np.zeros((self.num_states, self.num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb75f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP()\n",
    "agent = Agent(num_states=mdp.num_states, num_actions=mdp.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02f485",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f2daa",
   "metadata": {},
   "source": [
    "Iteratively approximate the value function for the initial random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc02a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Policy Evaluation\n",
      "*************************************************************************************\n",
      "Value function after 1 iterations.\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Value function after 2 iterations.\n",
      "---------------------------------\n",
      "|  0.00 | -1.75 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -1.75 | -2.00 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -2.00 | -1.75 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -1.75 |  0.00 | \n",
      "---------------------------------\n",
      "Value function after 3 iterations.\n",
      "---------------------------------\n",
      "|  0.00 | -2.44 | -2.94 | -3.00 | \n",
      "---------------------------------\n",
      "| -2.44 | -2.88 | -3.00 | -2.94 | \n",
      "---------------------------------\n",
      "| -2.94 | -3.00 | -2.88 | -2.44 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.94 | -2.44 |  0.00 | \n",
      "---------------------------------\n",
      "Value function after 10 iterations.\n",
      "---------------------------------\n",
      "|  0.00 | -6.14 | -8.35 | -8.97 | \n",
      "---------------------------------\n",
      "| -6.14 | -7.74 | -8.43 | -8.35 | \n",
      "---------------------------------\n",
      "| -8.35 | -8.43 | -7.74 | -6.14 | \n",
      "---------------------------------\n",
      "| -8.97 | -8.35 | -6.14 |  0.00 | \n",
      "---------------------------------\n",
      "Policy evaluation converged after 89 iterations.\n",
      "Value Function:\n",
      "-------------------------------------\n",
      "|   0.00 | -13.90 | -19.84 | -21.83 | \n",
      "-------------------------------------\n",
      "| -13.90 | -17.86 | -19.85 | -19.84 | \n",
      "-------------------------------------\n",
      "| -19.84 | -19.85 | -17.86 | -13.90 | \n",
      "-------------------------------------\n",
      "| -21.83 | -19.84 | -13.90 |   0.00 | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "print('*************************************************************************************')\n",
    "print('Policy Evaluation')\n",
    "print('*************************************************************************************')\n",
    "threshold  = 0.01\n",
    "i=0\n",
    "while np.max(np.abs(agent.old_value_function - agent.value_function)) > threshold:\n",
    "    i += 1\n",
    "    agent.policy_evaluation(mdp=mdp)\n",
    "    if i in [1, 2, 3, 10]:\n",
    "        print(f'Value function after {i} iterations.')\n",
    "        agent.render_value_function()\n",
    "    \n",
    "print(f'Policy evaluation converged after {i} iterations.')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "\n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12e9fe",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "Find a optimal policy given the current value function. In the example below, we show the optimal policies individually for each value function resulting from policy evaluation iterations with the random initial policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c68f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.75 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -1.75 | -2.00 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -2.00 | -1.75 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -1.75 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   | ←↑↓→ | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "| ←↑↓→ |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -2.44 | -2.94 | -3.00 | \n",
      "---------------------------------\n",
      "| -2.44 | -2.88 | -3.00 | -2.94 | \n",
      "---------------------------------\n",
      "| -2.94 | -3.00 | -2.88 | -2.44 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.94 | -2.44 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  |  ←↓  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑→  |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "\n",
    "for i in range(3):\n",
    "        print('*************************************************************************************')\n",
    "        print('Iteration ' + str(i+1))\n",
    "        print('*************************************************************************************')\n",
    "        agent.policy_evaluation(mdp=mdp)\n",
    "        print('Value Function:')\n",
    "        agent.render_value_function()\n",
    "        agent.policy_improvement(mdp=mdp)\n",
    "        print('Policy:')\n",
    "        agent.render_policy()\n",
    "        agent.reset_policy() # Reset the policy such that the value function is iterated with respect to the radom initial policy. Therefore results are comparable with Figure 4.1 in Sutton & Barto\n",
    "        \n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fd105",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "Iterative policy evalation until convergence and policy improvement until policy is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18332439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 89 iterations.\n",
      "Value Function:\n",
      "-------------------------------------\n",
      "|   0.00 | -13.90 | -19.84 | -21.83 | \n",
      "-------------------------------------\n",
      "| -13.90 | -17.86 | -19.85 | -19.84 | \n",
      "-------------------------------------\n",
      "| -19.84 | -19.85 | -17.86 | -13.90 | \n",
      "-------------------------------------\n",
      "| -21.83 | -19.84 | -13.90 |   0.00 | \n",
      "-------------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑   |  ←↓  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   |  ↑→  |  ↓   |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 4 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Policy evaluation converged after 1 iterations.\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "policy_stable = False\n",
    "threshold = 0.01\n",
    "j = 0\n",
    "while not policy_stable:\n",
    "    j += 1\n",
    "    print('*************************************************************************************')\n",
    "    print('Iteration ' + str(j))\n",
    "    print('*************************************************************************************')\n",
    "    i = 0\n",
    "    while True:\n",
    "        agent.policy_evaluation(mdp=mdp)\n",
    "        i += 1\n",
    "        if np.max(np.abs(agent.old_value_function - agent.value_function)) < threshold :\n",
    "            break\n",
    "    print(f'Policy evaluation converged after {i} iterations.')\n",
    "    print('Value Function:')\n",
    "    agent.render_value_function()\n",
    "    \n",
    "    agent.policy_improvement(mdp=mdp)\n",
    "\n",
    "    if np.max(np.abs(agent.old_policy - agent.policy)) == 0 :\n",
    "            policy_stable = True\n",
    "    print('Policy:')\n",
    "    agent.render_policy()\n",
    "    \n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbe9fd",
   "metadata": {},
   "source": [
    "## Alternating one policy evaluation step with policy improvement step (Value Iteration)\n",
    "\n",
    "Next, we **alternating one policy evaluation step with one policy improvement step**.  That is, we perform policy improvement after each sweep of policy evaluation.  This essentially corresponds to the Value Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac205bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************\n",
      "Initial setup\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "|  0.00 |  0.00 |  0.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 1\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -1.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ | ←↑↓→ | ←↑↓→ | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "| ←↑↓→ | ←↑↓→ |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 2\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -2.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   | ←↑↓→ | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "| ←↑↓→ |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 3\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n",
      "*************************************************************************************\n",
      "Iteration 4\n",
      "*************************************************************************************\n",
      "Value Function:\n",
      "---------------------------------\n",
      "|  0.00 | -1.00 | -2.00 | -3.00 | \n",
      "---------------------------------\n",
      "| -1.00 | -2.00 | -3.00 | -2.00 | \n",
      "---------------------------------\n",
      "| -2.00 | -3.00 | -2.00 | -1.00 | \n",
      "---------------------------------\n",
      "| -3.00 | -2.00 | -1.00 |  0.00 | \n",
      "---------------------------------\n",
      "Policy:\n",
      "-----------------------------\n",
      "|      |  ←   |  ←   |  ←↓  | \n",
      "-----------------------------\n",
      "|  ↑   |  ←↑  | ←↑↓→ |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑   | ←↑↓→ |  ↓→  |  ↓   | \n",
      "-----------------------------\n",
      "|  ↑→  |  →   |  →   |      | \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('*************************************************************************************')\n",
    "print('Initial setup')\n",
    "print('*************************************************************************************')\n",
    "print('Value Function:')\n",
    "agent.render_value_function()\n",
    "print('Policy:')\n",
    "agent.render_policy()\n",
    "policy_stable = False\n",
    "\n",
    "j = 0\n",
    "while not policy_stable:\n",
    "    j += 1\n",
    "    print('*************************************************************************************')\n",
    "    print('Iteration ' + str(j))\n",
    "    print('*************************************************************************************')\n",
    "    agent.policy_evaluation(mdp=mdp)\n",
    "    print('Value Function:')\n",
    "    agent.render_value_function()\n",
    "    \n",
    "    agent.policy_improvement(mdp=mdp)\n",
    "\n",
    "    if np.max(np.abs(agent.old_policy - agent.policy)) == 0 :\n",
    "            policy_stable = True\n",
    "    print('Policy:')\n",
    "    agent.render_policy()\n",
    "    \n",
    "agent.reset_value_function()\n",
    "agent.reset_old_value_function()\n",
    "agent.reset_policy()\n",
    "agent.reset_old_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede4986-b67e-41a9-980a-ccd083fa5723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
