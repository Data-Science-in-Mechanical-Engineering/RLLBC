


import gymnasium as gym
import numpy as np
import control as ct
import math
import custom_envs
import matplotlib.pyplot as plt
from typing import Optional, Union
from IPython.display import Video
from IPython.display import display
from matplotlib.animation import FuncAnimation
from gymnasium import logger, spaces
from gymnasium.envs.classic_control import utils

%matplotlib inline





env = gym.make('CustomCartPole-v1', render_mode='rgb_array')





cartpole = ct.NonlinearIOSystem(env.ct_sys_update, env.ct_sys_output, states=4, name='cartpole', inputs=['action'], outputs=['x', 'x_dot', 'theta', 'theta_dot'])
linsys = cartpole.linearize(x0=np.array([0., 0., 0., 0.]), u0=np.array([0.]))
linsys_d = linsys.sample(env.tau)





cost_x = 5
cost_x_dot = 1
cost_theta = 10
cost_theta_dot = 1
cost_control = 1

Q = np.diag([cost_x, cost_x_dot, cost_theta, cost_theta_dot])
R = np.diag([cost_control])

K, _, _ = ct.lqr(linsys_d, Q, R)





max_steps = 500
start_state = np.array([0.5, 0, 20 * 2 * math.pi / 360, 0])





done = False
steps = 0
state, _ = env.reset(start_state=start_state.copy())

cont_arr_lqr = np.zeros(max_steps)
state_arr_lqr = np.zeros([max_steps, 4])

frames = []  # collect rgb_image of agent env interaction

while not done:
    action = -np.matmul(K, state)[0]
    next_state, reward, done, _, _ = env.step(action)
    cont_arr_lqr[steps] = action
    state_arr_lqr[steps, :] = state
    out = env.render()
    frames.append(out)
    state = next_state
    steps += 1
    if steps == max_steps:
        done = True





def create_video(frames, title):
    if all(frame is not None for frame in frames):
        fig = plt.figure(figsize=(10, 6))
        plt.axis('off')
        img = plt.imshow(frames[0])

        def animate(index):
            img.set_data(frames[index])
            # plt.show()
            return [img]
        anim = FuncAnimation(fig, animate, frames=len(frames), interval=10)
        plt.close()
        anim.save(title, writer="ffmpeg", fps=30)


create_video(frames, 'cartpole_lqr.mp4')
Video("cartpole_lqr.mp4", html_attributes="loop autoplay")





cont_arr_lin_lqr = np.zeros(max_steps)
state_arr_lin_lqr = np.zeros([max_steps, 4])
state = start_state.copy()

for steps in range(max_steps):
    action = -np.matmul(K, state)[0]
    next_state = linsys_d.dynamics(env.tau, state, action)
    cont_arr_lin_lqr[steps] = action
    state_arr_lin_lqr[steps, :] = state
    state = next_state

plt.plot(state_arr_lin_lqr[:, 0], label='$x_{lin}$', color='gold')
plt.plot(state_arr_lqr[:, 0], label='$x$', color='darkorange')
plt.plot(state_arr_lin_lqr[:, 1], label='$\dot{x}_{lin}$', color='limegreen')
plt.plot(state_arr_lqr[:, 1], label='$\dot{x}$', color='darkgreen')
plt.plot(state_arr_lin_lqr[:, 2], label='$\\theta_{lin}$', color='lightsteelblue')
plt.plot(state_arr_lqr[:, 2], label='$\\theta$', color='darkblue')
plt.plot(state_arr_lin_lqr[:, 3], label='$\dot{\\theta}_{lin}$', color='tomato')
plt.plot(state_arr_lqr[:, 3], label='$\dot{\\theta}$', color='darkred')
plt.legend(loc='upper right')
plt.show()


plt.plot(cont_arr_lin_lqr, label='$u_{lin}$', color='gold')
plt.plot(cont_arr_lqr, label='$u$', color='darkorange')
plt.legend(loc='upper right')
plt.show()








max_steps = 500

done = False
steps = 0
state, _ = env.reset(start_state=start_state.copy())

cont_arr_ff = np.zeros(max_steps)
state_arr_ff = np.zeros([max_steps, 4])

frames = []  # collect rgb_image of agent env interaction

while not done:
    action = cont_arr_lin_lqr[steps]
    next_state, reward, done, _, _ = env.step(action)
    cont_arr_ff[steps] = action
    state_arr_ff[steps, :] = state
    out = env.render()
    frames.append(out)
    state = next_state
    steps += 1
    if steps == max_steps:
        done = True





create_video(frames, 'cartpole_feedforward.mp4')
Video("cartpole_feedforward.mp4", html_attributes="loop autoplay")


plt.plot(state_arr_ff[:, 0], label='$x$', color='darkorange')
plt.plot(state_arr_ff[:, 1], label='$\dot{x}$', color='darkgreen')
plt.plot(state_arr_ff[:, 2], label='$\\theta$', color='darkblue')
plt.plot(state_arr_ff[:, 3], label='$\dot{\\theta}$', color='darkred')
plt.legend(loc='upper right')
plt.show()





cont_arr_lin_ff = np.zeros(max_steps)
state_arr_lin_ff = np.zeros([max_steps, 4])
state = start_state.copy()

for steps in range(max_steps):
    action = cont_arr_lin_lqr[steps]
    next_state = linsys_d.dynamics(env.tau, state, action)
    cont_arr_lin_ff[steps] = action
    state_arr_lin_ff[steps, :] = state
    state = next_state

plt.plot(state_arr_lin_ff[:, 0], label='$x_{lin}$', color='gold')
plt.plot(state_arr_lin_ff[:, 1], label='$\dot{x}_{lin}$', color='limegreen')
plt.plot(state_arr_lin_ff[:, 2], label='$\\theta_{lin}$', color='lightsteelblue')
plt.plot(state_arr_lin_ff[:, 3], label='$\dot{\\theta}_{lin}$', color='tomato')
plt.legend(loc='upper right')
plt.show()





cont_arr_lin_ff = np.zeros(max_steps)
state_arr_lin_ff = np.zeros([max_steps, 4])
state = start_state.copy()+np.array([0., 0., 1e-15, 0.])

for steps in range(max_steps):
    action = cont_arr_lin_lqr[steps]
    next_state = linsys_d.dynamics(env.tau, state, action)
    cont_arr_lin_ff[steps] = action
    state_arr_lin_ff[steps, :] = state
    state = next_state

plt.plot(state_arr_lin_ff[:, 0], label='$x_{lin}$', color='gold')
plt.plot(state_arr_lin_ff[:, 1], label='$\dot{x}_{lin}$', color='limegreen')
plt.plot(state_arr_lin_ff[:, 2], label='$\\theta_{lin}$', color='lightsteelblue')
plt.plot(state_arr_lin_ff[:, 3], label='$\dot{\\theta}_{lin}$', color='tomato')
plt.legend(loc='upper right')
plt.show()
