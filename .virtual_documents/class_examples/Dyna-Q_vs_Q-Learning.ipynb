


import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import custom_envs
import copy
from IPython.display import Video
from IPython.display import display
from matplotlib.animation import FuncAnimation





def plot_action_value(q_fn, title): 
    fig, ax=plt.subplots(figsize=(7,5))
    lines = 4
    rows = 4        
    # Define grid positions:
    pos_x_left = 0.2
    pos_x_mid = 0.5
    pos_x_right = 0.8
    pos_y_up = 0.2
    pos_y_mid = 0.5
    pos_y_down = 0.8
    grid_size = {'x': lines, 'y': rows}
    def gridcreator(pos_x, pos_y):
        grid = []
        for i in range(grid_size['x']):
            for j in range(grid_size['y']):
                x = pos_x + j
                y = pos_y + i
                grid.append((x, y))
        return grid
    top = q_fn[:,3].reshape((4,4))
    top_value_positions = gridcreator(pos_x_mid, pos_y_up)
    right = q_fn[:,2].reshape((4,4))
    right_value_positions = gridcreator(pos_x_right, pos_y_mid)
    bottom = q_fn[:,1].reshape((4,4))
    bottom_value_positions = gridcreator(pos_x_mid, pos_y_down)
    left= q_fn[:,0].reshape((4,4))
    left_value_positions = gridcreator(pos_x_left, pos_y_mid)
    # Define triangles
    ax.set_ylim(lines, 0)
    anchor_points = np.array([[0,0],[0,1],[.5,.5],[1,0],[1,1]]) # Corner coordinates
    corner_indizes = np.array([[0,1,2], [0,2,3],[2,3,4],[1,2,4]]) # Corner indices
    xy_coordinates = np.zeros((lines * rows * 5,2))
    triangles = np.zeros((lines * rows * 4, 3))
    for i in range(lines):
        for j in range(rows):
            k = i*rows+j
            xy_coordinates[k*5:(k+1)*5,:] = np.c_[anchor_points[:,0]+j, 
                                                  anchor_points[:,1]+i]
            triangles[k*4:(k+1)*4,:] = corner_indizes + k*5
    colours = np.c_[left.flatten(), top.flatten(), 
            right.flatten(), bottom.flatten()].flatten()
    ax.triplot(xy_coordinates[:,0], xy_coordinates[:,1], triangles, 
               **{"color":"k", "lw":1})
    tripcolor = ax.tripcolor(xy_coordinates[:,0], xy_coordinates[:,1], triangles, 
                             facecolors=colours, **{"cmap": "coolwarm"})
    ax.margins(0)
    ax.set_aspect("equal")
    fig.colorbar(tripcolor)
    # Define text:
    textsize = 10
    for i, (xi,yi) in enumerate(top_value_positions):
        plt.text(xi,yi,round(top.flatten()[i],2), size=textsize, color="w", 
                 ha='center', va='center')
    for i, (xi,yi) in enumerate(right_value_positions):
        plt.text(xi,yi,round(right.flatten()[i],2), size=textsize, color="w", 
                 ha='center', va='center')
    for i, (xi,yi) in enumerate(left_value_positions):
        plt.text(xi,yi,round(left.flatten()[i],2), size=textsize, color="w", 
                 ha='center', va='center')
    for i, (xi,yi) in enumerate(bottom_value_positions):
        plt.text(xi,yi,round(bottom.flatten()[i],2), size=textsize, color="w", 
                 ha='center', va='center')
    ax.axis('off')
    plt.title(str(title))
    # add lines for separation
    for i in range(lines+1):
        x = [0, rows]
        y = [i, i]
        plt.plot(x,y, color='black')
    for i in range(rows+1):
        x = [i, i]
        y = [0, lines]
        plt.plot(x,y, color='black')
    plt.show()

def evaluate(agent, env, num_runs=1, file=None):
    frames = []
    video_created = False
    tot_reward = [0]
    for _ in range(num_runs):
        done = False
        obs, info = env.reset()
        reward_per_run = 0
        while not done:
            action = agent.get_best_action(obs)
            obs, reward, done, truncated, info = env.step(action)
            reward_per_run += reward
            # save frame
            out = env.render()
            frames.append(out)
        tot_reward.append(reward_per_run + tot_reward[-1])

    # create animation out of saved frames
    if all(frame is not None for frame in frames):
        fig = plt.figure(figsize=(10, 6))
        plt.axis('off')
        img = plt.imshow(frames[0])

        def animate(index):
            img.set_data(frames[index])
            return [img]
        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)
        plt.close()
        anim.save(file, writer="ffmpeg", fps=5)
        video_created = True

    return tot_reward, video_created





class TD_Agent():
    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1, dyn_q_iters=25):
        self.env = env
        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n)) # the q-fn
        self.model = np.zeros((self.env.observation_space.n, self.env.action_space.n, 2))
        class StateActionTracker:
            def __init__(self):
                self.states = {}
                self.actions = {}
            def add_state_action_pair(self, state, action):
                if state not in self.states:
                    self.states[state] = 1
                    self.actions[state] = set([action])
                else:
                    self.states[state] += 1
                    if action not in self.actions[state]:
                        self.actions[state].add(action)
            def get_state(self):
                if self.states:
                    return np.random.choice(list(self.states.keys()))
            def get_action(self, state):
                if state in self.actions and self.actions[state]:
                    return np.random.choice(list(self.actions[state]))
        self.visited_states_and_actions = StateActionTracker()
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.dyn_q_iters = dyn_q_iters

    def get_random_action(self):
        random_action = np.random.choice(range(self.env.action_space.n))
        return random_action
    def get_best_action(self, obs):
        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(), rtol=0.01)))
        return best_action
    
    def epsilon_greedy_policy(self, obs):
        # returns action, choosing a random action with probability epsilon or the best action based on Q(s,a)
        randomly = np.random.random() < self.epsilon
        if randomly:
            action = self.get_random_action()
        else:
            action = self.get_best_action(obs)
        return action
    
    def q_learning_update(self, obs, action, next_obs, reward):
        best_next_action = self.get_best_action(next_obs)
        td_target = reward + self.gamma * self.action_value_fn[next_obs][best_next_action]
        update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target
        self.action_value_fn[obs][action] = update
        
    def q_learning(self, num_episodes):
        returns = []
        for i in range(num_episodes):
            returns_i = 0
            obs, info = self.env.reset()
            done = False
            while not done:
                # Generate next_obs and reward based on state-action pair
                action = self.epsilon_greedy_policy(obs)
                next_obs, reward, done, truncated, info = self.env.step(action)
                # Update the value function
                self.q_learning_update(obs, action, next_obs, reward)
                obs = next_obs
                returns_i += reward
            returns.append(returns_i)
        return returns
    
    def dyna_q(self, num_episodes):
        returns = []
        for i in range(num_episodes):
            returns_i = 0
            obs, info = self.env.reset()
            done = False
            while not done:
                # Generate next_obs and reward based on state-action pair
                action = self.epsilon_greedy_policy(obs)
                next_obs, reward, done, truncated, info = env.step(action)
                # Update the value function
                self.q_learning_update(obs, action, next_obs, reward)
                # Update the model
                self.model[obs][action] = np.array([next_obs, reward]) # Assuming deterministic environment
                self.visited_states_and_actions.add_state_action_pair(obs, action)
                returns_i += reward
                obs = next_obs
                # Perform more updates using the model
                for j in range(self.dyn_q_iters):
                    mod_state = self.visited_states_and_actions.get_state()
                    mod_action = self.visited_states_and_actions.get_action(mod_state)
                    mod_next_state, mod_reward = int(self.model[mod_state][mod_action][0]), self.model[mod_state][mod_action][1]
                    self.q_learning_update(mod_state, mod_action, mod_next_state, mod_reward)
            returns.append(returns_i)
        return returns

    def reset(self):
        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))





def smooth(array, size):
    window_size = size
    filter = np.ones(window_size) / window_size
    smoothed_array = np.convolve(array, filter, mode='same')
    return smoothed_array





# Import visualization
setattr(TD_Agent, "evaluate", evaluate)

# Set values
map = ["SFHH", "HFFH", "HFFH", "HHFG"]
env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False)
render_env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)
filter = 50
num_episodes = 10000
env.reset()

agent_1 = TD_Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.2)
q_learning_training_history = smooth(agent_1.q_learning(num_episodes=num_episodes), filter)
plot_action_value(agent_1.action_value_fn, "Q-Learning")
video_file_1 = "q_agent.mp4"
agent_1.evaluate(render_env, 1, video_file_1)
Video(video_file_1, html_attributes="loop autoplay")


agent_2 = TD_Agent(env, gamma=0.9, learning_rate=0.1, epsilon=0.2)
dyna_q_training_history = smooth(agent_2.dyna_q(num_episodes=num_episodes), filter)
plot_action_value(agent_2.action_value_fn, "Dyna-Q")
video_file_2 = "dyna_q_agent.mp4"
agent_2.evaluate(render_env, 1, video_file_2)
Video(video_file_2, html_attributes="loop autoplay")





fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.
ax.plot(range(num_episodes), q_learning_training_history, label="Q-Learning")
ax.plot(range(num_episodes), dyna_q_training_history, label="Dyna-Q")
ax.grid(False)
ax.set_xlabel('Episodes')
ax.set_ylabel('Returns')
ax.set_title("Comparison of Dyna-Q and Q-Learning")
ax.set_ylim(-1, 2)
ax.set_xlim(0, 1000)
ax.legend()
plt.show()
