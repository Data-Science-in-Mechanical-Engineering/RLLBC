





import time
import warnings
from collections import defaultdict
from IPython.display import Video
from IPython.display import display
import numpy as np
import custom_envs
import gymnasium as gym
from tqdm import notebook
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation





class Agent():
    def __init__(self, env, gamma=0.9, epsilon=0.1, learning_rate=0.05):
        self.env = env
        self.action_value_fn = np.zeros((self.env.observation_space.n, self.env.action_space.n))
        self.gamma = gamma 
        self.epsilon = epsilon 
        self.rtol = 0.08
        # Only necessary for first-visit MC:
        self.returns = defaultdict(float) 
        self.returns_count = defaultdict(float)
        # Only necessary for SARSA
        self.learning_rate = learning_rate

    def get_random_action(self):
        random_action = np.random.choice(range(self.env.action_space.n))
        return random_action
    
    def get_best_action(self, obs):
        best_action = np.random.choice(np.flatnonzero(np.isclose(self.action_value_fn[obs], self.action_value_fn[obs].max(),
                                                                 self.rtol))) 
        return best_action
    
    def epsilon_greedy_policy(self, obs):
        randomly = np.random.random() < self.epsilon
        if randomly:
            action = self.get_random_action()
        else:
            action = self.get_best_action(obs)
        return action





def first_visit_MC(self, num_episodes=2000):
    returns_log = [] # Used for logging
    # Run through episodes sampled to improve policy incrementally
    for i_episode in range(1, num_episodes + 1):
        # Generate an episode (state, action, reward)
        episode = []
        returns_i = 0
        obs, info = env.reset()
        done = False
        while not done:
            action = self.epsilon_greedy_policy(obs)
            next_obs, reward, done, truncated, info = env.step(action)
            episode.append((obs, action, reward))
            returns_i += reward
            obs = next_obs
        returns_log.append(returns_i)
        episode = np.array(episode)
        episode_duration = len(episode[:,:1])
        # Calculate returns for the whole episode from the back to save memory and resources
        G = np.zeros([episode_duration, ])
        for i in range(episode_duration - 1, -1, -1):
            if i + 1 > episode_duration - 1:
                G[i] = episode[i][2]
            else:
                G[i] = episode[i][2] + self.gamma * G[i + 1]
        # Find indices of first visits of state-action pairs in the episode
        first_visit_indices = sorted(np.unique(episode[:,:1], return_index=True)[1])
        # Update the policy with average over all episodes
        for index in first_visit_indices:
            state = episode[index][0]
            action = episode[index][1]
            self.returns[(state, action)] += G[index]
            self.returns_count[(state, action)] += 1.0
            update = self.returns[(state, action)] / self.returns_count[(state, action)]
            self.action_value_fn[int(state)][int(action)] = update
    return returns_log

setattr(Agent, "first_visit_MC", first_visit_MC)





def sarsa(self, num_episodes=2000):
    returns_log = [] # Used for logging
    # Run through episodes
    for i in range(num_episodes):
        returns_i = 0
        obs, info = env.reset()
        action = self.epsilon_greedy_policy(obs)
        done = False
        while not done:
            # In each step, update the action value function
            next_obs, reward, done, truncated, info = env.step(action)
            next_action = self.epsilon_greedy_policy(next_obs)
            td_target = reward + self.gamma * self.action_value_fn[next_obs][next_action]
            update = (1-self.learning_rate) * self.action_value_fn[obs][action] + self.learning_rate * td_target
            self.action_value_fn[obs][action] = update
            obs = next_obs
            action = next_action
            returns_i += reward
        returns_log.append(returns_i)
    return returns_log

setattr(Agent, "sarsa", sarsa)





# Initialize environment
map = ["SFFH", "FFFH", "HFFH", "HFFG"]
env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) # render mode is None to speed up learning
env.reset()

# Parameters
gamma = 0.9
epsilon = 0.1
learning_rate = 0.1
train_episodes = 2000

# Initialize agents
mc_agent = Agent(env, gamma, epsilon)
td_agent = Agent(env, gamma, epsilon, learning_rate)

# Train agents
mc_training_history = mc_agent.first_visit_MC(train_episodes)
td_training_history = td_agent.sarsa(train_episodes)





def smooth(array, size):
    window_size = size
    filter = np.ones(window_size) / window_size
    smoothed_array = np.convolve(array, filter, mode='same')
    return smoothed_array





# Set the size for smoothing
size = 100

# Compare the returns
fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.
ax.plot(range(train_episodes), smooth(mc_training_history, size), label="MC-Agent")
ax.plot(range(train_episodes), smooth(td_training_history, size), label="TD-Agent")
ax.grid(False)
ax.set_xlabel('Episodes')
ax.set_ylabel('Total Rewards')
ax.set_title("Comparison of MC and TD Training")
ax.set_ylim(0, 1)
ax.set_xlim(0, train_episodes-size)
ax.legend()
plt.show()





def evaluate(self, env, num_runs=10, file=None):
    frames = []  # collect rgb_image of agent env interaction
    video_created = False
    for _ in range(num_runs):
        done = False
        obs, info = env.reset()
        reward_per_run = 0
        out = env.render()
        frames.append(out)
        while not done:
            action = self.get_best_action(obs)
            obs, reward, done, truncated, info = env.step(action)
            reward_per_run += reward
            # save frame
            out = env.render()
            frames.append(out)

    # create animation out of saved frames
    if all(frame is not None for frame in frames):
        fig = plt.figure(figsize=(10, 6))
        plt.axis('off')
        img = plt.imshow(frames[0])

        def animate(index):
            img.set_data(frames[index])
            return [img]
        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)
        plt.close()
        anim.save(file, writer="ffmpeg", fps=5)
        video_created = True

    return video_created

setattr(Agent, "evaluate", evaluate)





# Switching the render_mode to rgb_array
env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False)

# Defining the number of runs for the evaluation
num_runs = 10





mc_video_file = "mc_trained.mp4"
mc_agent.evaluate(env, num_runs, mc_video_file)
Video(mc_video_file, html_attributes="loop autoplay")





td_video_file = "td_trained.mp4"
td_agent.evaluate(env, num_runs, td_video_file)
Video(td_video_file, html_attributes="loop autoplay")
