


from collections import defaultdict
import matplotlib.pyplot as plt
import gymnasium as gym
import custom_envs
import numpy as np
import warnings
# Ignore numpy depraction warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)





class MC_Agent():
    def __init__(self, env, gamma=1.0):
        self.env = env
        self.returns = defaultdict(float) # Summing up returns in each state
        self.returns_count = defaultdict(float) # Keeping track of count in each state
        self.action_value_fn = np.zeros((self.env.observation_space[0].n, self.env.observation_space[1].n, self.env.observation_space[2].n, self.env.action_space.n))
        self.gamma = gamma

    def get_random_action(self):
        random_action = np.random.choice(range(self.env.action_space.n))
        return random_action

    def get_greedy_action(self, obs):
        action_probability = np.isclose(self.action_value_fn[obs],self.action_value_fn[obs].max(),rtol=0.01)
        actions = np.flatnonzero(action_probability)
        best_action = np.random.choice(actions)
        return best_action

    def train(self, num_episodes=2000, episode_max_duration=100):
        # Run through episodes sampled to improve policy incrementally
        for i_episode in range(1, num_episodes + 1):
            # Generate an episode (state, action, reward)
            episode = []
            obs, info = env.reset()
            for t in range(episode_max_duration):
                # For realizing exploring starts, a random action is first chosen...
                if t == 0:
                    action = self.get_random_action()
                # ... and a greedy action is chosen in all subsequent states
                else:
                    action = self.get_greedy_action(obs)
                next_obs, reward, done, truncated, info = env.step(action)
                episode.append((obs, action, reward))
                if done:
                    break
                obs = next_obs
            episode = np.array(episode)
            episode_duration = len(episode[:,:1])
            # Calculate returns for the whole episode from the back to save memory and resources
            G = np.zeros([episode_duration, ])
            for i in range(episode_duration - 1, -1, -1):
                if i + 1 > episode_duration - 1:
                    G[i] = episode[i][2] # Last step of the episode
                else:
                    G[i] = episode[i][2] + self.gamma * G[i + 1]  # Every other step #!
            # Find indices of first visits of state-action pairs in the episode
            first_visit_indices = sorted(np.unique(episode[:,:1], return_index=True)[1])
            # Update the policy with average over all episodes
            for index in first_visit_indices:
                state = episode[index][0]
                action = episode[index][1]
                self.returns[(state, action)] += G[index]
                self.returns_count[(state, action)] += 1.0
                update = self.returns[(state, action)] / self.returns_count[(state, action)]
                self.action_value_fn[state][int(action)] = update





def evaluate(self, env, num_runs=1):
    tot_reward = [0]
    for _ in range(num_runs):
        done = False
        obs, info = env.reset()
        reward_per_run = 0
        while not done:
            action = self.get_greedy_action(obs)
            obs, reward, done, truncated, info = env.step(action)
            reward_per_run += reward
        tot_reward.append(reward_per_run + tot_reward[-1])
    return tot_reward

setattr(MC_Agent, "evaluate", evaluate)





env = gym.make('BlackJack-v1', render_mode=None, sab=True)  # set render_mode=None for speeding up learning
num_runs = 500
env.reset()

# Evaluating a random agent
random_agent = MC_Agent(env, gamma=0.9)
random_results = random_agent.evaluate(env, num_runs=num_runs)

# Evaluating a trained agent
env.reset()
trained_agent = MC_Agent(env, gamma=0.9)
trained_agent.train(num_episodes=1000000)
trained_results = trained_agent.evaluate(env, num_runs=num_runs)

# Comparing both agents
fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.
ax.plot(range(num_runs + 1), random_results, label="Random Agent")
ax.plot(range(num_runs + 1), trained_results, label="Trained Agent")
ax.set_xlabel('Episodes')
ax.set_ylabel('Cumulative Reward')
ax.legend()
plt.show()





def visualize_policy(self):
    player_sum = np.array(['12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])
    dealer_showing = np.array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])
    x11 = []
    y11 = []
    x12 = []
    y12 = []
    x21 = []
    y21 = []
    x22 = []
    y22 = []
    # Only a part of the value function array is necessary for showing the policy. This is sliced following S&B:
    sliced_value_fn = self.action_value_fn[12:22, 1:11, :, 0]
    # We use this array to retrieve the indices for player current sum, dealer value, and usable ace
    for i in range(sliced_value_fn.shape[0]):
        for j in range(sliced_value_fn.shape[1]):
            for k in range(sliced_value_fn.shape[2]):
                # Collect values for the policy
                orig_i = i + 12
                orig_j = j + 1
                orig_k = k
                if orig_k == 1:
                    if self.get_greedy_action((orig_i, orig_j, orig_k)) == 0:
                        x11.append(j)
                        y11.append(i)
                    else:
                        x12.append(j)
                        y12.append(i)
                else:
                    if self.get_greedy_action((orig_i, orig_j, orig_k)) == 0:
                        x21.append(j)
                        y21.append(i)
                    else:
                        x22.append(j)
                        y22.append(i)
    # Print the policy
    plt.figure(0)
    plt.title('With Usable Ace')
    plt.scatter(x11, y11, color='red')
    plt.scatter(x12, y12, color='blue')
    plt.xticks(range(len(dealer_showing)), dealer_showing)
    plt.yticks(range(len(player_sum)), player_sum)
    plt.xlabel('Dealer Card')
    plt.ylabel('Player Sum')
    plt.figure(1)
    plt.title('Without Usable Ace')
    plt.scatter(x21, y21, color='red')
    plt.scatter(x22, y22, color='blue')
    plt.xticks(range(len(dealer_showing)), dealer_showing)
    plt.yticks(range(len(player_sum)), player_sum)
    plt.xlabel('Dealer Card')
    plt.ylabel('Player Sum')
    
setattr(MC_Agent, "visualize_policy", visualize_policy)





trained_agent.visualize_policy()





def visualize_value_function(self):
    player_sum = np.array(['12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])
    dealer_showing = np.array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])
    # Only a part of the value function array is necessary for showing the policy. This is sliced following S&B:
    sliced_value_fn = self.action_value_fn[12:22, 1:11, :, 0]
    state_value_fn = np.zeros((10, 10, 2))
    # We use this array to retrieve the indices for player current sum, dealer value, and usable ace
    for i in range(sliced_value_fn.shape[0]):
        for j in range(sliced_value_fn.shape[1]):
            for k in range(sliced_value_fn.shape[2]):
                # Collect values for the policy
                orig_i = i + 12
                orig_j = j + 1
                orig_k = k
                # Compute the state_value function
                if np.allclose(self.action_value_fn[orig_i, orig_j, orig_k],
                               self.action_value_fn[orig_i, orig_j, orig_k][0], rtol=0.01, atol=0.01):
                    state_value_fn[i, j, k] = sum(self.action_value_fn[
                        orig_i, orig_j, orig_k])/2
                else:
                    state_value_fn[i, j, k] = self.action_value_fn[
                        orig_i, orig_j, orig_k, self.get_greedy_action((orig_i, orig_j, orig_k))]
    # Print the value function
    fig, axes = plt.subplots(nrows=2, figsize=(5, 8), subplot_kw={'projection': '3d'})
    axes[0].set_title('Without Usable Ace')
    axes[1].set_title('With Usable Ace')
    X, Y = np.meshgrid(dealer_showing.astype(int), player_sum.astype(int))
    axes[0].plot_wireframe(X, Y, state_value_fn[:, :, 0])
    axes[1].plot_wireframe(X, Y, state_value_fn[:, :, 1])
    for ax in axes[0], axes[1]:
        ax.set_zlim(-1, 1)
        ax.set_ylabel('Player Sum')
        ax.set_xlabel('Dealer Showing')
        ax.set_zlabel('State-Value')
    plt.show()

setattr(MC_Agent, "visualize_value_function", visualize_value_function)





trained_agent.visualize_value_function()
