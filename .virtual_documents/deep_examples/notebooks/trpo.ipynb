





import os
import time
import copy
import random
import warnings
from datetime import datetime
from functools import partial

import numpy as np
import matplotlib.pyplot as plt
from tqdm import notebook
from easydict import EasyDict as edict
from IPython.display import Video

import utils.helper_fns as hf

import gym
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical


os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'trpo.ipynb'

warnings.filterwarnings("ignore", category=DeprecationWarning)

plt.rcParams['figure.dpi'] = 100
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

%load_ext autoreload
%autoreload 2





def kl_divergence(dist_true, dist_pred):
    """
    Wrapper for the PyTorch implementation of the full form KL Divergence

    :param dist_true: the p distribution
    :param dist_pred: the q distribution
    :return: KL(dist_true||dist_pred)
    """
    return torch.distributions.kl_divergence(dist_true, dist_pred)


def compute_policy_grad(actor, kl_div, policy_objective):
    """
    Compute gradients for kl div and surrogate objective wrt the policy parameters
    :param actor: actor model in agent
    :param kl_div: The KL divergence objective
    :param policy_objective: The surrogate advantage
    :return: List of actor params, gradients and gradients shape.
    """
    # Contains the gradients of surrogate advantage wrt to each policy parameters
    policy_objective_gradients = []
    # Contains gradients of the KL divergence wrt each policy parameter
    grad_kl = []
    # Save the shape of the gradients of the KL divergence w.r.t each policy parameter
    # Will be used to unflatten the gradients into its original shape later in the training loop
    grad_shape = []
    # List of policy parameters
    actor_params = []

    for param in actor.parameters():
        kl_param_grad, *_ = torch.autograd.grad(
            kl_div,
            param,
            create_graph=True,
            retain_graph=True,
            allow_unused=True,
            only_inputs=True
        )

        policy_objective_grad, *_ = torch.autograd.grad(
            policy_objective,
            param,
            retain_graph=True,
            only_inputs=True
        )

        grad_shape.append(kl_param_grad.shape)
        grad_kl.append(kl_param_grad.reshape(-1))
        policy_objective_gradients.append(policy_objective_grad.reshape(-1))
        actor_params.append(param)

    # Concatenate gradients before the conjugate gradient step
    policy_objective_gradients = torch.cat(policy_objective_gradients)
    grad_kl = torch.cat(grad_kl)
    return actor_params, policy_objective_gradients, grad_kl, grad_shape


def hessian_vector_product(params, grad_kl, cg_damping, vector, retain_graph=True):
    """
    Computes the matrix-vector product with the Fisher information matrix.

    :param params: list of parameters used to compute the Hessian
    :param grad_kl: flattened gradient of the KL divergence between the old and new policy
    :param vector: vector to compute the dot product the hessian-vector dot product with
    :param retain_graph: if True, the graph will be kept after computing the Hessian
    :return: Hessian-vector dot product (with damping)
    """
    jacobian_vector_product = (grad_kl * vector).sum()

    hessian_vector_product = torch.autograd.grad(
        jacobian_vector_product,
        params,
        create_graph=False,
        retain_graph=retain_graph,
        allow_unused=True
    )

    hessian_vector_product_flattened = torch.cat([torch.ravel(grad) for grad in hessian_vector_product if grad is not None])
    return hessian_vector_product_flattened + cg_damping * vector


def conjugate_gradient_solver(matrix_vector_dot_fn, b, max_iter=10, residual_tol=1e-10):
    """
    Finds an approximate solution to a set of linear equations Ax = b

    Sources:
     - https://github.com/ajlangley/trpo-pytorch/blob/master/conjugate_gradient.py
     - https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py#L122

    Reference:
     - https://epubs.siam.org/doi/abs/10.1137/1.9781611971446.ch6

    :param matrix_vector_dot_fn:
        a function that right multiplies a matrix A by a vector v
    :param b:
        the right hand term in the set of linear equations Ax = b
    :param max_iter:
        the maximum number of iterations (default is 10)
    :param residual_tol:
        residual tolerance for early stopping of the solving (default is 1e-10)
    :return x:
        the approximate solution to the system of equations defined by `matrix_vector_dot_fn`
        and b
    """

    # The vector is not initialized at 0 because of the instability issues when the gradient becomes small.
    # Hence, a small random gaussian noise is used for the initialization.
    x = 1e-4 * torch.randn_like(b)
    residual = b - matrix_vector_dot_fn(x)
    # Equivalent to th.linalg.norm(residual) ** 2 (L2 norm squared)
    residual_squared_norm = torch.matmul(residual, residual)

    if residual_squared_norm < residual_tol:
        # If the gradient becomes extremely small
        # The denominator in alpha will become zero
        # Leading to a division by zero
        return x

    p = residual.clone()

    for i in range(max_iter):
        # A @ p (matrix vector multiplication)
        A_dot_p = matrix_vector_dot_fn(p)

        alpha = residual_squared_norm / p.dot(A_dot_p)
        x += alpha * p

        if i == max_iter - 1:
            return x

        residual -= alpha * A_dot_p
        new_residual_squared_norm = torch.matmul(residual, residual)

        if new_residual_squared_norm < residual_tol:
            return x

        beta = new_residual_squared_norm / residual_squared_norm
        residual_squared_norm = new_residual_squared_norm
        p = residual + beta * p
    # Note: this return statement is only used when max_iter=0
    return x








exp = edict()

exp.exp_name = 'TRPO'  # algorithm name, in this case it should be 'PPO'
exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.device = device.type  # save the device type used to load tensors and perform tensor operations

exp.random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RLLBC_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if exp.random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = exp.random_seed





hypp = edict()

exp.num_envs = 4  # number of parallel game environments
hypp.num_steps = 128  # number of steps to run in each environment per policy rollout

# Intialize vectorized gym evn
envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])

# RollOut Buffer Init
observations = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_observation_space.shape).to(device)
actions = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_action_space.shape).to(device)
logprobs = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
rewards = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
dones = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
values = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)

envs.close()





def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class Agent(nn.Module):
    def __init__(self, envs):
        super().__init__()
        self.critic = nn.Sequential(
            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 1), std=1.0),
        )
        self.actor = nn.Sequential(
            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),
        )

    def get_value(self, x):
        return self.critic(x)

    def get_distribution(self, x):
        logits = self.actor(x)
        probs = Categorical(logits=logits)
        return probs

    def get_action(self, x, greedy=False):
        distr = self.get_distribution(x)
        action = distr.sample() if not greedy else distr.mode
        return action

    def get_action_and_value(self, x, action=None):
        distr = self.get_distribution(x)
        if action is None:
            action = distr.sample()
        return action, distr.log_prob(action), self.critic(x)





# flags for logging purposes
exp.enable_wandb_logging = True
exp.capture_video = False  # disable to speed up training

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 10
exp.eval_frequency = 20
exp.device = device.type

# putting the run into the designated log folder for structuring
exp.exp_type = None  # directory the run is saved to. Should be None or a string value

# agent learning specific flags
hypp.total_timesteps = 500000  # the training duration in number of time steps
hypp.num_minibatches = 4  # number of minibatches for gradient updates
hypp.update_epochs = 10  # only applies to critic updates in TRPO

hypp.batch_size = int(exp.num_envs * hypp.num_steps)  # len of the rollout buffer
hypp.minibatch_size = int(hypp.batch_size // hypp.num_minibatches)  # rollout buffer size / minibatch count

hypp.learning_rate = 1e-3  # size of gradient update step
hypp.anneal_lr = False  # when True reduces the learning rate as the training progresses
hypp.gamma = 0.99  # discount factor over future rewards
hypp.gae_lambda = 0.95  # factor for trade-off bias vs variance for Generalized Advantage Estimator
hypp.norm_adv = True
hypp.cg_max_steps = 10                   # maximum number of steps in the Conjugate Gradient algorithm for computing the Hessian vector product
hypp.cg_damping = 0.1                    # damping in the Hessian vector product computation
hypp.line_search_shrinking_factor = 0.8  # step-size reduction factor for the line-search (i.e., `theta_new = theta + shrinking_factor^i * step`)
hypp.line_search_max_iter = 10           # maximum number of iteration for the backtracking line-search
hypp.target_kl = 0.01  # the target KL divergence threshold for the policy





# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

# Init tensorboard logging and wandb logging
writer = hf.setup_logging(wandb_prj_name, exp, hypp)

# create two vectorized envs: one to fill the rollout buffer with trajectories, and
# another to evaluate the agent performance at different stages of training  
envs.close()
envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])
envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.eval_count)])

# init list to track agent's performance throughout training
tracked_returns_over_training = []
tracked_episode_len_over_training = []
tracked_episode_count = []
last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated
greedy_evaluation = False  # whether to perform the evaluation in a greedy way or not
eval_max_return = -float('inf')

# Create Agent class Instance and network optimizer
agent = Agent(envs).to(device)
optimizer = optim.Adam(agent.critic.parameters(), lr=hypp.learning_rate)

# Init observation to start learning
start_time = time.time()
obs = torch.Tensor(envs.reset()).to(device)
done = torch.zeros(exp.num_envs).to(device)
num_updates = int(hypp.total_timesteps // hypp.batch_size)

global_step = 0
episode_step = 0
gradient_step = 0

pbar = notebook.tqdm(range(1, num_updates + 1))

# training loop
for update in pbar:
    # Annealing the rate if instructed to do so.
    if hypp.anneal_lr:
        frac = 1.0 - (update - 1.0) / num_updates
        lrnow = frac * hypp.learning_rate
        optimizer.param_groups[0]["lr"] = lrnow

    agent.eval()

    # collect trajectories
    for step in range(0, hypp.num_steps):
        observations[step] = obs
        dones[step] = done

        # sample action and collect value from learned agent policy and value networks
        with torch.no_grad():
            action, logprob, value = agent.get_action_and_value(obs)
            values[step] = value.flatten()
        actions[step] = action
        logprobs[step] = logprob

        # execute the game and log data.
        next_obs, reward, done, infos = envs.step(action.cpu().numpy())

        for idx, info in enumerate(infos):
            agent.eval()
            # bootstrap value of the observation when done is true and the episode is truncated
            if (
                done[idx]
                and info.get("terminal_observation") is not None
                and info.get("TimeLimit.truncated", False)
               ):
                terminal_obs = torch.tensor(info["terminal_observation"]).to(device)
                with torch.no_grad():
                    terminal_value = agent.get_value(terminal_obs)
                reward[idx] += hypp.gamma * terminal_value

            # log episode return and length to tensorboard
            if "episode" in info.keys():
                episode_step += 1
                pbar.set_description(f"global_step={global_step}, episodic_return={info['episode']['r']}")
                writer.add_scalar("rollout/episodic_return", info["episode"]["r"], global_step+idx)
                writer.add_scalar("rollout/episodic_length", info["episode"]["l"], global_step+idx)
                writer.add_scalar("Charts/episode_step", episode_step, global_step)
                writer.add_scalar("Charts/gradient_step", gradient_step, global_step)

        rewards[step] = torch.tensor(reward).to(device).view(-1)
        next_obs, done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)

        global_step += 1 * exp.num_envs
        obs = next_obs

        # generate average performance statistics of current learned agent
        if exp.eval_agent and episode_step % exp.eval_frequency == 0 and last_evaluated_episode != episode_step:
            last_evaluated_episode = episode_step
            tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)
            tracked_returns_over_training.append(tracked_return)
            tracked_episode_len_over_training.append(tracked_episode_len)
            tracked_episode_count.append([episode_step, global_step])

            # if there has been improvment of the model -
            if np.mean(tracked_return) > eval_max_return:
                eval_max_return = np.mean(tracked_return)
                # call helper function save_and_log_agent to save model, create video, log video to wandb
                hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)

    next_done = done
    agent.train()

    # compute generalised advantage estimate and returns
    with torch.no_grad():
        next_value = agent.get_value(next_obs).reshape(1, -1)
        advantages = torch.zeros_like(rewards).to(device)
        lastgaelam = 0
        for t in reversed(range(hypp.num_steps)):
            if t == hypp.num_steps - 1:
                nextnonterminal = 1.0 - next_done
                nextvalues = next_value
            else:
                nextnonterminal = 1.0 - dones[t + 1]
                nextvalues = values[t + 1]
            delta = rewards[t] + hypp.gamma * nextvalues * nextnonterminal - values[t]
            advantages[t] = lastgaelam = delta + hypp.gamma * hypp.gae_lambda * nextnonterminal * lastgaelam
        returns = advantages + values

    # flatten the batch
    b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)
    b_logprobs = logprobs.reshape(-1)
    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
    b_advantages = advantages.reshape(-1)
    b_returns = returns.reshape(-1)
    b_values = values.reshape(-1)

    with torch.no_grad():
        old_distribution = copy.copy(agent.get_distribution(b_observations))

    distribution = agent.get_distribution(b_observations)
    log_prob = distribution.log_prob(b_actions)

    if hypp.norm_adv:
        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)

    # ratio between old and new policy, should be one at the start of each iteration
    ratio = torch.exp(log_prob - b_logprobs)

    # compute the surrogate advantage
    policy_objective = (b_advantages * ratio).mean()

    # KL Divergence
    kl_div = kl_divergence(distribution, old_distribution).mean()

    # Compute the gradient of the surroagate advantage and the kl divergence wrt to the policy network parameters
    optimizer.zero_grad()
    actor_params, policy_objective_gradients, grad_kl, grad_shape = compute_policy_grad(agent.actor, kl_div,
                                                                                        policy_objective)

    # Compute search_direction, ie ${H^-1}.{g}$, using the conjugate gradient method
    hessian_vector_product_fn = partial(hessian_vector_product, actor_params, grad_kl, hypp.cg_damping)

    search_direction = conjugate_gradient_solver(
        hessian_vector_product_fn,
        policy_objective_gradients,
        max_iter=hypp.cg_max_steps
    )

    line_search_max_step_size = 2 * hypp.target_kl
    line_search_max_step_size /= torch.matmul(
        search_direction, hessian_vector_product_fn(search_direction, retain_graph=False)
    )
    line_search_max_step_size = torch.sqrt(line_search_max_step_size)

    line_search_backtrack_coeff = 1.0
    original_actor_params = [param.detach().clone() for param in actor_params]

    policy_objective_values = []  # average value of the surrogate advantage
    kl_divergences = []  # average KL-divergence between policies across states visited by the old policy

    is_line_search_success = False
    with torch.no_grad():
        # update policy parameters by backtracking line-search
        for _ in range(hypp.line_search_max_iter):

            start_idx = 0
            # Applying the scaled step direction
            for param, original_param, shape in zip(actor_params, original_actor_params, grad_shape):
                n_params = param.numel()
                param.data = (
                        original_param.data
                        + line_search_backtrack_coeff
                        * line_search_max_step_size
                        * search_direction[start_idx: (start_idx + n_params)].view(shape)
                )
                start_idx += n_params

            # Recomputing the policy log-probabilities
            distribution = agent.get_distribution(b_observations)
            log_prob = distribution.log_prob(b_actions)

            # New policy objective
            ratio = torch.exp(log_prob - b_logprobs)
            new_policy_objective = (b_advantages * ratio).mean()

            # New KL-divergence
            kl_div = kl_divergence(distribution, old_distribution).mean()

            # Constraint criteria:
            # we need to improve the surrogate policy objective
            # while being close enough (in terms of kl div) to the old policy
            if (kl_div < hypp.target_kl) and (new_policy_objective > policy_objective):
                is_line_search_success = True
                break

            # Reducing step size if line-search wasn't successful
            line_search_backtrack_coeff *= hypp.line_search_shrinking_factor

        if not is_line_search_success:
            # If the line-search wasn't successful we revert to the original parameters
            for param, original_param in zip(actor_params, original_actor_params):
                param.data = original_param.data.clone()

            policy_objective_values = policy_objective.item()
            kl_divergences = 0
        else:
            policy_objective_values = new_policy_objective.item()
            kl_divergences = kl_div.item()

    b_inds = np.arange(hypp.batch_size)

    value_losses = []

    # update the critic network
    for i in range(hypp.update_epochs):
        np.random.shuffle(b_inds)
        for start in range(0, hypp.batch_size, hypp.minibatch_size):
            end = start + hypp.minibatch_size
            mb_inds = b_inds[start:end]

            newvals = agent.get_value(b_observations[mb_inds])
            newvals = newvals.view(-1)

            v_loss = 0.5 * ((newvals - b_returns[mb_inds]) ** 2).mean()

            optimizer.zero_grad()
            v_loss.backward()
            optimizer.step()

            value_losses.append(v_loss.item())
            gradient_step += 1

    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
    var_y = np.var(y_true)
    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y

    # log losses to tensorboard summary writer
    writer.add_scalar("hyperparameters/learning_rate", optimizer.param_groups[0]["lr"], global_step)
    writer.add_scalar("train/value_loss", np.mean(value_losses), global_step)
    writer.add_scalar("train/kl_divergence", kl_divergences, global_step)
    writer.add_scalar("train/explained_variance", explained_var, global_step)
    writer.add_scalar("train/is_line_search_success", int(is_line_search_success), global_step)
    writer.add_scalar("train/surrogate_objective", policy_objective_values, global_step)
    writer.add_scalar("Charts/episode_step", episode_step, global_step)
    writer.add_scalar("Charts/gradient_step", gradient_step, global_step)

# one last evaluation stage
if exp.eval_agent:
    last_evaluated_episode = episode_step
    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=greedy_evaluation)
    tracked_returns_over_training.append(tracked_return)
    tracked_episode_len_over_training.append(tracked_episode_len)
    tracked_episode_count.append([episode_step, global_step])

    # if there has been improvment of the model -
    if np.mean(tracked_return) > eval_max_return:
        eval_max_return = np.mean(tracked_return)
        # call helper function save_and_log_agent to save model, create video, log video to wandb
        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=True)

    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)

envs.close()
envs_eval.close()
writer.close()
if wandb.run is not None:
    wandb.finish(quiet=True)
    wandb.init(mode="disabled")

hf.save_train_config_to_yaml(exp, hypp)








agent_name = exp.run_name
agent_exp_type = exp.exp_type  # both are needed to identify the agent location


exp_folder = "" if agent_exp_type is None else agent_exp_type
filepath, _ = hf.create_folder_relative(f"{exp_folder}/{agent_name}/videos")

hf.record_video(exp.env_id, agent_name, f"{filepath}/best.mp4", exp_type=agent_exp_type, greedy=True)
Video(data=f"{filepath}/best.mp4", html_attributes='loop autoplay', embed=True)





eval_params = edict()  # eval_params - evaluation settings for trained agent

eval_params.run_name00 = exp.run_name
eval_params.exp_type00 = exp.exp_type

# eval_params.run_name01 = "CartPole-v1__PPO__1__230323_224300"
# eval_params.exp_type01 = None 

# eval_params.run_name02 = "CartPole-v1__PPO__1__230302_221245"
# eval_params.exp_type02 = None

agent_labels = []

episode_axis_limit = None

hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)





# %load_ext tensorboard
# %tensorboard --logdir logs --host localhost
