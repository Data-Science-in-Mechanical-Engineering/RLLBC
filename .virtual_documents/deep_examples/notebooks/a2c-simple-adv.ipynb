





import os
import time
import random
import warnings
from datetime import datetime

import numpy as np
import matplotlib.pyplot as plt
from tqdm import notebook
from easydict import EasyDict as edict
from IPython.display import Video

import utils.helper_fns as hf

import gym
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical

warnings.filterwarnings("ignore", category=DeprecationWarning)

os.environ['SDL_VIDEODRIVER']='dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'a2c-simple-adv.ipynb'

plt.rcParams['figure.dpi'] = 100
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

%load_ext autoreload
%autoreload 2








exp = edict()

exp.exp_name = 'A2C'  # algorithm name, in this case it should be 'A2C'
exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.device = device.type  # save the device type used to load tensors and perform tensor operations

set_random_seed = True  # set random seed for reproducibility
exp.seed = 1

# project name under which logs are saved on to Weights & Biases (if weights and biases logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RLLBC_{exp.env_id}"

# name prefix for files logged in this notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"


if set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = set_random_seed





class Agent(nn.Module):

    def __init__(self, env):
        super().__init__()

        self.value_network = nn.Sequential(
            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )

        self.policy_network = nn.Sequential(
            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, env.single_action_space.n),
        )

    def get_action_and_value(self, x):
        value, logits = self.value_network(x), self.policy_network(x)
        probs = Categorical(logits=logits)
        action = probs.sample()

        return action, probs.log_prob(action), probs.entropy(), value

    def get_value(self, x):
        return self.value_network(x)

    def get_action(self, x, greedy=False):

        logits = self.policy_network(x)

        if greedy:
            action = logits.argmax(1)
        else:
            probs = Categorical(logits=logits)
            action = probs.sample()
        return action


class Agent_shared(nn.Module):

    def __init__(self, env):
        super().__init__()

        # shared layers between actor and critic
        self.network = nn.Sequential(
            nn.Linear(np.array(env.single_observation_space.shape).prod(), 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
        )

        # linear layers for actor and critic
        self.value_layer = nn.Linear(64, 1)
        self.action_layer = nn.Linear(64, env.single_action_space.n)

    def get_action_and_value(self, x):
        out = self.network(x)
        value, logits = self.value_layer(out), self.action_layer(out)
        probs = Categorical(logits=logits)
        action = probs.sample()

        return action, probs.log_prob(action), probs.entropy(), value

    def get_value(self, x):
        out = self.network(x)
        return self.value_layer(out)

    def get_action(self, x, greedy=False):
        out = self.network(x)
        logits = self.action_layer(out)

        if greedy:
            action = logits.argmax(1)
        else:
            probs = Categorical(logits=logits)
            action = probs.sample()
        return action





hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = True
exp.capture_video = True
exp.num_envs = 4  # number of parallel game environments

# flags to generate agent average performance during training.
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 10
exp.eval_frequency = 20

# putting the run into the designated log folder for structuring
exp.exp_type = None  # directory the run is saved to. Should be None or a string value

# agent learning specific flags
hypp.total_timesteps = 300000

hypp.num_steps = 5  # number of environment steps before optimization
hypp.learning_rate = 3e-4  # learning rate used by the optimizer
hypp.gamma = 0.99  # discount factor of future rewards
hypp.norm_adv = True  # whether the advantages are normalized
hypp.ent_coef = 0.01  # weighting coefficient for the entropy loss
hypp.vf_coef = 0.5  # weighting coefficient for the value function loss
hypp.max_grad_norm = 1  # the maximum norm of the gradients computed
hypp.batch_size = int(exp.num_envs * hypp.num_steps)  # batch size for one step of gradient descent





# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

# Init tensorboard logging and wandb logging
writer = hf.setup_logging(wandb_prj_name, exp, hypp)

# Create two vectorized envs: one to fill the rollout buffer with trajectories and
# another to evaluate the agent performance at different stages of training
envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])
envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(exp.num_envs)])

# Init list to track agent's performance throughout training
tracked_returns_over_training = []
tracked_episode_len_over_training = []
tracked_episode_count = []
last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated
greedy_evaluation = False  # whether to perform the evaluation in a greedy way or not
eval_max_return = -float('inf')

# Create Agent class instance and network optimizer
agent = Agent(envs).to(device)
# agent = Agent_shared(envs).to(device)
optimizer = optim.Adam(agent.parameters(), lr=hypp.learning_rate)

# Init observation to start learning
gradient_step = 0
global_step = 0
episode_step = 0
start_time = time.time()
next_obs = torch.Tensor(envs.reset()).to(device)
next_done = torch.zeros(exp.num_envs).to(device)
num_updates = hypp.total_timesteps // hypp.batch_size

pbar = notebook.tqdm(range(1, num_updates + 1))

# training loop
for update in pbar:

    # initialize the rollout buffer
    observations = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_observation_space.shape).to(device)
    actions = torch.zeros((hypp.num_steps, exp.num_envs) + envs.single_action_space.shape).to(device)
    logprobs = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
    rewards = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
    dones = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
    values = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)
    entropies = torch.zeros((hypp.num_steps, exp.num_envs)).to(device)

    # run environments for specified number of steps
    for step in range(0, hypp.num_steps):
        global_step += 1 * exp.num_envs
        observations[step] = next_obs
        dones[step] = next_done

        # get action from agent + bookkeeping
        action, logprob, entropy, value = agent.get_action_and_value(next_obs)
        values[step] = value.flatten()
        actions[step] = action
        logprobs[step] = logprob
        entropies[step] = entropy

        # take action in environment
        next_obs, reward, done, info = envs.step(action.cpu().numpy())
        rewards[step] = torch.tensor(reward).to(device).view(-1)
        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)

        for item in info:
            if "episode" in item.keys():
                pbar.set_description(f"global_step={global_step}, episodic_return={item['episode']['r']}")
                writer.add_scalar("rollout/episodic_return", item["episode"]["r"], global_step)
                writer.add_scalar("rollout/episodic_length", item["episode"]["l"], global_step)
                writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
                writer.add_scalar("Charts/episode_step", episode_step, global_step)
                episode_step += 1
                break

        # evaluation of the agent
        if exp.eval_agent and (episode_step % exp.eval_frequency == 0) and last_evaluated_episode != episode_step:
            last_evaluated_episode = episode_step
            tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,
                                                                    exp.seed, greedy_actor=greedy_evaluation)
            tracked_returns_over_training.append(tracked_return)
            tracked_episode_len_over_training.append(tracked_episode_len)
            tracked_episode_count.append([episode_step, global_step])

            # if there has been improvement of the model - save model, create video, log video to wandb
            if np.mean(tracked_return) > eval_max_return:
                eval_max_return = np.mean(tracked_return)
                # call helper function save_and_log_agent to save model, create video, log video to wandb
                hf.save_and_log_agent(exp, agent, episode_step,
                                      greedy=greedy_evaluation, print_path=False)

    # calculate advantages and returns
    with torch.no_grad():
        next_value = agent.get_value(next_obs).reshape(1, -1)
        advantages = torch.zeros_like(rewards).to(device)
        returns = torch.zeros_like(rewards).to(device)
        for t in reversed(range(hypp.num_steps)):
            if t == hypp.num_steps - 1:
                nextnonterminal = 1.0 - next_done
                returns[t] = rewards[t] + hypp.gamma * next_value * nextnonterminal
            else:
                nextnonterminal = 1.0 - dones[t + 1]
                returns[t] = rewards[t] + hypp.gamma * returns[t+1] * nextnonterminal
        advantages = returns - values

    # update policy and value function
    # normalizing advantages
    if hypp.norm_adv:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # flatten the batch
    b_observations = observations.reshape((-1,) + envs.single_observation_space.shape)
    b_logprobs = logprobs.reshape(-1)
    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
    b_advantages = advantages.reshape(-1)
    b_returns = returns.reshape(-1)
    b_values = values.reshape(-1)
    b_entropies = entropies.reshape(-1)

    # policy Loss
    policy_loss = -(b_logprobs * b_advantages).mean()

    # value Loss - MSE
    value_loss = 0.5 * ((b_values - b_returns)**2).mean()

    # entropy loss
    entropy_loss = -b_entropies.mean()

    total_loss = policy_loss + hypp.vf_coef * value_loss + hypp.ent_coef * entropy_loss

    optimizer.zero_grad()

    total_loss.backward()

    # clipping the gradients
    nn.utils.clip_grad_norm_(agent.parameters(), hypp.max_grad_norm)

    optimizer.step()
    gradient_step += 1

    # calculating the explained variance
    y_pred, y_true = b_values.cpu().detach().numpy(), b_returns.cpu().detach().numpy()
    var_y = np.var(y_true)
    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y

    # logging metrics
    writer.add_scalar("train/value_loss", value_loss.item(), global_step)
    writer.add_scalar("train/policy_loss", policy_loss.item(), global_step)
    writer.add_scalar("train/entropy", -entropy_loss.item(), global_step)
    writer.add_scalar("train/explained_variance", explained_var, global_step)
    writer.add_scalar("others/SPS", int(global_step / (time.time() - start_time)), global_step)
    writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
    writer.add_scalar("Charts/episode_step", episode_step, global_step)


# one last evaluation stage
if exp.eval_agent:
    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_evaluation)
    tracked_returns_over_training.append(tracked_return)
    tracked_episode_len_over_training.append(tracked_episode_len)
    tracked_episode_count.append([episode_step, global_step])

    # if there has been improvement of the model - save model, create video, log video to wandb
    if np.mean(tracked_return) > eval_max_return:
        eval_max_return = np.mean(tracked_return)
        # call helper function save_and_log_agent to save model, create video, log video to wandb
        hf.save_and_log_agent(exp, agent, episode_step, greedy=greedy_evaluation, print_path=False)

    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training,
                           tracked_episode_count, exp.eval_count, exp.run_name, exp.exp_type)

envs.close()
writer.close()
if wandb.run is not None:
    wandb.finish(quiet=True)
    wandb.init(mode="disabled")

hf.save_train_config_to_yaml(exp, hypp)








agent_name = exp.run_name
agent_exp_type = exp.exp_type  # both are needed to identify the agent location


exp_folder = "" if agent_exp_type is None else agent_exp_type
filepath, _ = hf.create_folder_relative(f"{exp_folder}/{agent_name}/videos")

hf.record_video(exp.env_id, agent_name, f"{filepath}/best.mp4", exp_type=agent_exp_type, greedy=True)
Video(data=f"{filepath}/best.mp4", html_attributes='loop autoplay', embed=True)





eval_params = edict()  # eval_params - evaluation settings for trained agent

eval_params.run_name00 = exp.run_name
eval_params.exp_type00 = exp.exp_type

# eval_params.run_name01 = "CartPole-v1__PPO__1__230302_224624"
# eval_params.exp_type01 = None

# eval_params.run_name02 = "CartPole-v1__PPO__1__230302_221245"
# eval_params.exp_type02 = None

agent_labels = []

episode_axis_limit = None

hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)





# %load_ext tensorboard
# %tensorboard --logdir runs --host localhost
