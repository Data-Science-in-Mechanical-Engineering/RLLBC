





import os
import time
import random
import warnings
from datetime import datetime

import numpy as np
import matplotlib.pyplot as plt
from tqdm import notebook
from easydict import EasyDict as edict
from IPython.display import Video

import utils.helper_fns as hf

import gym
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from stable_baselines3.common.buffers import ReplayBuffer

warnings.filterwarnings("ignore", category=DeprecationWarning)

os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'ddpg.ipynb'

plt.rcParams['figure.dpi'] = 100
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

%load_ext autoreload
%autoreload 2








exp = edict()

exp.exp_name = 'DDPG'  # algorithm name, in this case it should be 'DDPG'

exp.env_id = 'Pendulum-v1'  # name of the continuous gym environment to be used in this experiment.
exp.device = device.type  # save the device type used to load tensors and perform tensor operations

set_random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RLLBC_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = set_random_seed





class QNetwork(nn.Module):
    def __init__(self, env):
        super().__init__()
        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, x, a):
        x = torch.cat([x, a], 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class ActorNetwork(nn.Module):
    def __init__(self, env):
        super().__init__()
        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc_mu = nn.Linear(64, np.prod(env.single_action_space.shape))

        # action rescaling
        self.register_buffer(
            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
        )
        self.register_buffer(
            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
        )

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = torch.tanh(self.fc_mu(x))
        return x * self.action_scale + self.action_bias


class Agent(nn.Module):
    def __init__(self, env):
        super().__init__()
        # does it work in this way? Are all used networks put to device later?
        self.q_net = QNetwork(env)
        self.q_target_net = QNetwork(env)
        self.actor_net = ActorNetwork(env)
        self.actor_target_net = ActorNetwork(env)

        self.q_target_net.load_state_dict(self.q_net.state_dict())
        self.actor_target_net.load_state_dict(self.actor_net.state_dict())

    def get_q_value(self, x, a):
        return self.q_net(x, a)

    def get_target_q_value(self, x, a):
        return self.q_target_net(x, a)

    def get_action(self, x, greedy=True):
        return self.actor_net(x)

    def get_target_action(self, x):
        return self.actor_target_net(x)

    def track_networks(self, tau):
        for param, target_param in zip(self.actor_net.parameters(), self.actor_target_net.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
        for param, target_param in zip(self.q_net.parameters(), self.q_target_net.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)






hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = True
exp.capture_video = True

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 10
exp.eval_frequency = 20

# putting the run into the designated log folder for structuring
exp.exp_type = None  # directory the run is saved to. Should be None or a string value

# agent training specific parameters and hyperparameters
hypp.total_timesteps = 20000  # the training duration in number of time steps
hypp.learning_rate = 3e-4  # the learning rate for the optimizer
hypp.gamma = 0.99  # decay factor of future rewards
hypp.buffer_size = 10000  # the size of the replay memory buffer
hypp.tau = 0.005  # target smoothing coefficient
hypp.batch_size = 128  # number of samples taken from the replay buffer for one step
hypp.exploration_noise = 0.1  # the degree of (gaussian) exploration
hypp.start_learning = 10000  # the timestep the learning starts (before that the replay buffer is filled)
hypp.policy_frequency = 2  # the frequency of training the policy





# Initialization of Replay Buffer

envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])

rb = ReplayBuffer(
    hypp.buffer_size,
    envs.single_observation_space,
    envs.single_action_space,
    device,
    handle_timeout_termination=True,
)

envs.close()





# reinit run_name
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

# init tensorboard logging and wandb logging
writer = hf.setup_logging(wandb_prj_name, exp, hypp)

# create two vectorized envs: one to fill the rollout buffer with trajectories and
# another one to evaluate the agent performance at different stages of training
envs = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed)])
envs_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])

# making sure that the observation space of the environment is of type np.float32
envs.single_observation_space.dtype = np.float32

# init list to track agent's performance throughout training
tracked_returns_over_training = []
tracked_episode_len_over_training = []
tracked_episode_count = []
last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated
eval_max_return = -float('inf')

# create Agent class instance and network optimizers: one for actor network, one for Q-network
agent = Agent(envs).to(device)
q_optimizer = optim.Adam(agent.q_net.parameters(), lr=hypp.learning_rate)
actor_optimizer = optim.Adam(agent.actor_net.parameters(), lr=hypp.learning_rate)

global_step = 0
episode_step = 0
gradient_step = 0

# init observation to start learning
start_time = time.time()
obs = envs.reset()

pbar = notebook.tqdm(range(1, hypp.total_timesteps + 1))

# training loop
for update in pbar:

    # get next action in environment
    # if learning has not started yet, sample the action randomly
    # otherwise follow the actor network and add exploration noise
    if global_step < hypp.start_learning:
        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
    else:
        with torch.no_grad():
            actions = agent.get_action(torch.Tensor(obs).to(device))
            actions += torch.normal(0, agent.actor_net.action_scale * hypp.exploration_noise)
            actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high)

    # taking a step in the environment - applying the action
    next_obs, rewards, dones, infos = envs.step(actions)
    global_step += 1

    # on episode end: log episodic return and length to tensorboard
    for info in infos:
        if "episode" in info.keys():
            pbar.set_description(f"global_step: {global_step}, episodic_return={info['episode']['r']}")
            writer.add_scalar("rollout/episodic_return", info["episode"]["r"], global_step)
            writer.add_scalar("rollout/episodic_length", info["episode"]["l"], global_step)
            writer.add_scalar("Charts/episode_step", episode_step, global_step)
            writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
            episode_step += 1
            break

    # evaluation of the agent
    if exp.eval_agent and (episode_step % exp.eval_frequency == 0) and last_evaluated_episode != episode_step:
        last_evaluated_episode = episode_step
        tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count,
                                                                exp.seed, greedy_actor=True)
        tracked_returns_over_training.append(tracked_return)
        tracked_episode_len_over_training.append(tracked_episode_len)
        tracked_episode_count.append([episode_step, global_step])

        # if there has been improvement of the model - save model, create video, log video to wandb
        if np.mean(tracked_return) > eval_max_return:
            eval_max_return = np.mean(tracked_return)
            hf.save_and_log_agent(exp, agent, episode_step,
                                  greedy=True, print_path=False)

    # handling the terminal observation
    real_next_obs = next_obs.copy()
    for idx, d in enumerate(dones):
        if d:
            real_next_obs[idx] = infos[idx]["terminal_observation"]

    # add data to replay buffer
    rb.add(obs, real_next_obs, actions, rewards, dones, infos)

    # update obs
    obs = next_obs

    # training of the agent
    if global_step > hypp.start_learning:

        data = rb.sample(hypp.batch_size)

        # calculate the TD target using the target networks
        with torch.no_grad():
            next_state_actions = agent.get_target_action(data.next_observations)
            q_next_target = agent.get_target_q_value(data.next_observations, next_state_actions)
            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * hypp.gamma * (q_next_target).view(-1)

        # calulate mean squared error between target and value
        q_a_values = agent.get_q_value(data.observations, data.actions).view(-1)
        td_loss = F.mse_loss(q_a_values, next_q_value)

        # optimize the Q-network
        q_optimizer.zero_grad()
        td_loss.backward()
        q_optimizer.step()

        if global_step % hypp.policy_frequency == 0:

            # improve policy by following the policy gradient
            policy_loss = -agent.get_q_value(data.observations, agent.get_action(data.observations, greedy=True)).mean()
            actor_optimizer.zero_grad()
            policy_loss.backward()
            actor_optimizer.step()
            gradient_step += 1

            # update the target network
            agent.track_networks(tau=hypp.tau)

        # log actor loss, td-loss, and q-values to tensorboard
        if global_step % 100 == 0:
            writer.add_scalar("train/td_loss", td_loss.item(), global_step)
            writer.add_scalar("train/policy_loss", policy_loss.item(), global_step)
            writer.add_scalar("train/q_values", q_a_values.mean().item(), global_step)
            writer.add_scalar("Charts/episode_step", episode_step, global_step)
            writer.add_scalar("Charts/gradient_step", gradient_step, global_step)
            writer.add_scalar("other/SPS", int(global_step / (time.time() - start_time)), global_step)

# one last evaluation stage
if exp.eval_agent:
    tracked_return, tracked_episode_len = hf.evaluate_agent(envs_eval, agent, exp.eval_count, exp.seed, greedy_actor=True)
    tracked_returns_over_training.append(tracked_return)
    tracked_episode_len_over_training.append(tracked_episode_len)
    tracked_episode_count.append([episode_step, global_step])

    # if there has been improvement of the model - save model, create video, log video to wandb
    if np.mean(tracked_return) > eval_max_return:
        eval_max_return = np.mean(tracked_return)
        # call helper function save_and_log_agent to save model, create video, log video to wandb
        hf.save_and_log_agent(exp, agent, episode_step,
                              greedy=True, print_path=False)

    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name)    

envs.close()
writer.close()
pbar.close()
if wandb.run is not None:
    wandb.finish(quiet=True)
    wandb.init(mode= 'disabled')

hf.save_train_config_to_yaml(exp, hypp)








agent_name = exp.run_name
agent_exp_type = exp.exp_type  # both are needed to identify the agent location


exp_folder = "" if agent_exp_type is None else agent_exp_type
filepath, _ = hf.create_folder_relative(f"{exp_folder}/{agent_name}/videos")

hf.record_video(exp.env_id, agent_name, f"{filepath}/best.mp4", exp_type=agent_exp_type, greedy=True)
Video(data=f"{filepath}/best.mp4", html_attributes='loop autoplay', embed=True)





eval_params = edict()  # eval_params - evaluation settings for trained agent

eval_params.run_name00 = exp.run_name
eval_params.exp_type00 = exp.exp_type

# eval_params.run_name01 = "CartPole-v1__PPO__1__230302_224624"
# eval_params.exp_type01 = None

# eval_params.run_name02 = "CartPole-v1__PPO__1__230302_221245"
# eval_params.exp_type02 = None

agent_labels = []

episode_axis_limit = None

hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)





# %load_ext tensorboard
# %tensorboard --logdir runs --host localhost
